from typing import Dict, Any, List, Optional, TypedDict, Annotated
from langgraph.graph import StateGraph, END
from langgraph.graph import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
import time

from ..config import settings
from ..models import QueryType, RAGSearchResult, QueryResponse
from ..models.llm_implementations import llm_service
from ..models.llm_providers import LLMMessage
from ..services.rag_service import rag_service
from ..services.database_service import db_service
from ..services.vector_embedding_service import vector_service
from ..services.history_service import history_service
from ..services.memory_service import memory_service
from ..utils.intent_classifier import intent_classifier

import logging
import json

logger = logging.getLogger(__name__)

class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    query: str
    intent: Optional[QueryType]
    rag_results: Optional[List[RAGSearchResult]]
    db_results: Optional[Dict[str, Any]]
    sql_query: Optional[str]
    source_links: Optional[List[str]]
    final_answer: Optional[str]
    reasoning: Optional[str]
    needs_tools: bool
    tool_to_use: Optional[str]
    # æ–°å¢å­—æ®µæ”¯æŒéªŒè¯å’Œfallbackæµç¨‹
    db_results_valid: bool
    needs_fallback: bool
    needs_vector_search: bool  # Added this missing field!
    fallback_executed: bool
    has_mixed_results: bool
    vector_enhanced: Optional[bool]  # Also add this for tracking
    suggestions_used: Optional[Dict[str, Any]]  # And this for tracking suggestions
    validation_reason: Optional[str]  # And validation reason
    # Memory-related fields
    memory_info: Optional[str]  # Memory context from memory service
    user_email: Optional[str]  # User email for memory retrieval
    session_id: Optional[str]  # Session ID for memory tracking
    start_time: Optional[float]  # Start time for query
    # Data formatting fields
    formatted_db_results: Optional[str]  # LLM-formatted database results

@tool
async def rag_search_tool(query: str, memory_info: str = "", dataset_id: Optional[str] = None) -> Dict[str, Any]:
    """æœç´¢RAGæ–‡æ¡£åº“è·å–ç›¸å…³ä¿¡æ¯"""
    if not dataset_id:
        dataset_id = settings.ragflow_default_dataset_id
    
    result = await rag_service.search_rag_with_answer(query, dataset_id, memory_info)
    return result

@tool 
async def database_query_tool(question: str, memory_info: str = "",) -> Dict[str, Any]:
    """æŸ¥è¯¢æ•°æ®åº“è·å–ç»Ÿè®¡ä¿¡æ¯"""
    # ç›´æ¥æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢
    result = await db_service.query_database(question, memory_info)
    
    # è¿”å›æ ¼å¼åŒ–ç»“æœ
    if result.get("success"):
        return format_db_results(result)
    else:
        return {
            "content": f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}",
            "sql_query": result.get("sql_query"),
            "results": [],
            "success": False,
            "error": result.get("error")
        }

@tool
async def vector_database_query_tool(question: str, failed_sql: str = None, memory_info: str = "") -> Dict[str, Any]:
    """ä½¿ç”¨å‘é‡æœç´¢å¢å¼ºæ•°æ®åº“æŸ¥è¯¢ï¼Œæ‰¾åˆ°å®é™…å­˜åœ¨çš„å€¼å¹¶é‡æ–°ç”ŸæˆSQL"""
    import yaml
    from pathlib import Path
    from ..utils.sql_parser import SQLParser
    
    logger.info(f"Starting vector-enhanced database query for: {question}")
    
    try:
        # Load vector search configuration
        config_path = Path(__file__).parent.parent / "config" / "vector_search_config.yaml"
        vector_config = {}
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                vector_config = yaml.safe_load(f)
        else:
            logger.warning("Vector search config not found, using defaults")
            return {
                "content": "å‘é‡æœç´¢é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°",
                "success": False,
                "error": "Missing vector_search_config.yaml"
            }
        
        # Parse the failed SQL to identify tables and fields
        sql_parser = SQLParser()
        parsed_sql = {}
        if failed_sql:
            parsed_sql = sql_parser.parse_sql(failed_sql)
            logger.info(f"Parsed SQL - tables: {parsed_sql.get('tables')}, fields: {parsed_sql.get('fields')}")
        
        # Collect suggestions for all relevant fields
        all_suggestions = {}
        vector_fields = vector_config.get('vector_search_fields', [])
        
        for field_config in vector_fields:
            table = field_config['table']
            field = field_config['field']
            description = field_config.get('description', field)
            
            # Check if this field is relevant to the query
            should_search = False
            
            # If we have a parsed SQL, check if the table/field is mentioned
            if parsed_sql:
                if table in parsed_sql.get('tables', []) or field in parsed_sql.get('fields', []):
                    should_search = True
                    logger.info(f"Field {table}.{field} found in failed SQL")
            
            # Also check if keywords from the question match this field's keywords
            keywords = field_config.get('keywords', [])
            question_lower = question.lower()
            for keyword in keywords:
                if keyword.lower() in question_lower:
                    should_search = True
                    logger.info(f"Keyword '{keyword}' found in question for field {table}.{field}")
                    break
            
            if should_search:
                logger.info(f"Searching for similar values in {table}.{field} ({description})")
                
                # Get similar values from the database
                suggestions = await vector_service.search_similar_values(
                    search_text=question,
                    table_name=table,
                    field_name=field,
                    top_k=settings.max_suggestions or 3,  # This is already the max_suggestions value
                    similarity_threshold=settings.similarity_threshold or 0.3  # This is already the threshold value
                )
                
                if suggestions:
                    # Limit suggestions to max_suggestions
                    limited_suggestions = suggestions[:settings.max_suggestions]
                    # Extract just the field values for display
                    field_values = [suggestion['field_value'] for suggestion in limited_suggestions]
                    logger.info(f"Found {len(limited_suggestions)} suggestions for {description}: {field_values[:3]}...")
                    all_suggestions[f"{table}.{field}"] = {
                        "values": field_values,
                        "description": description,
                        "table": table,
                        "field": field
                    }
        
        if not all_suggestions:
            logger.info("No vector search suggestions found")
            return {
                "content": "æœªæ‰¾åˆ°ç›¸ä¼¼çš„æ•°æ®åº“å€¼",
                "success": False,
                "suggestions": {},
                "message": "å‘é‡æœç´¢æœªæ‰¾åˆ°åŒ¹é…çš„å€¼"
            }
        
        # Build enhanced prompt with suggestions
        suggestion_text = "\n\nåŸºäºå‘é‡æœç´¢æ‰¾åˆ°çš„æ•°æ®åº“å®é™…å€¼ï¼š\n"
        for field_key, field_data in all_suggestions.items():
            suggestion_text += f"- {field_data['description']} ({field_data['table']}.{field_data['field']}): "
            suggestion_text += f"{', '.join(field_data['values'])}"
            if len(field_data['values']) > 3:
                suggestion_text += f" ç­‰{len(field_data['values'])}ä¸ªå€¼"
            suggestion_text += "\n"
        
        enhanced_question = f"{question}{suggestion_text}\nè¯·ä½¿ç”¨è¿™äº›å®é™…å­˜åœ¨çš„å€¼é‡æ–°ç”ŸæˆSQLæŸ¥è¯¢ã€‚å¦‚æœæœ‰å¤šä¸ªåŒ¹é…å€¼ï¼Œä½¿ç”¨æœ€ç›¸ä¼¼çš„å€¼æ¥æŸ¥è¯¢ã€‚"
        
        # Generate new SQL with enhanced context
        logger.info("Generating new SQL with vector search suggestions")
        enhanced_result = await db_service.query_database(enhanced_question, memory_info)
        
        if enhanced_result.get("success"):
            logger.info(f"Vector-enhanced query successful, got {len(enhanced_result.get('results', []))} results")
            enhanced_result["vector_enhanced"] = True
            enhanced_result["suggestions_used"] = all_suggestions
            return format_db_results(enhanced_result)
        else:
            logger.warning(f"Vector-enhanced query failed: {enhanced_result.get('error')}")
            return {
                "content": f"å‘é‡å¢å¼ºæŸ¥è¯¢å¤±è´¥: {enhanced_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                "success": False,
                "suggestions": all_suggestions,
                "sql_query": enhanced_result.get("sql_query"),
                "error": enhanced_result.get("error")
            }
            
    except Exception as e:
        logger.error(f"Error in vector database query tool: {e}")
        return {
            "content": f"å‘é‡æœç´¢å‡ºé”™: {str(e)}",
            "success": False,
            "error": str(e)
        }

def format_db_results(result: Dict[str, Any]) -> Dict[str, Any]:
    """æ ¼å¼åŒ–æ•°æ®åº“æŸ¥è¯¢ç»“æœ"""
    # å¤„ç†æ—¥æœŸåºåˆ—åŒ–é—®é¢˜
    def serialize_dates(obj):
        """é€’å½’å¤„ç†å¯¹è±¡ä¸­çš„æ—¥æœŸç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
        if hasattr(obj, 'isoformat'):  # datetime, date å¯¹è±¡
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {key: serialize_dates(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [serialize_dates(item) for item in obj]
        else:
            return obj
    
    # åºåˆ—åŒ–ç»“æœä¸­çš„æ—¥æœŸå¯¹è±¡
    serialized_results = serialize_dates(result.get("results", []))
    
    success = False
    try:
        success = True
        formatted_content = json.dumps({
            "sql_query": result.get("sql_query"),
            "results": serialized_results,
            "vector_enhanced": result.get("vector_enhanced", False),
            "suggestions_used": result.get("suggestions_used", [])
        }, ensure_ascii=False, indent=2)
    except (TypeError, ValueError) as e:
        # å¦‚æœä»ç„¶æœ‰åºåˆ—åŒ–é—®é¢˜ï¼Œä½¿ç”¨æ›´å®‰å…¨çš„æ ¼å¼åŒ–æ–¹æ³•
        logger.warning(f"JSON serialization failed, using fallback formatting: {e}")
        formatted_content = f"SQLæŸ¥è¯¢: {result.get('sql_query', 'N/A')}\n"
        formatted_content += f"ç»“æœæ•°é‡: {len(serialized_results)}\n"
        formatted_content += "ç»“æœæ•°æ®:\n"
        for i, row in enumerate(serialized_results[:5], 1):  # åªæ˜¾ç¤ºå‰5è¡Œ
            formatted_content += f"  {i}. {row}\n"
        if len(serialized_results) > 5:
            formatted_content += f"  ... è¿˜æœ‰ {len(serialized_results) - 5} è¡Œæ•°æ®"
    
    return {
        "success": success,
        "content": formatted_content,
        "sql_query": result.get("sql_query"),
        "results": serialized_results,
        "vector_enhanced": result.get("vector_enhanced", False),
        "suggestions_used": result.get("suggestions_used", [])
    }

class KangniReActAgent:
    def __init__(self):
        # ä½¿ç”¨é›†ä¸­å¼LLMæœåŠ¡
        self.llm_available = llm_service.llm_available
        self.llm_provider = llm_service.llm_provider
        
        if self.llm_available:
            try:
                # ç»‘å®šå·¥å…· - æ·»åŠ vector_database_query_tool
                self.tools = [rag_search_tool, database_query_tool, vector_database_query_tool]
                
                # æ„å»ºçŠ¶æ€å›¾
                self.workflow = self._build_workflow()

                # ä¿å­˜çŠ¶æ€å›¾çš„å¯è§†åŒ–è¡¨ç¤º
                self._save_graph_visualization()
                
                logger.info(f"Agent initialized successfully with LLM provider: {self.llm_provider}")
                
            except Exception as e:
                logger.warning(f"Failed to initialize agent: {e}")
                self.llm_available = False
                self.workflow = None
        else:
            logger.warning("LLM service not available, agent features will be disabled")
            self.workflow = None
    
    def _save_graph_visualization(self, filename: str = "docs/workflow_graph.png") -> None:
        """ä¿å­˜çŠ¶æ€å›¾çš„å¯è§†åŒ–è¡¨ç¤º
        
        Args:
            filename: ä¿å­˜æ–‡ä»¶è·¯å¾„
        """
        if not self.workflow:
            logger.warning("No workflow to visualize")
            return
            
        try:
            # å°è¯•ç”Ÿæˆ Mermaid å›¾
            graph = self.workflow.get_graph()
            
            # é¦–å…ˆå°è¯•ä¿å­˜ä¸º PNGï¼ˆéœ€è¦ graphvizï¼‰
            try:
                with open(filename, "wb") as f:
                    f.write(graph.draw_mermaid_png())
                logger.info(f"Graph visualization saved as PNG: {filename}")
                print(f"âœ… Workflow graph saved as: {filename}")
            except Exception as png_error:
                logger.warning(f"Could not save as PNG (graphviz may not be installed): {png_error}")
                
                # é€€è€Œæ±‚å…¶æ¬¡ï¼Œä¿å­˜ä¸º Mermaid æ–‡æœ¬æ ¼å¼
                mermaid_filename = filename.replace('.png', '.mermaid')
                try:
                    mermaid_text = graph.draw_mermaid()
                    with open(mermaid_filename, "w", encoding="utf-8") as f:
                        f.write(mermaid_text)
                    logger.info(f"Graph saved as Mermaid text: {mermaid_filename}")
                    print(f"âœ… Workflow graph saved as Mermaid text: {mermaid_filename}")
                    print(f"   You can visualize it at: https://mermaid.live/")
                    
                    # åŒæ—¶æ‰“å°å›¾å½¢ç»“æ„
                    print("\nğŸ“Š Workflow Structure:")
                    print(mermaid_text)
                except Exception as mermaid_error:
                    logger.error(f"Could not save Mermaid text: {mermaid_error}")
                    
                    # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰“å°èŠ‚ç‚¹å’Œè¾¹
                    print("\nğŸ“Š Workflow Nodes and Edges:")
                    print(f"Nodes: {graph.nodes}")
                    print(f"Edges: {graph.edges}")
                    
        except Exception as e:
            logger.error(f"Failed to save graph visualization: {e}")
            print(f"âŒ Could not visualize workflow: {e}")
    
    def _build_workflow(self) -> StateGraph:
        """æ„å»ºLangGraphå·¥ä½œæµ"""
        workflow = StateGraph(AgentState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("load_memory", self.load_memory_info)  # New memory node
        workflow.add_node("classify_intent", self.classify_intent)
        workflow.add_node("agent_reasoning", self.agent_reasoning)
        workflow.add_node("tool_execution", self.execute_tools)
        workflow.add_node("validate_results", self.validate_results)
        workflow.add_node("vector_search", self.vector_search_enhancement)  # æ–°å¢å‘é‡æœç´¢èŠ‚ç‚¹
        workflow.add_node("fallback_search", self.fallback_search)
        workflow.add_node("generate_response", self.generate_response)
        workflow.add_node("save_memory", self.save_memory)  # New save memory node
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("load_memory")
        
        # æ·»åŠ åŸºç¡€è¾¹
        workflow.add_edge("load_memory", "classify_intent")
        workflow.add_edge("classify_intent", "agent_reasoning")
        
        # ç›´æ¥è¾¹ï¼šæ€»æ˜¯æ‰§è¡Œå·¥å…·
        workflow.add_edge("agent_reasoning", "tool_execution")
        workflow.add_edge("tool_execution", "validate_results")
        
        # æ¡ä»¶è¾¹ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦å‘é‡æœç´¢å¢å¼º
        def should_use_vector_search(state: AgentState) -> str:
            # Debug logging
            logger.info(f"Checking vector search condition - needs_vector_search: {state.get('needs_vector_search')}, needs_fallback: {state.get('needs_fallback')}")
            
            # å¦‚æœæ•°æ®åº“æŸ¥è¯¢è¿”å›0ç»“æœï¼Œä½¿ç”¨å‘é‡æœç´¢
            if state.get("needs_vector_search", False):
                logger.info("Routing to vector_search")
                return "vector_search"
            # å¦åˆ™æ£€æŸ¥æ˜¯å¦éœ€è¦fallback
            elif state.get("needs_fallback", False):
                logger.info("Routing to fallback_search")
                return "fallback_search"
            else:
                logger.info("Routing to generate_response")
                return "generate_response"
        
        workflow.add_conditional_edges(
            "validate_results", 
            should_use_vector_search,
            {
                "vector_search": "vector_search",
                "fallback_search": "fallback_search",
                "generate_response": "generate_response"
            }
        )
        workflow.add_edge("vector_search", "generate_response")
        workflow.add_edge("fallback_search", "generate_response")
        workflow.add_edge("generate_response", "save_memory")  # Save memory after generating response
        workflow.add_edge("save_memory", END)
        
        return workflow.compile()
    
    async def load_memory_info(self, state: AgentState) -> AgentState:
        """Load memory context for the user"""
        user_email = state.get("user_email")
        session_id = state.get("session_id")
        query = state["query"]
        
        logger.info(f"Loading memory context for user: {user_email}, session: {session_id}")
        
        # Get memory context from memory service
        memory_info = ""
        if user_email:
            try:
                memory_context = await memory_service.get_memory_context_for_agent(
                    user_email=user_email,
                    question=query,
                    session_id=session_id
                )

                # Build memory context string
                memory_info = ""
                if memory_context:
                    # Add recent interactions
                    # recent_interactions = memory_context.get("recent_interactions", [])
                    # if recent_interactions:
                    #     memory_info += "\næœ€è¿‘çš„äº¤äº’å†å²:\n"
                    #     for interaction in recent_interactions[:3]:
                    #         memory_info += f"- Q: {interaction['question']}...\n"
                    #         if interaction.get('answer'):
                    #             memory_info += f"  A: {interaction['answer']}...\n"
                    
                    # # Add long-term memories
                    # long_term = memory_context.get("long_term_memories", [])
                    # if long_term:
                    #     memory_info += "\nç›¸å…³çš„é•¿æœŸè®°å¿†:\n"
                    #     for mem in long_term[:3]:
                    #         memory_info += f"- {mem['content'][:150]}... (é‡è¦æ€§: {mem.get('importance', 'unknown')})\n"
                    
                    # Add short-term memories
                    short_term = memory_context.get("short_term_memories", [])
                    if short_term:
                        memory_info += "\nä¼šè¯ä¸Šä¸‹æ–‡:\n"
                        for mem in short_term[:3]:
                            memory_info += f"- {mem['content']}...\n"

                logger.info(f"Loaded memory context with {len(memory_context.get('short_term_memories', []))} short-term and {len(memory_context.get('long_term_memories', []))} long-term memories")
            except Exception as e:
                logger.error(f"Failed to load memory context: {e}")
        
        return {
            **state,
            "start_time": time.time(),
            "memory_info": memory_info
        }
    
    async def classify_intent(self, state: AgentState) -> AgentState:
        """æ„å›¾åˆ†ç±»èŠ‚ç‚¹"""
        query = state["query"]
        intent = intent_classifier.classify_intent(query)
        explanation = intent_classifier.get_classification_explanation(query, intent)
        
        logger.info(f"Intent classified as: {intent} - {explanation}")
        
        return {
            **state,
            "intent": intent,
            "reasoning": explanation
        }
    
    async def agent_reasoning(self, state: AgentState) -> AgentState:
        """Agentæ¨ç†èŠ‚ç‚¹"""
        query = state["query"]
        intent = state["intent"]
        memory_info = state.get("memory_info", {})
        
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œéœ€è¦åˆ†æç”¨æˆ·é—®é¢˜å¹¶å†³å®šä½¿ç”¨å“ªäº›å·¥å…·ã€‚ä½ æœ‰ä¸¤ä¸ªå·¥å…·å¯ç”¨ï¼š

1. rag_search_tool: ç”¨äºæœç´¢æ–‡æ¡£å’ŒçŸ¥è¯†åº“ï¼Œé€‚åˆå›ç­”æ¦‚å¿µã€åŸå› ã€æ–¹æ³•ç­‰é—®é¢˜
2. database_query_tool: ç”¨äºæŸ¥è¯¢æ•°æ®åº“ï¼Œé€‚åˆå›ç­”ç»Ÿè®¡ã€æ•°æ®åˆ†æç­‰é—®é¢˜
   - ç‰¹åˆ«æ³¨æ„ï¼šå½“ç”¨æˆ·æåˆ°"è®¢å•"ä½†æ²¡æœ‰æŒ‡å®šå…·ä½“ç±»å‹æ—¶ï¼Œç³»ç»Ÿä¼šé»˜è®¤æŸ¥è¯¢ kn_quality_trace_prod_orderï¼ˆç”Ÿäº§è®¢å•è¡¨ï¼‰

å½“å‰é—®é¢˜æ„å›¾åˆ†ç±»ä¸º: {intent}
åˆ†ç±»åŸå› : {state.get('reasoning', '')}

ç”¨æˆ·é—®é¢˜: {query}

è¯·åˆ†æé—®é¢˜å¹¶å†³å®šä½¿ç”¨å“ªäº›å·¥å…·ï¼š
1. å¦‚æœé—®é¢˜æ˜ç¡®éœ€è¦æ•°æ®åº“æŸ¥è¯¢ï¼ˆå¦‚ç»Ÿè®¡ã€è®¡æ•°ã€å…·ä½“æ•°æ®ï¼‰ï¼Œé€‰æ‹© database_query_tool
2. å¦‚æœé—®é¢˜æ˜ç¡®éœ€è¦æ–‡æ¡£æœç´¢ï¼ˆå¦‚æ¦‚å¿µè§£é‡Šã€åŸå› åˆ†æï¼‰ï¼Œé€‰æ‹© rag_search_tool  
3. å¦‚æœé—®é¢˜æ„å›¾ä¸æ˜ç¡®æˆ–éœ€è¦ç»¼åˆä¿¡æ¯ï¼Œé€‰æ‹© bothï¼ˆåŒæ—¶ä½¿ç”¨ä¸¤ä¸ªå·¥å…·ï¼‰
4. è€ƒè™‘ç”¨æˆ·çš„å†å²äº¤äº’æ¨¡å¼ï¼Œå¦‚æœç”¨æˆ·é€šå¸¸è¯¢é—®æŸç±»é—®é¢˜ï¼Œå¯ä»¥å‚è€ƒå†å²åå¥½

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
å·¥å…·é€‰æ‹©: [rag_search_tool/database_query_tool/both]
ç†ç”±: [ç®€è¦è¯´æ˜ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›å·¥å…·]
"""
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºLLMMessage
        llm_messages = [
            LLMMessage(role="system", content=system_prompt),
            LLMMessage(role="user", content=query)
        ]
        
        # Debug logging for LLM input
        logger.debug(f"LLM reasoning input: {system_prompt}")
        
        try:
            response = await llm_service.chat(llm_messages)
            
            # Debug logging for LLM output
            logger.debug(f"LLM reasoning output: {response.content}")
            
            # è§£æå“åº”ï¼Œç¡®å®šå·¥å…·é€‰æ‹©
            response_text = response.content.lower()
            
            # ç¡®å®šå·¥å…·ç±»å‹
            tool_to_use = None
            needs_tools = True  # æ€»æ˜¯éœ€è¦å·¥å…·ï¼Œä¸å†ç›´æ¥å›ç­”
            
            if "å·¥å…·é€‰æ‹©: both" in response_text or "å·¥å…·é€‰æ‹©ï¼šboth" in response_text:
                tool_to_use = "both"
            elif "rag_search_tool" in response_text:
                tool_to_use = "rag_search_tool"
            elif "database_query_tool" in response_text:
                tool_to_use = "database_query_tool"
            else:
                # é»˜è®¤ä½¿ç”¨bothå·¥å…·
                tool_to_use = "both"
            
            logger.info(f"Tool analysis: needs_tools={needs_tools}, tool_to_use={tool_to_use}")
            
            # åˆ›å»ºAIMessageå“åº”
            ai_message = AIMessage(content=response.content)
            
            return {
                **state,
                "needs_tools": needs_tools,
                "tool_to_use": tool_to_use,
                "messages": [*state["messages"], ai_message]
            }
            
        except Exception as e:
            logger.error(f"LLM reasoning error: {e}")
            error_message = AIMessage(content=f"æŠ±æ­‰ï¼Œåˆ†ææ‚¨çš„é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {
                **state,
                "needs_tools": False,
                "messages": [*state["messages"], error_message]
            }
    
    async def execute_tools(self, state: AgentState) -> AgentState:
        """å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹"""
        tool_to_use = state.get("tool_to_use")
        query = state["query"]
        memory_info = state.get("memory_info", "")
        
        if not tool_to_use:
            logger.warning("No tool specified for execution")
            return state
            
        logger.info(f"Executing tool: {tool_to_use} with query: {query}")
        
        try:
            messages = state["messages"]
            
            if tool_to_use == "rag_search_tool":
                result = await rag_search_tool.ainvoke({"query": query, "memory_info": memory_info})
                
                # å­˜å‚¨RAGç»“æœ
                state["rag_results"] = result.get("rag_results", [])
                
                # åˆ›å»ºå·¥å…·æ¶ˆæ¯ - ç›´æ¥ä½¿ç”¨LLMç”Ÿæˆçš„ç­”æ¡ˆ
                tool_message = AIMessage(content=f"RAGæœç´¢ç»“æœï¼š\n{result['content']}")
                messages.append(tool_message)
                
            elif tool_to_use == "database_query_tool":
                result = await database_query_tool.ainvoke({"question": query, "memory_info": memory_info})
                
                # å­˜å‚¨æ•°æ®åº“ç»“æœ
                state["db_results"] = result.get("results", [])
                state["sql_query"] = result.get("sql_query")
                
                # åˆ›å»ºå·¥å…·æ¶ˆæ¯
                tool_message = AIMessage(content=f"æ•°æ®åº“æŸ¥è¯¢ç»“æœï¼š\n{result['content']}")
                messages.append(tool_message)
                
            elif tool_to_use == "both":
                # åŒæ—¶æ‰§è¡Œä¸¤ä¸ªå·¥å…·
                logger.info("Executing both RAG and database tools")
                
                # æ‰§è¡ŒRAGæœç´¢
                rag_result = await rag_search_tool.ainvoke({"query": query, "memory_info": memory_info})
                state["rag_results"] = rag_result.get("rag_results", [])
                rag_message = AIMessage(content=f"RAGæœç´¢ç»“æœï¼š\n{rag_result['content']}")
                messages.append(rag_message)
                
                if "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ä¿¡æ¯" in rag_result['content']:
                    # æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢
                    db_result = await database_query_tool.ainvoke({"question": query, "memory_info": memory_info})
                    state["db_results"] = db_result.get("results", [])
                    state["sql_query"] = db_result.get("sql_query")
                    db_message = AIMessage(content=f"æ•°æ®åº“æŸ¥è¯¢ç»“æœï¼š\n{db_result['content']}")
                    messages.append(db_message)
                    
                    logger.info("Both tools executed successfully")
                else:
                    tool_to_use = "rag_search_tool"
                    logger.info("RAG search successfully, skipping database query")
                
            else:
                tool_message = AIMessage(content=f"æœªçŸ¥å·¥å…·: {tool_to_use}")
                messages.append(tool_message)
            
            logger.debug(f"Tool {tool_to_use} execution completed")
            
            return {
                **state,
                "tool_to_use": tool_to_use,
                "messages": messages
            }
            
        except Exception as e:
            logger.error(f"Tool execution error: {e}")
            error_message = AIMessage(content=f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}")
            return {
                **state,
                "messages": [*state["messages"], error_message]
            }
    
    
    async def validate_results(self, state: AgentState) -> AgentState:
        """éªŒè¯å·¥å…·æ‰§è¡Œç»“æœçš„æœ‰æ•ˆæ€§ - ä½¿ç”¨LLMæ™ºèƒ½åˆ¤æ–­"""
        tool_to_use = state.get("tool_to_use")
        
        if tool_to_use in ["database_query_tool", "both"]:
            # è·å–æ•°æ®åº“æŸ¥è¯¢ç»“æœ
            db_results = state.get("db_results", [])
            sql_query = state.get("sql_query")
            query = state["query"]
            
            # è·å–RAGç»“æœï¼ˆå¦‚æœä½¿ç”¨äº†bothå·¥å…·ï¼‰
            rag_results = state.get("rag_results", [])
            memory_info = state.get("memory_info", {})
            
            # æ„å»ºè®©LLMåˆ¤æ–­å’Œæ ¼å¼åŒ–çš„æç¤º
            validation_prompt = f"""è¯·åˆ†æä»¥ä¸‹å·¥å…·æ‰§è¡Œç»“æœï¼Œåˆ¤æ–­æŸ¥è¯¢æ˜¯å¦æˆåŠŸã€‚å¦‚æœæŸ¥è¯¢ç»“æœæˆåŠŸï¼Œç›´æ¥æ ¼å¼åŒ–ç»“æœã€‚å¦‚æœå¤±è´¥ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨å‘é‡æœç´¢å¢å¼ºã€‚

ç¬¬ä¸€æ­¥éœ€è¦ç†è§£ç”¨æˆ·çš„é—®é¢˜ï¼Œæ ¹æ®ä¸‹é¢çš„æ­¥éª¤ç†è§£ç”¨æˆ·çš„é—®é¢˜ã€‚
1. å…ˆæ£€æŸ¥ç”¨æˆ·é—®é¢˜æ˜¯å¦éœ€è¦å†å²è®°å¿†æ¥å›ç­”  
2. å¦‚æœéœ€è¦ï¼Œæ‰¾åˆ°ä¸é—®é¢˜æœ€ç›¸å…³çš„è®°å¿†å¹¶å¼•ç”¨  
3. å†æ ¹æ®å½“å‰è¾“å…¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ

ç”¨æˆ·å¯¹è¯å†å²ï¼š{memory_info}
ç”¨æˆ·é—®é¢˜: {query}

ç”Ÿæˆçš„SQL: {sql_query if sql_query else "æ— "}

æ•°æ®åº“æŸ¥è¯¢ç»“æœ: {json.dumps(db_results, ensure_ascii=False, indent=2) if db_results else "ç©ºç»“æœ"}

RAGæœç´¢ç»“æœ: {"æ‰¾åˆ°ç›¸å…³æ–‡æ¡£" if rag_results else "æ— RAGç»“æœ"}

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å¤„ç†ï¼š

1. åˆ†ææŸ¥è¯¢ç»“æœï¼š
   - SQLæŸ¥è¯¢æ˜¯å¦æˆåŠŸç”Ÿæˆï¼Ÿï¼ˆæ˜¯/å¦ï¼‰
   - æ•°æ®åº“æŸ¥è¯¢æ˜¯å¦è¿”å›äº†æœ‰æ•ˆæ•°æ®ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰
   - å¦‚æœæ²¡æœ‰è¿”å›æ•°æ®ï¼Œæ˜¯å¦åº”è¯¥ä½¿ç”¨å‘é‡æœç´¢æ¥æ‰¾åˆ°æ­£ç¡®çš„æ•°æ®åº“å€¼ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰
   - å¦‚æœå‘é‡æœç´¢ä¹Ÿä¸é€‚ç”¨ï¼Œæ˜¯å¦åº”è¯¥ä½¿ç”¨RAGæ–‡æ¡£æœç´¢ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆï¼Ÿï¼ˆæ˜¯/å¦ï¼‰

2. å¦‚æœæ•°æ®åº“æŸ¥è¯¢è¿”å›äº†æœ‰æ•ˆæ•°æ®ï¼Œè¯·ç›´æ¥æ ¼å¼åŒ–ç»“æœï¼š
   - ç”¨è‡ªç„¶è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜
   - å¦‚æœæŸ¥è¯¢ç»“æœåŒ…å«COUNT(*)æˆ–æ•°é‡ç»Ÿè®¡ï¼Œè¯·æ˜ç¡®è¯´å‡ºå…·ä½“çš„æ•°å­—
   - å¯¹äºæ•°æ®åˆ—è¡¨ï¼Œç”¨æ¸…æ™°çš„æ–¹å¼å±•ç¤ºå…³é”®ä¿¡æ¯
   - å¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œåªæ˜¾ç¤ºå‰å‡ æ¡è®°å½•å¹¶è¯´æ˜æ€»æ•°
   - ä¿æŒå›ç­”ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡º

åˆ¤æ–­æ ‡å‡†ï¼š
- å¦‚æœSQLç”ŸæˆæˆåŠŸä½†è¿”å›0æ¡è®°å½•ï¼Œå¾ˆå¯èƒ½æ˜¯æŸ¥è¯¢æ¡ä»¶ä¸­çš„å€¼ä¸å‡†ç¡®ï¼ˆå¦‚é¡¹ç›®åç§°æ‹¼å†™é”™è¯¯ï¼‰ï¼Œåº”è¯¥ä½¿ç”¨å‘é‡æœç´¢
- å¦‚æœSQLç”Ÿæˆå¤±è´¥æˆ–æŸ¥è¯¢å‡ºé”™ï¼Œåº”è¯¥ä½¿ç”¨RAGä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
- å¦‚æœæŸ¥è¯¢è¿”å›äº†æ•°æ®ï¼Œåˆ™è®¤ä¸ºæˆåŠŸï¼Œä¸éœ€è¦é¢å¤–æ“ä½œ
- å¦‚æœä½¿ç”¨äº†bothå·¥å…·ä¸”RAGæœ‰ç»“æœï¼Œä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ç»“æœï¼ŒRAGä½œä¸ºè¡¥å……

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
SQLç”ŸæˆæˆåŠŸ: [æ˜¯/å¦]
æœ‰æ•ˆæ•°æ®: [æ˜¯/å¦]  
éœ€è¦å‘é‡æœç´¢: [æ˜¯/å¦]
éœ€è¦RAGå¤‡ç”¨: [æ˜¯/å¦]
åŸå› : [ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±]
æ ¼å¼åŒ–ç»“æœ: [å¦‚æœæœ‰æ•ˆæ•°æ®ï¼Œè¯·ç›´æ¥ç»™å‡ºæ ¼å¼åŒ–çš„å›ç­”ï¼Œå¦åˆ™å†™"æ— "]"""

            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºLLMMessage
            llm_messages = [
                LLMMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“æŸ¥è¯¢ç»“æœåˆ†æåŠ©æ‰‹ï¼Œéœ€è¦å‡†ç¡®åˆ¤æ–­æŸ¥è¯¢ç»“æœçš„æœ‰æ•ˆæ€§ã€‚"),
                LLMMessage(role="user", content=validation_prompt)
            ]
            
            try:
                response = await llm_service.chat(llm_messages)
                response_text = response.content.lower()
                
                # è§£æLLMçš„åˆ¤æ–­ç»“æœ
                has_valid_sql = "sqlç”ŸæˆæˆåŠŸ: æ˜¯" in response_text or "sqlç”ŸæˆæˆåŠŸï¼šæ˜¯" in response_text
                has_valid_data = "æœ‰æ•ˆæ•°æ®: æ˜¯" in response_text or "æœ‰æ•ˆæ•°æ®ï¼šæ˜¯" in response_text
                needs_vector = "éœ€è¦å‘é‡æœç´¢: æ˜¯" in response_text or "éœ€è¦å‘é‡æœç´¢ï¼šæ˜¯" in response_text
                needs_rag = "éœ€è¦ragå¤‡ç”¨: æ˜¯" in response_text or "éœ€è¦ragå¤‡ç”¨ï¼šæ˜¯" in response_text
                
                # æå–åŸå› 
                reason_match = response_text.find("åŸå› :")
                if reason_match == -1:
                    reason_match = response_text.find("åŸå› ï¼š")
                reason = response_text[reason_match:].split('\n')[0] if reason_match != -1 else "æœªæä¾›åŸå› "
                
                # æå–æ ¼å¼åŒ–ç»“æœ
                formatted_db_results = None
                format_match = response_text.find("æ ¼å¼åŒ–ç»“æœ:")
                if format_match == -1:
                    format_match = response_text.find("æ ¼å¼åŒ–ç»“æœï¼š")
                
                if format_match != -1:
                    # æå–æ ¼å¼åŒ–ç»“æœéƒ¨åˆ†ï¼ˆåŒ…æ‹¬å¤šè¡Œï¼‰
                    format_text = response_text[format_match:]
                    if "æ ¼å¼åŒ–ç»“æœ:" in format_text:
                        formatted_db_results = format_text.split("æ ¼å¼åŒ–ç»“æœ:")[1].strip()
                    elif "æ ¼å¼åŒ–ç»“æœï¼š" in format_text:
                        formatted_db_results = format_text.split("æ ¼å¼åŒ–ç»“æœï¼š")[1].strip()
                    
                    # å¦‚æœæ ¼å¼åŒ–ç»“æœæ˜¯"æ— "æˆ–ç©ºï¼Œåˆ™è®¾ä¸ºNone
                    if formatted_db_results in ["æ— ", "", "æ— æ•°æ®", "æ— ç»“æœ"]:
                        formatted_db_results = None
                
                # å¦‚æœæ²¡æœ‰ä»LLMè·å–åˆ°æ ¼å¼åŒ–ç»“æœï¼Œä½†æœ‰æœ‰æ•ˆæ•°æ®ï¼Œä½¿ç”¨ç®€å•æ ¼å¼åŒ–
                if has_valid_data and db_results and not formatted_db_results:
                    formatted_db_results = self._format_database_response(db_results, sql_query)
                
                logger.info(f"LLM validation result: sql_valid={has_valid_sql}, data_valid={has_valid_data}, needs_vector={needs_vector}, needs_rag={needs_rag}")
                logger.info(f"Validation reason: {reason}")
                logger.info(f"Formatted results: {'Yes' if formatted_db_results else 'No'}")
                
                return {
                    **state,
                    "db_results_valid": has_valid_data,
                    "needs_vector_search": needs_vector,  # Changed from needs_vector_search to match what we check
                    "needs_fallback": needs_rag,
                    "fallback_executed": False,
                    "validation_reason": reason,
                    "formatted_db_results": formatted_db_results
                }
                
            except Exception as e:
                logger.error(f"LLM validation error: {e}, falling back to simple check")
                # å¦‚æœLLMåˆ¤æ–­å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„å¤‡ç”¨é€»è¾‘
                # æ£€æŸ¥æ˜¯å¦æœ‰SQLå’Œç»“æœ
                has_sql = bool(sql_query)
                has_results = False
                
                # å°è¯•å¤šç§æ–¹å¼æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
                if db_results:
                    if isinstance(db_results, list) and len(db_results) > 0:
                        has_results = True
                    elif isinstance(db_results, dict) and db_results:
                        has_results = True
                    elif isinstance(db_results, str) and db_results.strip():
                        has_results = True
                
                # ç®€å•åˆ¤æ–­é€»è¾‘
                needs_vector_search = has_sql and not has_results and state.get("intent") == QueryType.DATABASE
                needs_fallback = not has_sql or (not has_results and state.get("intent") != QueryType.DATABASE)
                
                # If database results are valid, format them using simple formatting
                formatted_db_results = None
                if has_sql and has_results and db_results:
                    formatted_db_results = self._format_database_response(db_results, sql_query)
                    logger.info("Database results formatted using simple formatting (fallback)")
                
                logger.info(f"Fallback validation: has_sql={has_sql}, has_results={has_results}, needs_vector={needs_vector_search}, needs_fallback={needs_fallback}")
                
                return {
                    **state,
                    "db_results_valid": has_sql and has_results,
                    "needs_vector_search": needs_vector_search,
                    "needs_fallback": needs_fallback,
                    "fallback_executed": False,
                    "validation_reason": "ä½¿ç”¨å¤‡ç”¨éªŒè¯é€»è¾‘",
                    "formatted_db_results": formatted_db_results
                }
        else:
            # RAGæœç´¢æˆ–å…¶ä»–æƒ…å†µï¼Œç›´æ¥è®¤ä¸ºæœ‰æ•ˆ
            return {
                **state,
                "db_results_valid": True,
                "needs_vector_search": False,
                "needs_fallback": False,
                "fallback_executed": False,
                "validation_reason": "RAGæœç´¢ï¼Œæ— éœ€æ•°æ®åº“éªŒè¯"
            }

    async def vector_search_enhancement(self, state: AgentState) -> AgentState:
        """ä½¿ç”¨å‘é‡æœç´¢å¢å¼ºæ•°æ®åº“æŸ¥è¯¢"""
        query = state["query"]
        failed_sql = state.get("sql_query")
        memory_info = state.get("memory_info", "")

        logger.info(f"Starting vector search enhancement for query: {query}")
        
        try:
            # è°ƒç”¨å‘é‡æ•°æ®åº“æŸ¥è¯¢å·¥å…·
            result = await vector_database_query_tool.ainvoke({
                "question": query,
                "failed_sql": failed_sql,
                "memory_info": memory_info
            })
            
            # æ£€æŸ¥å‘é‡æœç´¢æ˜¯å¦æˆåŠŸ
            # æ³¨æ„ï¼šå¯¹äºCOUNTæŸ¥è¯¢ï¼Œå³ä½¿ç»“æœæ˜¯0ä¹Ÿæ˜¯æˆåŠŸçš„æŸ¥è¯¢
            vector_success = result.get("success", False)
            
            if vector_success:
                # å‘é‡æœç´¢æˆåŠŸï¼Œæ›´æ–°çŠ¶æ€
                results = result.get("results", [])
                logger.info(f"Vector search successful, found {len(results)} results")
                
                # åˆ›å»ºå·¥å…·æ¶ˆæ¯ï¼Œæ˜¾ç¤ºå…·ä½“çš„æŸ¥è¯¢ç»“æœ
                tool_message = AIMessage(
                    content=f"å‘é‡æœç´¢å¢å¼ºç»“æœï¼š\n{result.get('content', '')}"
                )
                
                return {
                    **state,
                    "db_results": results,
                    "sql_query": result.get("sql_query"),
                    "vector_enhanced": True,
                    "suggestions_used": result.get("suggestions_used", {}),
                    "db_results_valid": True,  # å³ä½¿COUNTæ˜¯0ï¼Œä¹Ÿæ˜¯æœ‰æ•ˆçš„æŸ¥è¯¢ç»“æœ
                    "messages": [*state["messages"], tool_message]
                }
            else:
                # å‘é‡æœç´¢å¤±è´¥ï¼ˆæ¯”å¦‚SQLç”Ÿæˆå¤±è´¥ï¼‰
                logger.warning("Vector search enhancement failed")
                
                no_result_message = AIMessage(
                    content="å‘é‡æœç´¢æœªèƒ½æ‰¾åˆ°åŒ¹é…çš„æ•°æ®åº“å€¼ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥æŸ¥è¯¢æ¡ä»¶"
                )
                
                # å¦‚æœå‘é‡æœç´¢ä¹Ÿå¤±è´¥ï¼Œå¯èƒ½éœ€è¦å°è¯•RAG
                return {
                    **state,
                    "vector_enhanced": False,
                    "needs_fallback": True,  # å°è¯•RAGä½œä¸ºæœ€åçš„æ‰‹æ®µ
                    "messages": [*state["messages"], no_result_message]
                }
                
        except Exception as e:
            logger.error(f"Vector search enhancement error: {e}")
            error_message = AIMessage(content=f"å‘é‡æœç´¢å¢å¼ºæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            
            return {
                **state,
                "vector_enhanced": False,
                "needs_fallback": True,  # å‡ºé”™æ—¶å°è¯•RAG
                "messages": [*state["messages"], error_message]
            }
    
    async def fallback_search(self, state: AgentState) -> AgentState:
        """å½“æ•°æ®åº“æŸ¥è¯¢æ— æ•ˆç»“æœæ—¶ï¼Œæ‰§è¡ŒRAGæœç´¢ä½œä¸ºfallback"""
        query = state["query"]
        memory_info = state.get("memory_info", "")
        
        logger.info(f"Executing RAG fallback search for query: {query}")
        
        try:
            # æ‰§è¡ŒRAGæœç´¢
            result = await rag_search_tool.ainvoke({"query": query, "memory_info": memory_info})
            
            # å­˜å‚¨RAGç»“æœ
            rag_results = result.get("rag_results", [])
            source_links = result.get("source_links", [])
            
            # æ£€æŸ¥RAGæ˜¯å¦æœ‰æœ‰æ•ˆç»“æœ
            has_rag_results = bool(result.get("content") and result["content"].strip())
            
            if has_rag_results:
                # æ ‡è®°ä¸ºæ··åˆç»“æœï¼ˆåŒ…å«SQLå’ŒRAGï¼‰
                has_mixed_results = bool(state.get("sql_query"))
                
                # åˆ›å»ºå·¥å…·æ¶ˆæ¯
                tool_message = AIMessage(content=f"RAGæœç´¢ç»“æœï¼ˆä½œä¸ºè¡¥å……ï¼‰ï¼š\n{result['content']}")
                
                logger.info(f"Fallback search successful: found RAG answer")
                
                return {
                    **state,
                    "rag_results": rag_results,
                    "source_links": source_links,
                    "fallback_executed": True,
                    "has_mixed_results": has_mixed_results,
                    "messages": [*state["messages"], tool_message]
                }
            else:
                # RAGä¹Ÿæ²¡æœ‰æ‰¾åˆ°ç»“æœ
                no_result_message = AIMessage(content="RAGæœç´¢ä¹Ÿæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
                
                logger.warning("Fallback search found no results")
                
                return {
                    **state,
                    "fallback_executed": True,
                    "has_mixed_results": False,
                    "messages": [*state["messages"], no_result_message]
                }
                
        except Exception as e:
            logger.error(f"Fallback search error: {e}")
            error_message = AIMessage(content=f"è¡¥å……æœç´¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {
                **state,
                "fallback_executed": True,
                "messages": [*state["messages"], error_message]
            }
    
    async def generate_response(self, state: AgentState) -> AgentState:
        """ç”Ÿæˆæœ€ç»ˆå›ç­”"""
        # æ£€æŸ¥å·¥å…·æ‰§è¡Œç»“æœç±»å‹
        tool_to_use = state.get("tool_to_use")
        rag_results = state.get("rag_results", [])
        db_results = state.get("db_results", [])
        sql_query = state.get("sql_query")
        
        # å¦‚æœåªæœ‰RAGç»“æœï¼Œç›´æ¥è¿”å›RAGç­”æ¡ˆ
        if tool_to_use == "rag_search_tool" and rag_results:
            # ä»RAGå·¥å…·æ¶ˆæ¯ä¸­æå–ç­”æ¡ˆ
            rag_answer = ""
            for msg in state["messages"]:
                if isinstance(msg, AIMessage) and "RAGæœç´¢ç»“æœ" in msg.content:
                    # æå–RAGç­”æ¡ˆï¼ˆå»æ‰"RAGæœç´¢ç»“æœï¼š"å‰ç¼€ï¼‰
                    content = msg.content
                    if "RAGæœç´¢ç»“æœï¼š\n" in content:
                        rag_answer = content.split("RAGæœç´¢ç»“æœï¼š\n", 1)[1]
                    else:
                        rag_answer = content
                    break
            
            if rag_answer:
                logger.info("Returning RAG answer directly")
                ai_message = AIMessage(content=rag_answer)
                return {
                    **state,
                    "final_answer": rag_answer,
                    "messages": [*state["messages"], ai_message]
                }
        
        # å¦‚æœåªæœ‰æ•°æ®åº“ç»“æœï¼Œä½¿ç”¨é¢„æ ¼å¼åŒ–çš„ç»“æœæˆ–æ ¼å¼åŒ–æ•°æ®
        elif tool_to_use == "database_query_tool" and db_results:
            # ä¼˜å…ˆä½¿ç”¨LLMé¢„æ ¼å¼åŒ–çš„ç»“æœ
            if state.get("formatted_db_results"):
                formatted_answer = state["formatted_db_results"]
                logger.info("Returning LLM-formatted database response")
            else:
                # å›é€€åˆ°ç®€å•æ ¼å¼åŒ–
                formatted_answer = self._format_database_response(db_results, sql_query)
                logger.info("Returning simple formatted database response")
            
            ai_message = AIMessage(content=formatted_answer)
            return {
                **state,
                "final_answer": formatted_answer,
                "messages": [*state["messages"], ai_message]
            }
        
        # å¦‚æœæœ‰æ··åˆç»“æœï¼Œä¼˜å…ˆä½¿ç”¨RAGç­”æ¡ˆ
        elif state.get("has_mixed_results", False) and rag_results:
            # ä»RAGå·¥å…·æ¶ˆæ¯ä¸­æå–ç­”æ¡ˆ
            rag_answer = ""
            for msg in state["messages"]:
                if isinstance(msg, AIMessage) and "RAGæœç´¢ç»“æœ" in msg.content:
                    content = msg.content
                    if "RAGæœç´¢ç»“æœï¼š\n" in content:
                        rag_answer = content.split("RAGæœç´¢ç»“æœï¼š\n", 1)[1]
                    else:
                        rag_answer = content
                    break
            
            if rag_answer:
                logger.info("Returning RAG answer from mixed results")
                ai_message = AIMessage(content=rag_answer)
                return {
                    **state,
                    "final_answer": rag_answer,
                    "messages": [*state["messages"], ai_message]
                }
            else:
                return {
                    **state,
                    "final_answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯",
                    "messages": [*state["messages"], AIMessage(content="æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯")]
                }
    
    def _format_database_response(self, db_results: List[Dict], sql_query: str = None) -> str:
        """æ ¼å¼åŒ–æ•°æ®åº“æŸ¥è¯¢ç»“æœ"""
        if not db_results:
            return "æœªæ‰¾åˆ°ç›¸å…³æ•°æ®"
        
        # æ„å»ºæ ¼å¼åŒ–å›ç­”
        answer_parts = []
        
        # æ·»åŠ ç»“æœç»Ÿè®¡
        result_count = len(db_results)
        answer_parts.append(f"æŸ¥è¯¢ç»“æœï¼šå…±æ‰¾åˆ° {result_count} æ¡è®°å½•\n")
        
        # æ·»åŠ ç»“æœæ•°æ®
        if result_count > 0:
            answer_parts.append("æ•°æ®è¯¦æƒ…ï¼š")
            
            # æ˜¾ç¤ºå‰10æ¡è®°å½•
            display_count = min(10, result_count)
            for i, row in enumerate(db_results[:display_count], 1):
                if isinstance(row, dict):
                    # æ ¼å¼åŒ–å­—å…¸æ•°æ®
                    row_str = ", ".join([f"{k}: {v}" for k, v in row.items()])
                    answer_parts.append(f"{i}. {row_str}")
                else:
                    # æ ¼å¼åŒ–å…¶ä»–ç±»å‹æ•°æ®
                    answer_parts.append(f"{i}. {row}")
            
            if result_count > display_count:
                answer_parts.append(f"... è¿˜æœ‰ {result_count - display_count} æ¡è®°å½•")
        
        return "\n".join(answer_parts)
    
    async def save_memory(self, state: AgentState) -> AgentState:
        """Save interaction to memory system"""
        user_email = state.get("user_email")
        session_id = state.get("session_id")
        query = state["query"]
        final_answer = state.get("final_answer", "")
        
        if not user_email:
            logger.info("No user email provided, skipping memory save")
            return state
        
        try:
            query_history_id = await self._save_query_history(state)
            # Extract and store memories from this interaction
            if query_history_id and final_answer:
                memory_ids = await memory_service.extract_and_store_memories(
                    query_id=query_history_id,
                    user_email=user_email,
                    question=query,
                    answer=final_answer,
                    session_id=session_id,
                    feedback_type=None  # Will be updated when user provides feedback
                )
                logger.info(f"Saved {len(memory_ids)} memories for interaction")
            
            # Periodically consolidate memories (every 10th query)
            if query_history_id and query_history_id % 10 == 0:
                consolidation_result = await memory_service.consolidate_memories(
                    user_email=user_email,
                    session_id=session_id
                )
                logger.info(f"Memory consolidation result: {consolidation_result}")
        except Exception as e:
            logger.error(f"Failed to save memory: {e}")
        
        return state

    async def _save_query_history(self, state: AgentState) -> int:
        """
        Save query history to database
        
        Args:
            state: The agent state containing all query information
            
        Returns:
            int: The query history ID if successful, None if failed
        """
        try:
            # Extract data from agent state
            query = state.get("query", "")
            user_email = state.get("user_email")
            session_id = state.get("session_id", "default")
            answer = state.get("final_answer", "")
            sql_query = state.get("sql_query")
            query_type = state.get("intent")
            rag_results = state.get("rag_results", [])
            
            # Determine success based on whether we have an answer
            success = bool(answer and answer.strip())
            
            # Get LLM info from agent if available
            llm_provider = ""
            model_name = ""
            if hasattr(self, 'llm_provider') and self.llm_provider:
                llm_provider = str(self.llm_provider.value if hasattr(self.llm_provider, 'value') else self.llm_provider)
            if hasattr(self, 'model_name') and self.model_name:
                model_name = self.model_name
            
            # Calculate processing time (simplified - could be enhanced with actual timing)
            processing_time_ms = (time.time() - state.get("start_time")) * 1000  # Default value, could be calculated from actual start/end times

            # å¤„ç†RAGç»“æœï¼Œæå–æ–‡æ¡£ä¿¡æ¯å¹¶å»é‡
            unique_documents = {}
            if rag_results is not None:
                for result in rag_results:
                    if hasattr(result, 'metadata') and result.metadata:
                        doc_id = result.metadata.get('document_id')
                        if doc_id and doc_id not in unique_documents:
                            unique_documents[doc_id] = {
                                "document_id": doc_id,
                                "document_name": result.metadata.get('document_name', ''),
                                "dataset_name": result.metadata.get('dataset_name', '')
                            }
            
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            sources = list(unique_documents.values())

            # Save to history
            history = await history_service.save_query_history(
                session_id=session_id,
                user_email=user_email,
                question=query,
                answer=answer,
                sql_query=sql_query,
                sources=sources,
                query_type=query_type,
                success=success,
                processing_time_ms=processing_time_ms,
                llm_provider=llm_provider,
                model_name=model_name
            )
            
            logger.info(f"Saved query history with ID: {history.id}")
            return history.id
            
        except Exception as e:
            logger.error(f"Failed to save query history: {e}")
            return None
    
    async def query(self, question: str, context: Optional[str] = None, user_email: Optional[str] = None, session_id: Optional[str] = None) -> QueryResponse:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        if not self.llm_available or not self.workflow:
            return QueryResponse(
                answer="æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®",
                query_type=QueryType.HYBRID,
                confidence=0.0
            )
            
        try:
            # åˆå§‹çŠ¶æ€
            initial_state = {
                "messages": [HumanMessage(content=question)],
                "query": question,
                "intent": None,
                "rag_results": None,
                "db_results": None,
                "sql_query": None,
                "source_links": None,
                "final_answer": None,
                "reasoning": None,
                "needs_tools": False,
                "tool_to_use": None,
                # æ–°å¢å­—æ®µçš„é»˜è®¤å€¼
                "db_results_valid": False,
                "needs_fallback": False,
                "needs_vector_search": False,  # Added
                "fallback_executed": False,
                "has_mixed_results": False,
                "vector_enhanced": None,  # Added
                "suggestions_used": None,  # Added
                "validation_reason": None,  # Added
                # Memory-related fields
                "memory_info": None,
                "user_email": user_email,
                "session_id": session_id,
                # Data formatting fields
                "formatted_db_results": None
            }
            
            # è¿è¡Œå·¥ä½œæµ
            final_state = await self.workflow.ainvoke(
                initial_state,
                config=RunnableConfig(recursion_limit=10)
            )
            
            # ç¡®ä¿æœ‰æœ‰æ•ˆçš„ç­”æ¡ˆ
            answer = final_state.get("final_answer")
            if not answer or answer.strip() == "":
                # å¦‚æœæ²¡æœ‰æœ€ç»ˆç­”æ¡ˆï¼Œå°è¯•ä»æ¶ˆæ¯ä¸­è·å–æœ€åä¸€ä¸ªAIæ¶ˆæ¯
                messages = final_state.get("messages", [])
                for msg in reversed(messages):
                    if isinstance(msg, AIMessage) and msg.content and msg.content.strip():
                        answer = msg.content.strip()
                        break
                
                if not answer or answer.strip() == "":
                    answer = "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›ç­”"
            
            # æ„å»ºå¢å¼ºçš„å“åº”
            response = QueryResponse(
                answer=answer,
                query_type=final_state.get("intent", QueryType.HYBRID),
                reasoning=final_state.get("reasoning"),
                confidence=0.8
            )
            
            # æ·»åŠ SQLæŸ¥è¯¢ä¿¡æ¯ï¼ˆå¦‚æœæ¥è‡ªæ•°æ®åº“ï¼‰
            if final_state.get("sql_query"):
                response.sql_query = final_state["sql_query"]
            
            # æ·»åŠ RAGæºæ–‡ä»¶ä¿¡æ¯ï¼ˆå¦‚æœæ¥è‡ªRAGï¼‰
            if final_state.get("rag_results"):
                # å¤„ç†RAGç»“æœï¼Œæå–æ–‡æ¡£ä¿¡æ¯å¹¶å»é‡
                unique_documents = {}
                for result in final_state["rag_results"]:
                    if hasattr(result, 'metadata') and result.metadata:
                        doc_id = result.metadata.get('document_id')
                        if doc_id and doc_id not in unique_documents:
                            unique_documents[doc_id] = {
                                "document_id": doc_id,
                                "document_name": result.metadata.get('document_name', ''),
                                "dataset_name": result.metadata.get('dataset_name', '')
                            }
                
                # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
                response.sources = list(unique_documents.values())
            
            logger.info(f"Query completed successfully. SQL: {bool(response.sql_query)}, Sources: {len(response.sources) if response.sources else 0}")
            
            return response
            
        except Exception as e:
            logger.error(f"Agent query error: {e}")
            return QueryResponse(
                answer=f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                query_type=QueryType.HYBRID,
                confidence=0.0
            )

# å…¨å±€å®ä¾‹
kangni_agent = KangniReActAgent()