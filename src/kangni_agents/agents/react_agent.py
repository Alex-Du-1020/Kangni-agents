from typing import Dict, Any, List, Optional, TypedDict, Annotated
from langgraph.graph import StateGraph, END
from langgraph.graph import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
import time

from ..config import settings
from ..models import QueryType, RAGSearchResult, QueryResponse
from ..models.llm_implementations import llm_service
from ..models.llm_providers import LLMMessage
from ..services.rag_service import rag_service
from ..services.database_service import db_service
from ..services.vector_embedding_service import vector_service
from ..services.history_service import history_service
from ..services.memory_service import memory_service
from ..utils.intent_classifier import intent_classifier

import logging
import json

logger = logging.getLogger(__name__)

class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    query: str
    original_query: Optional[str]  # Store original question for history
    rewritten_query: Optional[str]  # Store memory-enhanced question
    intent: Optional[QueryType]
    # RAG-related fields
    rag_results: Optional[List[RAGSearchResult]]
    rag_content: Optional[str]
    rag_has_results: bool
    # Database-related fields (also used for vector search results)
    db_results: Optional[Dict[str, Any]]
    sql_query: Optional[str]
    db_success: bool
    db_results_valid: bool
    needs_vector_search: bool
    formatted_db_results: Optional[str]
    result_too_large: bool
    retry_attempted: bool
    # Response fields
    final_answer: Optional[str]
    source_links: Optional[List[str]]
    # Memory-related fields
    memory_info: Optional[str]
    user_email: Optional[str]
    session_id: Optional[str]
    start_time: Optional[float]

@tool
async def rag_search_tool(query: str) -> Dict[str, Any]:
    """æœç´¢RAGæ–‡æ¡£åº“èŽ·å–ç›¸å…³ä¿¡æ¯ - æ”¯æŒå¤šä¸ªæ•°æ®é›†"""
    # ç›´æŽ¥ä½¿ç”¨RAGæœåŠ¡ï¼Œå®ƒå†…éƒ¨ä¼šå¤„ç†å¤šä¸ªæ•°æ®é›†
    result = await rag_service.search_rag_with_answer(query, top_k=8)
    return result

@tool 
async def database_query_tool(question: str) -> Dict[str, Any]:
    """æŸ¥è¯¢æ•°æ®åº“èŽ·å–ç»Ÿè®¡ä¿¡æ¯"""
    # ç›´æŽ¥æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢
    result = await db_service.query_database(question)
    
    # è¿”å›žæ ¼å¼åŒ–ç»“æžœ
    if result.get("success"):
        return format_db_results(result)
    else:
        return {
            "content": f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}",
            "sql_query": result.get("sql_query"),
            "results": [],
            "success": False,
            "error": result.get("error")
        }

@tool
async def vector_database_query_tool(question: str, failed_sql: str = None) -> Dict[str, Any]:
    """ä½¿ç”¨å‘é‡æœç´¢å¢žå¼ºæ•°æ®åº“æŸ¥è¯¢ï¼Œæ‰¾åˆ°å®žé™…å­˜åœ¨çš„å€¼å¹¶é‡æ–°ç”ŸæˆSQL"""
    import yaml
    from pathlib import Path
    from ..utils.sql_parser import SQLParser
    
    logger.info(f"Starting vector-enhanced database query for: {question}")
    
    try:
        # Load vector search configuration
        config_path = Path(__file__).parent.parent / "config" / "vector_search_config.yaml"
        vector_config = {}
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                vector_config = yaml.safe_load(f)
        else:
            logger.warning("Vector search config not found, using defaults")
            return {
                "content": "å‘é‡æœç´¢é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°",
                "success": False,
                "error": "Missing vector_search_config.yaml"
            }
        
        # Parse the failed SQL to identify tables and fields
        sql_parser = SQLParser()
        parsed_sql = {}
        if failed_sql:
            parsed_sql = sql_parser.parse_sql(failed_sql)
            logger.info(f"Parsed SQL - tables: {parsed_sql.get('tables')}, fields: {parsed_sql.get('fields')}")
        
        # Collect suggestions for all relevant fields
        all_suggestions = {}
        vector_fields = vector_config.get('vector_search_fields', [])
        
        for field_config in vector_fields:
            table = field_config['table']
            field = field_config['field']
            description = field_config.get('description', field)
            
            # Check if this field is relevant to the query
            should_search = False
            
            # If we have a parsed SQL, check if the table/field is mentioned
            if parsed_sql:
                if table in parsed_sql.get('tables', []) and field in parsed_sql.get('fields', []):
                    should_search = True
                    logger.info(f"Field {table}.{field} found in failed SQL")
            
            # Also check if keywords from the question match this field's keywords
            keywords = field_config.get('keywords', [])
            question_lower = question.lower()
            for keyword in keywords:
                if keyword.lower() in question_lower:
                    should_search = True
                    logger.info(f"Keyword '{keyword}' found in question for field {table}.{field}")
                    break
            
            if should_search:
                logger.info(f"Searching for similar values in {table}.{field} ({description})")
                
                # Get similar values from the database
                suggestions = await vector_service.search_similar_values(
                    search_text=question,
                    table_name=table,
                    field_name=field,
                    top_k=settings.max_suggestions or 3,  # This is already the max_suggestions value
                    similarity_threshold=settings.similarity_threshold or 0.3  # This is already the threshold value
                )
                
                if suggestions:
                    # Limit suggestions to max_suggestions
                    limited_suggestions = suggestions[:settings.max_suggestions]
                    # Extract just the field values for display
                    field_values = [suggestion['field_value'] for suggestion in limited_suggestions]
                    logger.info(f"Found {len(limited_suggestions)} suggestions for {description}: {field_values[:3]}...")
                    all_suggestions[f"{table}.{field}"] = {
                        "values": field_values,
                        "description": description,
                        "table": table,
                        "field": field
                    }
        
        if not all_suggestions:
            logger.info("No vector search suggestions found")
            return {
                "content": "æœªæ‰¾åˆ°ç›¸ä¼¼çš„æ•°æ®åº“å€¼",
                "success": False,
                "suggestions": {},
                "message": "å‘é‡æœç´¢æœªæ‰¾åˆ°åŒ¹é…çš„å€¼"
            }
        
        # Build enhanced prompt with suggestions
        suggestion_text = "\n\nåŸºäºŽå‘é‡æœç´¢æ‰¾åˆ°çš„æ•°æ®åº“å®žé™…å€¼ï¼š\n"
        for field_key, field_data in all_suggestions.items():
            suggestion_text += f"- {field_data['description']} ({field_data['table']}.{field_data['field']}): "
            suggestion_text += f"{', '.join(field_data['values'])}"
            if len(field_data['values']) > 3:
                suggestion_text += f" ç­‰{len(field_data['values'])}ä¸ªå€¼"
            suggestion_text += "\n"
        
        enhanced_question = f"{question}{suggestion_text}\nè¯·ä½¿ç”¨è¿™äº›å®žé™…å­˜åœ¨çš„å€¼é‡æ–°ç”ŸæˆSQLæŸ¥è¯¢ã€‚å¦‚æžœæœ‰å¤šä¸ªåŒ¹é…å€¼ï¼Œä½¿ç”¨æœ€ç›¸ä¼¼çš„å€¼æ¥æŸ¥è¯¢ã€‚"
        
        # Generate new SQL with enhanced context
        logger.info("Generating new SQL with vector search suggestions")
        enhanced_result = await db_service.query_database(enhanced_question)
        
        if enhanced_result.get("success"):
            logger.info(f"Vector-enhanced query successful, got {len(enhanced_result.get('results', []))} results")
            enhanced_result["vector_enhanced"] = True
            enhanced_result["suggestions_used"] = all_suggestions
            return format_db_results(enhanced_result)
        else:
            logger.warning(f"Vector-enhanced query failed: {enhanced_result.get('error')}")
            return {
                "content": f"å‘é‡å¢žå¼ºæŸ¥è¯¢å¤±è´¥: {enhanced_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                "success": False,
                "suggestions": all_suggestions,
                "sql_query": enhanced_result.get("sql_query"),
                "error": enhanced_result.get("error")
            }
            
    except Exception as e:
        logger.error(f"Error in vector database query tool: {e}")
        return {
            "content": f"å‘é‡æœç´¢å‡ºé”™: {str(e)}",
            "success": False,
            "error": str(e)
        }

def format_db_results(result: Dict[str, Any]) -> Dict[str, Any]:
    """æ ¼å¼åŒ–æ•°æ®åº“æŸ¥è¯¢ç»“æžœ"""
    # å¤„ç†æ—¥æœŸåºåˆ—åŒ–é—®é¢˜
    def serialize_dates(obj):
        """é€’å½’å¤„ç†å¯¹è±¡ä¸­çš„æ—¥æœŸç±»åž‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
        if hasattr(obj, 'isoformat'):  # datetime, date å¯¹è±¡
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {key: serialize_dates(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [serialize_dates(item) for item in obj]
        else:
            return obj
    
    # åºåˆ—åŒ–ç»“æžœä¸­çš„æ—¥æœŸå¯¹è±¡
    serialized_results = serialize_dates(result.get("results", []))
    
    success = False
    try:
        success = True
        formatted_content = json.dumps({
            "sql_query": result.get("sql_query"),
            "results": serialized_results,
            "vector_enhanced": result.get("vector_enhanced", False),
            "suggestions_used": result.get("suggestions_used", [])
        }, ensure_ascii=False, indent=2)
    except (TypeError, ValueError) as e:
        # å¦‚æžœä»ç„¶æœ‰åºåˆ—åŒ–é—®é¢˜ï¼Œä½¿ç”¨æ›´å®‰å…¨çš„æ ¼å¼åŒ–æ–¹æ³•
        logger.warning(f"JSON serialization failed, using fallback formatting: {e}")
        formatted_content = f"SQLæŸ¥è¯¢: {result.get('sql_query', 'N/A')}\n"
        formatted_content += f"ç»“æžœæ•°é‡: {len(serialized_results)}\n"
        formatted_content += "ç»“æžœæ•°æ®:\n"
        for i, row in enumerate(serialized_results[:5], 1):  # åªæ˜¾ç¤ºå‰5è¡Œ
            formatted_content += f"  {i}. {row}\n"
        if len(serialized_results) > 5:
            formatted_content += f"  ... è¿˜æœ‰ {len(serialized_results) - 5} è¡Œæ•°æ®"
    
    return {
        "success": success,
        "content": formatted_content,
        "sql_query": result.get("sql_query"),
        "results": serialized_results,
        "vector_enhanced": result.get("vector_enhanced", False),
        "suggestions_used": result.get("suggestions_used", [])
    }

class KangniReActAgent:
    def __init__(self):
        # ä½¿ç”¨é›†ä¸­å¼LLMæœåŠ¡
        self.llm_available = llm_service.llm_available
        self.llm_provider = llm_service.llm_provider
        
        if self.llm_available:
            try:
                # ç»‘å®šå·¥å…· - æ·»åŠ vector_database_query_tool
                self.tools = [rag_search_tool, database_query_tool, vector_database_query_tool]
                
                # æž„å»ºçŠ¶æ€å›¾
                self.workflow = self._build_workflow()

                # ä¿å­˜çŠ¶æ€å›¾çš„å¯è§†åŒ–è¡¨ç¤º
                self._save_graph_visualization()
                
                logger.info(f"Agent initialized successfully with LLM provider: {self.llm_provider}")
                
            except Exception as e:
                logger.warning(f"Failed to initialize agent: {e}")
                self.llm_available = False
                self.workflow = None
        else:
            logger.warning("LLM service not available, agent features will be disabled")
            self.workflow = None
    
    def _save_graph_visualization(self, filename: str = "docs/workflow_graph.png") -> None:
        """ä¿å­˜çŠ¶æ€å›¾çš„å¯è§†åŒ–è¡¨ç¤º
        
        Args:
            filename: ä¿å­˜æ–‡ä»¶è·¯å¾„
        """
        if not self.workflow:
            logger.warning("No workflow to visualize")
            return
            
        try:
            # å°è¯•ç”Ÿæˆ Mermaid å›¾
            graph = self.workflow.get_graph()
            
            # é¦–å…ˆå°è¯•ä¿å­˜ä¸º PNGï¼ˆéœ€è¦ graphvizï¼‰
            try:
                with open(filename, "wb") as f:
                    f.write(graph.draw_mermaid_png())
                logger.info(f"Graph visualization saved as PNG: {filename}")
                print(f"âœ… Workflow graph saved as: {filename}")
            except Exception as png_error:
                logger.warning(f"Could not save as PNG (graphviz may not be installed): {png_error}")
                
                # é€€è€Œæ±‚å…¶æ¬¡ï¼Œä¿å­˜ä¸º Mermaid æ–‡æœ¬æ ¼å¼
                mermaid_filename = filename.replace('.png', '.mermaid')
                try:
                    mermaid_text = graph.draw_mermaid()
                    with open(mermaid_filename, "w", encoding="utf-8") as f:
                        f.write(mermaid_text)
                    logger.info(f"Graph saved as Mermaid text: {mermaid_filename}")
                    print(f"âœ… Workflow graph saved as Mermaid text: {mermaid_filename}")
                    print(f"   You can visualize it at: https://mermaid.live/")
                    
                    # åŒæ—¶æ‰“å°å›¾å½¢ç»“æž„
                    print("\nðŸ“Š Workflow Structure:")
                    print(mermaid_text)
                except Exception as mermaid_error:
                    logger.error(f"Could not save Mermaid text: {mermaid_error}")
                    
                    # æœ€åŽçš„å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰“å°èŠ‚ç‚¹å’Œè¾¹
                    print("\nðŸ“Š Workflow Nodes and Edges:")
                    print(f"Nodes: {graph.nodes}")
                    print(f"Edges: {graph.edges}")
                    
        except Exception as e:
            logger.error(f"Failed to save graph visualization: {e}")
            print(f"âŒ Could not visualize workflow: {e}")
    
    def _build_workflow(self) -> StateGraph:
        """æž„å»ºLangGraphå·¥ä½œæµ - æ–°æµç¨‹ï¼š1.åŠ è½½è®°å¿† 2.æ”¹å†™é—®é¢˜ 3.RAGæœç´¢ 4.æ•°æ®åº“æŸ¥è¯¢ 5.å‘é‡æœç´¢ 6.ç”Ÿæˆå“åº”"""
        workflow = StateGraph(AgentState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("load_memory", self.load_memory_info)
        workflow.add_node("rewrite_question", self.rewrite_question_with_memory)
        workflow.add_node("rag_search", self.execute_rag_search)
        workflow.add_node("check_rag_results", self.check_rag_results)
        workflow.add_node("database_query", self.execute_database_query)
        workflow.add_node("validate_db_results", self.validate_database_results)
        workflow.add_node("database_retry", self.execute_database_retry)
        workflow.add_node("validate_retry_results", self.validate_database_results)
        workflow.add_node("vector_search", self.execute_vector_search)
        workflow.add_node("validate_vector_results", self.validate_database_results)
        workflow.add_node("generate_response", self.generate_response)
        workflow.add_node("save_memory", self.save_memory)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("load_memory")
        
        # æ·»åŠ è¾¹
        workflow.add_edge("load_memory", "rewrite_question")
        workflow.add_edge("rewrite_question", "rag_search")
        workflow.add_edge("rag_search", "check_rag_results")
        
        # æ¡ä»¶è¾¹ï¼šæ£€æŸ¥RAGç»“æžœ
        def check_rag_condition(state: AgentState) -> str:
            has_rag_results = state.get("rag_has_results", False)
            logger.info(f"Checking RAG results: {has_rag_results}")
            
            if has_rag_results:
                logger.info("RAG has results, routing to generate_response")
                return "generate_response"
            else:
                logger.info("No RAG results, routing to database_query")
                return "database_query"
        
        workflow.add_conditional_edges(
            "check_rag_results",
            check_rag_condition,
            {
                "generate_response": "generate_response",
                "database_query": "database_query"
            }
        )
        
        workflow.add_edge("database_query", "validate_db_results")
        
        # æ¡ä»¶è¾¹ï¼šæ£€æŸ¥æ•°æ®åº“ç»“æžœ
        def check_db_condition(state: AgentState) -> str:
            needs_vector_search = state.get("needs_vector_search", False)
            db_results = state.get("db_results", [])
            result_too_large = state.get("result_too_large", False)
            
            logger.info(f"Checking database results - needs_vector: {needs_vector_search}, result_too_large: {result_too_large}")
            
            # å¦‚æžœç»“æžœå¤ªå¤§ï¼Œå…ˆå°è¯•é‡è¯•ä¼˜åŒ–SQL
            if result_too_large and not state.get("retry_attempted", False):
                logger.info("Result too large, routing to database_retry")
                return "database_retry"
            # å¦‚æžœä¸éœ€è¦å‘é‡æœç´¢ï¼Œç›´æŽ¥ç”Ÿæˆå“åº”
            elif not needs_vector_search:
                logger.info("Database has valid results, routing to generate_response")
                return "generate_response"
            # å¦åˆ™è¿›è¡Œå‘é‡æœç´¢
            else:
                logger.info("No valid database results, routing to vector_search")
                return "vector_search"
        
        workflow.add_conditional_edges(
            "validate_db_results",
            check_db_condition,
            {
                "generate_response": "generate_response",
                "database_retry": "database_retry",
                "vector_search": "vector_search"
            }
        )
        
        workflow.add_edge("database_retry", "validate_retry_results")
        workflow.add_edge("validate_retry_results", "generate_response")
        workflow.add_edge("vector_search", "validate_vector_results")
        workflow.add_edge("validate_vector_results", "generate_response")
        workflow.add_edge("generate_response", "save_memory")
        workflow.add_edge("save_memory", END)
        
        return workflow.compile()
    
    async def load_memory_info(self, state: AgentState) -> AgentState:
        """Load memory context for the user"""
        user_email = state.get("user_email")
        session_id = state.get("session_id")
        query = state["query"]
        
        logger.info(f"Loading memory context for user: {user_email}, session: {session_id}")
        
        # Get memory context from memory service
        memory_info = ""
        if user_email:
            try:
                memory_context = await memory_service.get_memory_context_for_agent(
                    user_email=user_email,
                    question=query,
                    session_id=session_id
                )

                # Build memory context string
                memory_info = ""
                if memory_context:
                    # Add recent interactions
                    # recent_interactions = memory_context.get("recent_interactions", [])
                    # if recent_interactions:
                    #     memory_info += "\næœ€è¿‘çš„äº¤äº’åŽ†å²:\n"
                    #     for interaction in recent_interactions[:3]:
                    #         memory_info += f"- Q: {interaction['question']}...\n"
                    #         if interaction.get('answer'):
                    #             memory_info += f"  A: {interaction['answer']}...\n"
                    
                    # # Add long-term memories
                    # long_term = memory_context.get("long_term_memories", [])
                    # if long_term:
                    #     memory_info += "\nç›¸å…³çš„é•¿æœŸè®°å¿†:\n"
                    #     for mem in long_term[:3]:
                    #         memory_info += f"- {mem['content'][:150]}... (é‡è¦æ€§: {mem.get('importance', 'unknown')})\n"
                    
                    # Add short-term memories
                    short_term = memory_context.get("short_term_memories", [])
                    if short_term:
                        memory_info += "\nä¼šè¯ä¸Šä¸‹æ–‡:\n"
                        for mem in short_term:
                            memory_info += f"- {mem['content']}...\n"

                logger.info(f"Loaded memory context with {len(memory_context.get('short_term_memories', []))} short-term and {len(memory_context.get('long_term_memories', []))} long-term memories")
            except Exception as e:
                logger.error(f"Failed to load memory context: {e}")
        
        return {
            **state,
            "start_time": time.time(),
            "memory_info": memory_info
        }
    
    async def rewrite_question_with_memory(self, state: AgentState) -> AgentState:
        """Rewrite question based on memory context to improve search accuracy"""
        original_query = state["query"]
        memory_info = state.get("memory_info", "")
        
        logger.info(f"Rewriting question with memory context: {original_query}")
        
        # If no memory context available, use original question
        if not memory_info or memory_info.strip() == "":
            logger.info("No memory context available, using original question")
            return {
                **state,
                "original_query": original_query,
                "rewritten_query": original_query
            }
        
        try:
            # Use LLM to rewrite question based on memory context
            rewrite_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹è®°å¿†ä¸Šä¸‹æ–‡ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦ç»“åˆè®°å¿†æ”¹å†™ç”¨æˆ·é—®é¢˜ï¼Œå¹¶åœ¨éœ€è¦æ—¶è¿›è¡ŒæŒ‡ä»£æ¶ˆè§£ä¸Žä¿¡æ¯è¡¥å…¨ï¼Œä½¿é—®é¢˜æ›´å…·ä½“ã€å¯æ£€ç´¢æ€§æ›´å¼ºã€‚

åŽŸå§‹é—®é¢˜ï¼š{original_query}

è®°å¿†ä¸Šä¸‹æ–‡ï¼š
{memory_info}

ä¸€ã€æ˜¯å¦éœ€è¦ç»“åˆè®°å¿†çš„åˆ¤æ–­æ ‡å‡†ï¼ˆæ»¡è¶³ä»¥ä¸‹ä»»ä¸€å¼ºæŒ‡ä»£ï¼Œå¿…é¡»ä½¿ç”¨è®°å¿†ï¼›æ»¡è¶³å¼±ä¿¡å·â‰¥2é¡¹ï¼Œå»ºè®®ä½¿ç”¨è®°å¿†ï¼‰ï¼š
ã€å¼ºæŒ‡ä»£ä¿¡å·ã€‘ï¼ˆä»»ä¸€å³ä¸ºâ€œéœ€è¦â€ï¼‰
1) å‡ºçŽ°ä»£è¯/æŒ‡ç¤ºè¯æˆ–æŒ‡å‘æ€§è¯è¯­ï¼Œä¸”ç¼ºå°‘è¢«æŒ‡ä»£å¯¹è±¡ï¼šå¦‚â€œä»–/å¥¹/å®ƒ/ä»–ä»¬/è¿™äº›/é‚£äº›/æ­¤/è¯¥/è¿™/é‚£/ä¸Šè¿°/å‰æ–‡/å‰é¢/ä¸Šé¢/ä¹‹å‰/åˆšæ‰/åŒä¸Š/è¯¥é—®é¢˜/è¿™ä¸ªæƒ…å†µ/é‚£éƒ¨åˆ†/è¿™é‡Œ/é‚£é‡Œ/ä¸Šè¿°æƒ…å†µ/è¯¥ç»“è®º/ä¸Šè¿°æ–‡ä»¶/è¿™ä¸¤ä¸ªâ€ç­‰ã€‚
2) è¯­ä¹‰æ˜¾å¼å¼•ç”¨ä¸Šä¸€æ­¥/ä¸Šæ–‡/å…ˆå‰å›žç­”ï¼šå¦‚â€œç»§ç»­â€â€œè¿˜æ˜¯é‚£ä¸ªâ€â€œæ¢ä¸€ç§æ–¹æ³•â€â€œå†è§£é‡Šä¸€ä¸‹â€â€œåŒå‰â€â€œæŒ‰åˆšæ‰çš„æ–¹æ³•â€â€œåŸºäºŽä¸Šé¢çš„ç»“æžœâ€â€œä¸Šä¸€ä¸ªç­”æ¡ˆé‡Œæåˆ°çš„â€¦â€¦â€ã€‚
3) æ—¶é—´/åºåˆ—æŒ‡ä»£ï¼šå¦‚â€œåˆšæ‰â€â€œä¸Šæ¬¡â€â€œä¹‹å‰â€â€œéšåŽâ€â€œåŽç»­â€â€œç»§ç»­ç¬¬2æ­¥â€â€œå†æ¥ä¸€æ¬¡â€ã€‚
4) éœ€è¦ä»Žå…ˆå‰ç­”æ¡ˆä¸­å–å®žä½“/æ•°å€¼/ç»“è®ºæ‰èƒ½å®Œæ•´è¡¨è¾¾ï¼ˆå¯ä¸ŽçŸ­æœŸè®°å¿†ä¸­çš„å®žä½“åè¯ã€æ•°å€¼ã€æ–‡ä»¶åã€æ•°æ®é›†åç­‰å½¢æˆæ˜Žç¡®æ˜ å°„ï¼‰ã€‚

ã€å¼±ä¿¡å·ã€‘ï¼ˆåŒæ—¶å‘½ä¸­â‰¥2é¡¹åˆ™â€œå»ºè®®â€ä½¿ç”¨è®°å¿†ï¼‰
5) é¢†åŸŸè¿žç»­ä½†ä¿¡æ¯ä¸å…¨ï¼šä¸»é¢˜æ˜Žæ˜¾å»¶ç»­ä¸Šè½®é¢†åŸŸ/é¡¹ç›®/æ•°æ®é›†ï¼Œä½†ç¼ºå°‘é™å®šè¯ï¼ˆå¦‚æ˜Žç¡®çš„å¯¹è±¡ã€å‚æ•°ã€ç‰ˆæœ¬ï¼‰ã€‚
6) é—®é¢˜æžçŸ­æˆ–æ¨¡ç³Šï¼šé•¿åº¦å¾ˆçŸ­æˆ–ä»…ä¸ºåè¯çŸ­è¯­/ç‰‡æ®µï¼ˆå¦‚â€œå†æ¥ä¸€ä¸ªç»“è®ºâ€â€œæŠ¥å‘Šé‡Œé‚£ä¸ªå›¾â€ï¼‰ã€‚
7) ä¸ŽçŸ­æœŸè®°å¿†å­˜åœ¨æ˜¾è‘—å…³é”®è¯é‡å æˆ–è¯­ä¹‰ç›¸ä¼¼ï¼ˆå¯ä¾æ®å…³é”®è¯é‡å ä½œä¸ºè¿‘ä¼¼åˆ¤æ–­ï¼‰ã€‚

äºŒã€ä½¿ç”¨è®°å¿†æ—¶çš„æ”¹å†™ç­–ç•¥
1) è¿›è¡ŒæŒ‡ä»£æ¶ˆè§£ï¼šå°†ä¸Šè¿°ä»£è¯/æŒ‡ç¤ºè¯ç”¨è®°å¿†ä¸­çš„æ˜Žç¡®å®žä½“ã€æ–‡ä»¶åã€æ•°æ®é›†åã€æ•°å€¼æˆ–ç»“è®ºæ›¿æ¢ã€‚
2) ä¿¡æ¯è¡¥å…¨ï¼šè¡¥å……å¿…è¦çš„é™å®šä¿¡æ¯ï¼ˆå¦‚ç‰ˆæœ¬ã€èŒƒå›´ã€å¯¹è±¡ã€æ—¶é—´ã€æ•°æ®é›†ï¼‰ä»¥å½¢æˆç‹¬ç«‹å¯æ£€ç´¢çš„é—®é¢˜ã€‚
3) ç›¸å…³æ€§çº¦æŸï¼šä»…å¼•å…¥ä¸Žå½“å‰é—®é¢˜å¼ºç›¸å…³çš„è®°å¿†ä¿¡æ¯ï¼Œé¿å…æ— å…³æ‰©å±•ã€‚
4) è‡ªç„¶æ€§ï¼šä¿æŒè‡ªç„¶è¯­è¨€è¡¨è¾¾ï¼Œä¸æ”¹å˜åŽŸå§‹æ„å›¾ã€‚

ä¸‰ã€æ— éœ€è®°å¿†æ—¶çš„å¤„ç†
1) è‹¥ä¸Šè¿°æ¡ä»¶å‡ä¸æ»¡è¶³ï¼Œåˆ™ä¸å¼•å…¥è®°å¿†ï¼Œç›´æŽ¥è¿”å›žåŽŸå§‹é—®é¢˜æˆ–å…¶ç­‰ä»·çš„æ¸…æ™°è¡¨è¿°ã€‚

è¾“å‡ºè¦æ±‚ï¼š
1) åªè¿”å›žæ”¹å†™åŽçš„æœ€ç»ˆé—®é¢˜ï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæˆ–ç†ç”±ã€‚
2) è‹¥æ— éœ€è®°å¿†ä¸”åŽŸå§‹é—®é¢˜å·²è¶³å¤Ÿæ¸…æ™°ï¼Œç›´æŽ¥åŽŸæ ·è¿”å›žã€‚
"""

            llm_messages = [
                LLMMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªé—®é¢˜æ”¹å†™ä¸“å®¶ï¼Œä¸“é—¨æ ¹æ®è®°å¿†ä¸Šä¸‹æ–‡ä¼˜åŒ–é—®é¢˜ä»¥æé«˜æœç´¢å‡†ç¡®æ€§ã€‚"),
                LLMMessage(role="user", content=rewrite_prompt)
            ]
            
            response = await llm_service.chat(llm_messages)
            rewritten_query = response.content.strip()
            
            # æ¸…ç†å¯èƒ½çš„markdownæ ¼å¼
            if rewritten_query.startswith("```"):
                rewritten_query = rewritten_query.split("\n", 1)[1] if "\n" in rewritten_query else rewritten_query[3:]
            if rewritten_query.endswith("```"):
                rewritten_query = rewritten_query.rsplit("\n", 1)[0] if "\n" in rewritten_query else rewritten_query[:-3]
            
            rewritten_query = rewritten_query.strip()
            
            # å¦‚æžœæ”¹å†™å¤±è´¥æˆ–ç»“æžœä¸ºç©ºï¼Œä½¿ç”¨åŽŸå§‹é—®é¢˜
            if not rewritten_query or rewritten_query == "":
                logger.warning("Question rewriting failed or returned empty, using original question")
                rewritten_query = original_query
            
            logger.info(f"Question rewritten: '{original_query}' -> '{rewritten_query}'")
            
            return {
                **state,
                "original_query": original_query,
                "rewritten_query": rewritten_query
            }
            
        except Exception as e:
            logger.error(f"Failed to rewrite question: {e}")
            # Fallback to original question
            return {
                **state,
                "original_query": original_query,
                "rewritten_query": original_query
            }
    
    async def execute_rag_search(self, state: AgentState) -> AgentState:
        """æ‰§è¡ŒRAGæœç´¢"""
        # Use rewritten query if available, otherwise fallback to original
        query = state.get("rewritten_query") or state["query"]
        
        logger.info(f"Executing RAG search for query: {query}")
        
        try:
            result = await rag_search_tool.ainvoke({"query": query})
            
            # å­˜å‚¨RAGç»“æžœ
            rag_results = result.get("rag_results", [])
            content = result.get("content", "")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆç»“æžœ - æ›´ä¸¥æ ¼çš„æ¡ä»¶
            has_results = bool(
                content and 
                content.strip() and 
                "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ä¿¡æ¯" not in content and
                "æ‰¾ä¸åˆ°" not in content and
                "æ²¡æœ‰æ‰¾åˆ°" not in content and
                "æ— æ³•æ‰¾åˆ°" not in content and
                "æ²¡æœ‰ç›¸å…³" not in content and
                "æ— ç›¸å…³ä¿¡æ¯" not in content and
                "æœªåŒ…å«æ‰€éœ€ä¿¡æ¯" not in content and
                len(content.strip()) > 50  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å®¹
            )
            
            # åˆ›å»ºå·¥å…·æ¶ˆæ¯
            tool_message = AIMessage(content=f"RAGæœç´¢ç»“æžœï¼š\n{content}")
            
            logger.info(f"RAG search completed. Has results: {has_results}")
            
            return {
                **state,
                "rag_results": rag_results,
                "rag_content": content,
                "rag_has_results": has_results,
                "messages": [*state["messages"], tool_message]
            }
            
        except Exception as e:
            logger.error(f"RAG search error: {e}")
            error_message = AIMessage(content=f"RAGæœç´¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {
                **state,
                "rag_has_results": False,
                "messages": [*state["messages"], error_message]
            }
    
    async def check_rag_results(self, state: AgentState) -> AgentState:
        """æ£€æŸ¥RAGæœç´¢ç»“æžœ"""
        # è¿™ä¸ªèŠ‚ç‚¹ä¸»è¦ç”¨äºŽè·¯ç”±å†³ç­–ï¼ŒçŠ¶æ€å·²ç»åœ¨execute_rag_searchä¸­è®¾ç½®
        return state
    
    async def execute_database_query(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢"""
        # Use rewritten query if available, otherwise fallback to original
        query = state.get("rewritten_query") or state["query"]
        
        logger.info(f"Executing database query for: {query}")
        
        try:
            result = await database_query_tool.ainvoke({"question": query})
            
            # å­˜å‚¨æ•°æ®åº“ç»“æžœ
            db_results = result.get("results", [])
            sql_query = result.get("sql_query")
            success = result.get("success", False)
            
            # åˆ›å»ºå·¥å…·æ¶ˆæ¯
            tool_message = AIMessage(content=f"æ•°æ®åº“æŸ¥è¯¢ç»“æžœï¼š\n{result['content']}")
            
            logger.info(f"Database query completed. Success: {success}, Results count: {len(db_results) if db_results else 0}")
            
            return {
                **state,
                "db_results": db_results,
                "sql_query": sql_query,
                "db_success": success,
                "messages": [*state["messages"], tool_message]
            }
            
        except Exception as e:
            logger.error(f"Database query error: {e}")
            error_message = AIMessage(content=f"æ•°æ®åº“æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {
                **state,
                "db_success": False,
                "messages": [*state["messages"], error_message]
            }
    
    async def validate_database_results(self, state: AgentState) -> AgentState:
        """éªŒè¯å·¥å…·æ‰§è¡Œç»“æžœçš„æœ‰æ•ˆæ€§ - ä½¿ç”¨LLMæ™ºèƒ½åˆ¤æ–­"""

        db_results = state.get("db_results", [])
        sql_query = state.get("sql_query")
        query = state.get("rewritten_query") or state["query"]

        # æ£€æŸ¥ç»“æžœå¤§å°æ˜¯å¦è¿‡å¤§ï¼ˆä»…åœ¨åˆæ¬¡éªŒè¯æ—¶æ£€æŸ¥ï¼‰
        if not state.get("retry_attempted", False):
            result_too_large = self._check_result_size(db_results)
            if result_too_large:
                logger.info("Result too large, routing to database_retry")
                return {
                    **state,
                    "result_too_large": result_too_large,
                    "retry_attempted": False,  # æ ‡è®°ä¸ºéœ€è¦é‡è¯•
                }
            
        # å®‰å…¨åœ°åºåˆ—åŒ–æ•°æ®åº“ç»“æžœï¼Œé¿å…JSONè½¬ä¹‰é—®é¢˜ï¼ˆæ”¯æŒ Decimal / datetime / date ç­‰ï¼‰
        def safe_serialize_results(results):
            if not results:
                return "ç©ºç»“æžœ"
            try:
                def default_serializer(obj):
                    # datetime/date å¯¹è±¡
                    if hasattr(obj, "isoformat"):
                        return obj.isoformat()
                    # å…¶ä»–å¦‚ Decimal ç­‰å¯¹è±¡
                    return str(obj)
                # ä½¿ç”¨json.dumpsç¡®ä¿æ­£ç¡®çš„JSONæ ¼å¼ï¼Œå¹¶å¤„ç†ç‰¹æ®Šç±»åž‹
                json_str = json.dumps(results, ensure_ascii=False, indent=2, default=default_serializer)
                return json_str
            except (TypeError, ValueError) as e:
                # å¦‚æžœJSONåºåˆ—åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²è¡¨ç¤º
                logger.warning(f"JSON serialization failed: {e}, using fallback")
                if isinstance(results, list):
                    return f"ç»“æžœåˆ—è¡¨ï¼Œå…±{len(results)}æ¡è®°å½•"
                elif isinstance(results, dict):
                    return f"ç»“æžœå­—å…¸ï¼ŒåŒ…å«{len(results)}ä¸ªå­—æ®µ"
                else:
                    return str(results)

        # æž„å»ºè®©LLMåˆ¤æ–­å’Œæ ¼å¼åŒ–çš„æç¤º
        validation_prompt = f"""è¯·åˆ†æžä»¥ä¸‹å·¥å…·æ‰§è¡Œç»“æžœï¼Œåˆ¤æ–­æŸ¥è¯¢æ˜¯å¦æˆåŠŸã€‚å¦‚æžœæŸ¥è¯¢ç»“æžœæˆåŠŸï¼Œç›´æŽ¥æ ¼å¼åŒ–ç»“æžœã€‚

ç”¨æˆ·é—®é¢˜: {query}

ç”Ÿæˆçš„SQL: {sql_query if sql_query else "æ— "}

æ•°æ®åº“æŸ¥è¯¢ç»“æžœ: {safe_serialize_results(db_results)}

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å¤„ç†ï¼š

1. åˆ†æžæŸ¥è¯¢ç»“æžœï¼š
   - SQLæŸ¥è¯¢æ˜¯å¦æˆåŠŸç”Ÿæˆï¼Ÿï¼ˆæ˜¯/å¦ï¼‰
   - æ•°æ®åº“æŸ¥è¯¢æ˜¯å¦è¿”å›žäº†æœ‰æ•ˆæ•°æ®ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰
   - å¦‚æžœSQLæˆåŠŸä½†è¿”å›ž0æ¡è®°å½•ï¼Œæˆ–è€…count(*)ä¸º0ï¼Œè¯´æ˜Žç”¨æˆ·è¾“å…¥å¯èƒ½ä¸å¤Ÿå‡†ç¡®ï¼Œéœ€è¦å‘é‡æœç´¢æ¥æ‰¾åˆ°æ­£ç¡®çš„æ•°æ®åº“å€¼ï¼ˆæ˜¯/å¦ï¼‰

2. å¦‚æžœæ•°æ®åº“æŸ¥è¯¢è¿”å›žäº†æœ‰æ•ˆæ•°æ®ï¼Œè¯·ç›´æŽ¥æ ¼å¼åŒ–ç»“æžœï¼š
   - ç”¨è‡ªç„¶è¯­è¨€å›žç­”ç”¨æˆ·çš„é—®é¢˜
   - å¦‚æžœæŸ¥è¯¢ç»“æžœåŒ…å«COUNT(*)æˆ–æ•°é‡ç»Ÿè®¡ï¼Œè¯·æ˜Žç¡®è¯´å‡ºå…·ä½“çš„æ•°å­—
   - å¯¹äºŽæ•°æ®åˆ—è¡¨ï¼Œç”¨æ¸…æ™°çš„æ–¹å¼å±•ç¤ºå…³é”®ä¿¡æ¯
   - å¦‚æžœæ•°æ®é‡å¾ˆå¤§ï¼Œåªæ˜¾ç¤ºå‰å‡ æ¡è®°å½•å¹¶è¯´æ˜Žæ€»æ•°
   - ä¿æŒå›žç­”ç®€æ´æ˜Žäº†ï¼Œé‡ç‚¹çªå‡º

åˆ¤æ–­æ ‡å‡†ï¼š
- å¦‚æžœSQLç”ŸæˆæˆåŠŸä½†è¿”å›ž0æ¡è®°å½•ï¼Œåˆ™è®¤ä¸ºå¤±è´¥
- å¦‚æžœSQLè¿è¡ŒæŠ¥é”™ï¼Œåˆ™è®¤ä¸ºå¤±è´¥
- å¦‚æžœæŸ¥è¯¢è¿”å›žäº†æ•°æ®ï¼Œåˆ™è®¤ä¸ºæˆåŠŸï¼Œä¸éœ€è¦é¢å¤–æ“ä½œ

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼å›žç­”ï¼š
SQLç”ŸæˆæˆåŠŸ: [æ˜¯/å¦]
æœ‰æ•ˆæ•°æ®: [æ˜¯/å¦]
éœ€è¦å‘é‡æœç´¢: [å¦‚æžœSQLæˆåŠŸä½†æ— æ•°æ®ï¼Œæˆ–ç”¨æˆ·è¾“å…¥ä¸å¤Ÿå‡†ç¡®ï¼Œåˆ™å¡«"æ˜¯"ï¼Œå¦åˆ™å¡«"å¦"]
æ ¼å¼åŒ–ç»“æžœ: [å¦‚æžœæœ‰æ•ˆæ•°æ®ï¼Œè¯·ç›´æŽ¥ç»™å‡ºæ ¼å¼åŒ–çš„å›žç­”ï¼Œå¦åˆ™å†™"æ— "]"""

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºLLMMessage
        llm_messages = [
            LLMMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“æŸ¥è¯¢ç»“æžœåˆ†æžåŠ©æ‰‹ï¼Œéœ€è¦å‡†ç¡®åˆ¤æ–­æŸ¥è¯¢ç»“æžœçš„æœ‰æ•ˆæ€§ã€‚"),
            LLMMessage(role="user", content=validation_prompt)
        ]
        
        # ä¸è®¾ç½®max_tokensï¼Œè®©æœåŠ¡å™¨ä½¿ç”¨é»˜è®¤å€¼
        response = await llm_service.chat(llm_messages)
        response_text = response.content.lower()
        
        # è§£æžLLMçš„åˆ¤æ–­ç»“æžœ
        has_valid_sql = "sqlç”ŸæˆæˆåŠŸ: æ˜¯" in response_text or "sqlç”ŸæˆæˆåŠŸï¼šæ˜¯" in response_text
        has_valid_data = "æœ‰æ•ˆæ•°æ®: æ˜¯" in response_text or "æœ‰æ•ˆæ•°æ®ï¼šæ˜¯" in response_text
        needs_vector = "éœ€è¦å‘é‡æœç´¢: æ˜¯" in response_text or "éœ€è¦å‘é‡æœç´¢ï¼šæ˜¯" in response_text
        
        # é¢å¤–æ£€æŸ¥ï¼šå¦‚æžœSQLæˆåŠŸä½†ç»“æžœä¸ºç©ºï¼Œä¹Ÿéœ€è¦å‘é‡æœç´¢
        result_count = len(db_results) if db_results else 0
        sql_successful_but_empty = has_valid_sql and result_count == 0

        
        # ç¡®å®šæ˜¯å¦éœ€è¦å‘é‡æœç´¢
        needs_vector_search = not has_valid_data or sql_successful_but_empty or needs_vector
              
        # æå–æ ¼å¼åŒ–ç»“æžœ
        formatted_db_results = None
        format_match = response_text.find("æ ¼å¼åŒ–ç»“æžœ:")
        if format_match == -1:
            format_match = response_text.find("æ ¼å¼åŒ–ç»“æžœï¼š")
        
        if format_match != -1:
            # æå–æ ¼å¼åŒ–ç»“æžœéƒ¨åˆ†ï¼ˆåŒ…æ‹¬å¤šè¡Œï¼‰
            format_text = response_text[format_match:]
            if "æ ¼å¼åŒ–ç»“æžœ:" in format_text:
                formatted_db_results = format_text.split("æ ¼å¼åŒ–ç»“æžœ:")[1].strip()
            elif "æ ¼å¼åŒ–ç»“æžœï¼š" in format_text:
                formatted_db_results = format_text.split("æ ¼å¼åŒ–ç»“æžœï¼š")[1].strip()
            
            # å¦‚æžœæ ¼å¼åŒ–ç»“æžœæ˜¯"æ— "æˆ–ç©ºï¼Œåˆ™è®¾ä¸ºNone
            if formatted_db_results in ["æ— ", "", "æ— æ•°æ®", "æ— ç»“æžœ"]:
                formatted_db_results = None
        
        # å¦‚æžœæ²¡æœ‰ä»ŽLLMèŽ·å–åˆ°æ ¼å¼åŒ–ç»“æžœï¼Œä½†æœ‰æœ‰æ•ˆæ•°æ®ï¼Œä½¿ç”¨ç®€å•æ ¼å¼åŒ–
        if has_valid_data and db_results and not formatted_db_results:
            formatted_db_results = self._format_database_response(db_results, sql_query)
        
        # å¯¹äºŽé‡è¯•åŽçš„ç»“æžœï¼Œä¸å†æ£€æŸ¥å¤§å°ï¼Œç›´æŽ¥ä½¿ç”¨LLMåˆ¤æ–­
        is_retry_validation = state.get("retry_attempted", False)
        
        logger.info(f"LLM validation result: sql_valid={has_valid_sql}, data_valid={has_valid_data}, result_count={result_count}, needs_vector_search={needs_vector_search}, is_retry={is_retry_validation}")
        logger.info(f"Formatted results: {'Yes' if formatted_db_results else 'No'}")
                
        return {
            **state,
            "db_results_valid": has_valid_data,
            "needs_vector_search": needs_vector_search,
            "result_too_large": False,  # é‡è¯•åŽä¸å†æ ‡è®°ä¸ºè¿‡å¤§
            "retry_attempted": is_retry_validation,  # ä¿æŒé‡è¯•çŠ¶æ€
            "formatted_db_results": formatted_db_results
        }
    
    async def execute_database_retry(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢é‡è¯• - ä¼˜åŒ–SQLä»¥å‡å°‘ç»“æžœå¤§å°"""
        # Use rewritten query if available, otherwise fallback to original
        query = state.get("rewritten_query") or state["query"]
        original_sql = state.get("sql_query")
        
        logger.info(f"Executing database retry for large results: {query}")
        
        try:
            # ä½¿ç”¨LLMä¼˜åŒ–SQL
            optimized_sql = await self._optimize_sql_for_large_results(original_sql, query)
            
            # ä½¿ç”¨ä¼˜åŒ–åŽçš„SQLé‡æ–°æŸ¥è¯¢æ•°æ®åº“
            logger.info(f"Retrying with optimized SQL: {optimized_sql}")
            db_results = await db_service.execute_sql_query(optimized_sql)
            
            if db_results is not None:
                logger.info(f"Database retry successful, got {len(db_results)} results")
                
                # æ ¼å¼åŒ–ç»“æžœ
                formatted_content = self._format_database_response(db_results, optimized_sql)
                
                # åˆ›å»ºå·¥å…·æ¶ˆæ¯
                tool_message = AIMessage(content=f"æ•°æ®åº“é‡è¯•ç»“æžœï¼ˆä¼˜åŒ–åŽï¼‰ï¼š\n{formatted_content}")
                
                return {
                    **state,
                    "db_results": db_results,
                    "sql_query": optimized_sql,
                    "db_success": True,
                    "retry_attempted": True,
                    "messages": [*state["messages"], tool_message]
                }
            else:
                logger.warning("Database retry failed: No results returned")
                error_message = AIMessage(content="æ•°æ®åº“é‡è¯•å¤±è´¥: æœªè¿”å›žç»“æžœ")
                return {
                    **state,
                    "db_success": False,
                    "retry_attempted": True,
                    "messages": [*state["messages"], error_message]
                }
                
        except Exception as e:
            logger.error(f"Database retry error: {e}")
            error_message = AIMessage(content=f"æ•°æ®åº“é‡è¯•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {
                **state,
                "db_success": False,
                "retry_attempted": True,
                "messages": [*state["messages"], error_message]
            }
    
    async def execute_vector_search(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œå‘é‡æœç´¢å¢žå¼º"""
        # Use rewritten query if available, otherwise fallback to original
        query = state.get("rewritten_query") or state["query"]
        failed_sql = state.get("sql_query")
        
        logger.info(f"Executing vector search enhancement for: {query}")
        
        try:
            result = await vector_database_query_tool.ainvoke({
                "question": query,
                "failed_sql": failed_sql or ""
            })
            
            # ä½¿ç”¨æ•°æ®åº“çŠ¶æ€å­—æ®µå­˜å‚¨å‘é‡æœç´¢ç»“æžœ
            db_success = result.get("success", False)
            results = result.get("results", [])
            
            # åˆ›å»ºå·¥å…·æ¶ˆæ¯
            tool_message = AIMessage(content=f"å‘é‡æœç´¢å¢žå¼ºç»“æžœï¼š\n{result.get('content', '')}")
            
            logger.info(f"Vector search completed. Success: {db_success}, Results count: {len(results) if results else 0}")
            
            return {
                **state,
                "db_results": results,
                "sql_query": result.get("sql_query"),
                "db_success": db_success,
                "messages": [*state["messages"], tool_message]
            }
            
        except Exception as e:
            logger.error(f"Vector search error: {e}")
            error_message = AIMessage(content=f"å‘é‡æœç´¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {
                **state,
                "db_success": False,
                "messages": [*state["messages"], error_message]
            }
    
    
    async def generate_response(self, state: AgentState) -> AgentState:
        """ç”Ÿæˆæœ€ç»ˆå›žç­” - æŒ‰ç…§æ–°æµç¨‹ï¼šRAG -> æ•°æ®åº“ -> å‘é‡æœç´¢"""
        rag_has_results = state.get("rag_has_results", False)
        db_results_valid = state.get("db_results_valid", False)
        formatted_db_results = state.get("formatted_db_results")
        original_query = state.get("original_query", state.get("query", ""))
        
        logger.info(f"Generating response for original query '{original_query}' - RAG: {rag_has_results}, DB: {db_results_valid}")
        
        # 1. å¦‚æžœRAGæœ‰ç»“æžœï¼Œç›´æŽ¥è¿”å›žRAGç­”æ¡ˆ
        if rag_has_results:
            rag_content = state.get("rag_content", "")
            if rag_content:
                logger.info("Returning RAG answer")
                ai_message = AIMessage(content=rag_content)
                return {
                    **state,
                    "final_answer": rag_content,
                    "messages": [*state["messages"], ai_message]
                }
        
        # 2. å¦‚æžœæ•°æ®åº“æŸ¥è¯¢æœ‰æœ‰æ•ˆç»“æžœï¼Œè¿”å›žæ ¼å¼åŒ–ç»“æžœ
        elif db_results_valid and formatted_db_results:
            logger.info("Returning formatted database/vector search answer")
            ai_message = AIMessage(content=formatted_db_results)
            return {
                **state,
                "final_answer": formatted_db_results,
                "messages": [*state["messages"], ai_message]
            }
        
        # 3. å¦‚æžœéƒ½æ²¡æœ‰ç»“æžœï¼Œè¿”å›žæ— æ³•æ‰¾åˆ°ä¿¡æ¯çš„æ¶ˆæ¯
        else:
            no_result_message = "æŠ±æ­‰ï¼Œæ— æ³•æ‰¾åˆ°ä¸Žæ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜æˆ–æä¾›æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚"
            logger.warning("No valid results found from any source")
            
            ai_message = AIMessage(content=no_result_message)
            return {
                **state,
                "final_answer": no_result_message,
                "messages": [*state["messages"], ai_message]
            }
    
    def _format_database_response(self, db_results: List[Dict], sql_query: str = None) -> str:
        """æ ¼å¼åŒ–æ•°æ®åº“æŸ¥è¯¢ç»“æžœ"""
        if not db_results:
            return "æœªæ‰¾åˆ°ç›¸å…³æ•°æ®"
        
        # æž„å»ºæ ¼å¼åŒ–å›žç­”
        answer_parts = []
        
        # æ·»åŠ ç»“æžœç»Ÿè®¡
        result_count = len(db_results)
        answer_parts.append(f"æŸ¥è¯¢ç»“æžœï¼šå…±æ‰¾åˆ° {result_count} æ¡è®°å½•\n")
        
        # æ·»åŠ ç»“æžœæ•°æ®
        if result_count > 0:
            answer_parts.append("æ•°æ®è¯¦æƒ…ï¼š")
            
            # æ˜¾ç¤ºå‰100æ¡è®°å½•
            display_count = min(100, result_count)
            for i, row in enumerate(db_results[:display_count], 1):
                if isinstance(row, dict):
                    # æ ¼å¼åŒ–å­—å…¸æ•°æ®
                    row_str = ", ".join([f"{k}: {v}" for k, v in row.items()])
                    answer_parts.append(f"{i}. {row_str}")
                else:
                    # æ ¼å¼åŒ–å…¶ä»–ç±»åž‹æ•°æ®
                    answer_parts.append(f"{i}. {row}")
            
            if result_count > display_count:
                answer_parts.append(f"... è¿˜æœ‰ {result_count - display_count} æ¡è®°å½•")
        
        return "\n".join(answer_parts)
    
    def _check_result_size(self, db_results: List[Dict]) -> bool:
        """æ£€æŸ¥æ•°æ®åº“ç»“æžœå¤§å°æ˜¯å¦è¿‡å¤§"""
        if not db_results:
            return False
        
        # è®¡ç®—ç»“æžœçš„å­—ç¬¦é•¿åº¦
        try:
            # ä½¿ç”¨è‡ªå®šä¹‰åºåˆ—åŒ–å™¨å¤„ç†Decimalç­‰ç‰¹æ®Šç±»åž‹
            def json_serializer(obj):
                if hasattr(obj, 'isoformat'):  # datetime, date å¯¹è±¡
                    return obj.isoformat()
                elif hasattr(obj, '__str__'):  # Decimalç­‰å¯¹è±¡
                    return str(obj)
                else:
                    return obj
            
            result_json = json.dumps(db_results, ensure_ascii=False, default=json_serializer)
            result_length = len(result_json)
            logger.info(f"Database result length: {result_length} characters")
            return result_length > 50000  # è¶…è¿‡10000å­—ç¬¦è®¤ä¸ºè¿‡å¤§
        except Exception as e:
            logger.warning(f"Failed to calculate result size: {e}")
            # å¦‚æžœæ— æ³•è®¡ç®—JSONé•¿åº¦ï¼Œä½¿ç”¨è®°å½•æ•°é‡ä½œä¸ºç²—ç•¥ä¼°è®¡
            return len(db_results) > 5000
    
    async def _optimize_sql_for_large_results(self, original_sql: str, question: str) -> str:
        """ä½¿ç”¨LLMä¼˜åŒ–SQLæŸ¥è¯¢ä»¥å‡å°‘ç»“æžœå¤§å°"""
        optimization_prompt = f"""è¯·ä¼˜åŒ–ä»¥ä¸‹SQLæŸ¥è¯¢ä»¥å‡å°‘ç»“æžœå¤§å°ã€‚åŽŸå§‹æŸ¥è¯¢è¿”å›žäº†å¤ªå¤šæ•°æ®ï¼Œéœ€è¦æ·»åŠ DISTINCTæˆ–GROUP BYæ¥èšåˆç»“æžœã€‚

ç”¨æˆ·é—®é¢˜: {question}
åŽŸå§‹SQL: {original_sql}

ä¼˜åŒ–è¦æ±‚:
1. å¦‚æžœæŸ¥è¯¢è¿”å›žé‡å¤æ•°æ®ï¼Œæ·»åŠ DISTINCT
2. å¦‚æžœéœ€è¦ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨GROUP BYå’Œèšåˆå‡½æ•°(COUNT, SUM, AVGç­‰)
3. ä¿æŒæŸ¥è¯¢çš„æ ¸å¿ƒé€»è¾‘ä¸å˜
4. ä¼˜å…ˆä½¿ç”¨èšåˆå‡½æ•°æ¥æä¾›ç»Ÿè®¡ä¿¡æ¯è€Œä¸æ˜¯è¯¦ç»†åˆ—è¡¨
5. å¦‚æžœå¯èƒ½ï¼Œæ·»åŠ LIMITå­å¥é™åˆ¶ç»“æžœæ•°é‡

è¯·åªè¿”å›žä¼˜åŒ–åŽçš„SQLæŸ¥è¯¢ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚"""

        llm_messages = [
            LLMMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªSQLä¼˜åŒ–ä¸“å®¶ï¼Œä¸“é—¨å¤„ç†å¤§æ•°æ®é‡æŸ¥è¯¢çš„ä¼˜åŒ–ã€‚"),
            LLMMessage(role="user", content=optimization_prompt)
        ]
        
        try:
            response = await llm_service.chat(llm_messages)
            optimized_sql = response.content.strip()
            
            # æ¸…ç†SQLï¼Œç§»é™¤å¯èƒ½çš„markdownæ ¼å¼
            if optimized_sql.startswith("```sql"):
                optimized_sql = optimized_sql[6:]
            if optimized_sql.endswith("```"):
                optimized_sql = optimized_sql[:-3]
            optimized_sql = optimized_sql.strip()
            
            logger.info(f"SQL optimized: {original_sql} -> {optimized_sql}")
            return optimized_sql
            
        except Exception as e:
            logger.error(f"Failed to optimize SQL: {e}")
            return original_sql
    
    async def save_memory(self, state: AgentState) -> AgentState:
        """Save interaction to memory system"""
        user_email = state.get("user_email")
        session_id = state.get("session_id")
        query = state["query"]
        final_answer = state.get("final_answer", "")
        
        if not user_email:
            logger.info("No user email provided, skipping memory save")
            return state
        
        try:
            query_history_id = await self._save_query_history(state)
            # Extract and store memories from this interaction
            if query_history_id and final_answer:
                memory_ids = await memory_service.extract_and_store_memories(
                    query_id=query_history_id,
                    user_email=user_email,
                    question=query,
                    answer=final_answer,
                    session_id=session_id,
                    feedback_type=None  # Will be updated when user provides feedback
                )
                logger.info(f"Saved {len(memory_ids)} memories for interaction")
            
            # Periodically consolidate memories (every 10th query)
            if query_history_id and query_history_id % 10 == 0:
                consolidation_result = await memory_service.consolidate_memories(
                    user_email=user_email,
                    session_id=session_id
                )
                logger.info(f"Memory consolidation result: {consolidation_result}")
        except Exception as e:
            logger.error(f"Failed to save memory: {e}")
        
        return state

    async def _save_query_history(self, state: AgentState) -> int:
        """
        Save query history to database
        
        Args:
            state: The agent state containing all query information
            
        Returns:
            int: The query history ID if successful, None if failed
        """
        try:
            # Extract data from agent state - use original query for history
            query = state.get("original_query") or state.get("query", "")
            user_email = state.get("user_email")
            session_id = state.get("session_id", "default")
            answer = state.get("final_answer", "")
            sql_query = state.get("sql_query")
            query_type = state.get("intent")
            rag_results = state.get("rag_results", [])
            
            # Determine success based on whether we have an answer
            success = bool(answer and answer.strip())
            
            # Get LLM info from agent if available
            llm_provider = ""
            model_name = ""
            if hasattr(self, 'llm_provider') and self.llm_provider:
                llm_provider = str(self.llm_provider.value if hasattr(self.llm_provider, 'value') else self.llm_provider)
            if hasattr(self, 'model_name') and self.model_name:
                model_name = self.model_name
            
            # Calculate processing time (simplified - could be enhanced with actual timing)
            processing_time_ms = (time.time() - state.get("start_time")) * 1000  # Default value, could be calculated from actual start/end times

            # å¤„ç†RAGç»“æžœï¼Œæå–æ–‡æ¡£ä¿¡æ¯å¹¶åŽ»é‡
            unique_documents = {}
            if rag_results is not None:
                for result in rag_results:
                    if hasattr(result, 'metadata') and result.metadata:
                        doc_id = result.metadata.get('document_id')
                        if doc_id and doc_id not in unique_documents:
                            unique_documents[doc_id] = {
                                "document_id": doc_id,
                                "document_name": result.metadata.get('document_name', ''),
                                "dataset_name": result.metadata.get('dataset_name', '')
                            }
            
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            sources = list(unique_documents.values())

            # Save to history
            history = await history_service.save_query_history(
                session_id=session_id,
                user_email=user_email,
                question=query,
                answer=answer,
                sql_query=sql_query,
                sources=sources,
                query_type=query_type,
                success=success,
                processing_time_ms=processing_time_ms,
                llm_provider=llm_provider,
                model_name=model_name
            )
            
            logger.info(f"Saved query history with ID: {history.id}")
            return history.id
            
        except Exception as e:
            logger.error(f"Failed to save query history: {e}")
            return None
    
    async def query(self, question: str, context: Optional[str] = None, user_email: Optional[str] = None, session_id: Optional[str] = None) -> QueryResponse:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        if not self.llm_available or not self.workflow:
            return QueryResponse(
                answer="æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®",
                query_type=QueryType.HYBRID,
                confidence=0.0
            )
            
        try:
            # åˆå§‹çŠ¶æ€
            initial_state = {
                "messages": [HumanMessage(content=question)],
                "query": question,
                "original_query": question,  # Store original question for history
                "rewritten_query": None,  # Will be set by rewrite_question_with_memory
                "intent": None,
                # RAG-related fields
                "rag_results": None,
                "rag_content": None,
                "rag_has_results": False,
                # Database-related fields (also used for vector search results)
                "db_results": None,
                "sql_query": None,
                "db_success": False,
                "db_results_valid": False,
                "needs_vector_search": False,
                "formatted_db_results": None,
                "result_too_large": False,
                "retry_attempted": False,
                # Response fields
                "final_answer": None,
                "source_links": None,
                # Memory-related fields
                "memory_info": None,
                "user_email": user_email,
                "session_id": session_id,
                "start_time": time.time()
            }
            
            # è¿è¡Œå·¥ä½œæµ
            final_state = await self.workflow.ainvoke(
                initial_state,
                config=RunnableConfig(recursion_limit=20)
            )
            
            # ç¡®ä¿æœ‰æœ‰æ•ˆçš„ç­”æ¡ˆ
            answer = final_state.get("final_answer")
            if not answer or answer.strip() == "":
                # å¦‚æžœæ²¡æœ‰æœ€ç»ˆç­”æ¡ˆï¼Œå°è¯•ä»Žæ¶ˆæ¯ä¸­èŽ·å–æœ€åŽä¸€ä¸ªAIæ¶ˆæ¯
                messages = final_state.get("messages", [])
                for msg in reversed(messages):
                    if isinstance(msg, AIMessage) and msg.content and msg.content.strip():
                        answer = msg.content.strip()
                        break
                
                if not answer or answer.strip() == "":
                    answer = "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›žç­”"
            
            # ç¡®å®šæŸ¥è¯¢ç±»åž‹
            query_type = final_state.get("intent")
            if not query_type:
                # æ ¹æ®ç»“æžœæ¥æºç¡®å®šæŸ¥è¯¢ç±»åž‹
                if final_state.get("rag_has_results", False):
                    query_type = QueryType.RAG
                elif final_state.get("db_results_valid", False):
                    query_type = QueryType.DATABASE
                else:
                    query_type = QueryType.HYBRID
            
            # æž„å»ºå¢žå¼ºçš„å“åº”
            response = QueryResponse(
                answer=answer,
                query_type=query_type,
                reasoning=None,  # ç®€åŒ–å“åº”ï¼Œä¸å†åŒ…å«reasoning
                confidence=0.8
            )
            
            # æ·»åŠ SQLæŸ¥è¯¢ä¿¡æ¯
            if final_state.get("sql_query"):
                response.sql_query = final_state["sql_query"]
            
            # æ·»åŠ RAGæºæ–‡ä»¶ä¿¡æ¯ï¼ˆå¦‚æžœæ¥è‡ªRAGï¼‰
            if final_state.get("rag_results"):
                # å¤„ç†RAGç»“æžœï¼Œæå–æ–‡æ¡£ä¿¡æ¯å¹¶åŽ»é‡
                unique_documents = {}
                for result in final_state["rag_results"]:
                    if hasattr(result, 'metadata') and result.metadata:
                        doc_id = result.metadata.get('document_id')
                        if doc_id and doc_id not in unique_documents:
                            unique_documents[doc_id] = {
                                "document_id": doc_id,
                                "document_name": result.metadata.get('document_name', ''),
                                "dataset_name": result.metadata.get('dataset_name', '')
                            }
                
                # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
                response.sources = list(unique_documents.values())
            
            # è®°å½•æŸ¥è¯¢å®Œæˆæƒ…å†µ
            rag_used = final_state.get("rag_has_results", False)
            db_used = final_state.get("db_results_valid", False)
            
            logger.info(f"Query completed - RAG: {rag_used}, DB: {db_used}, SQL: {bool(response.sql_query)}, Sources: {len(response.sources) if response.sources else 0}")
            
            return response
            
        except Exception as e:
            logger.error(f"Agent query error: {e}")
            return QueryResponse(
                answer=f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                query_type=QueryType.HYBRID,
                confidence=0.0
            )

# å…¨å±€å®žä¾‹
kangni_agent = KangniReActAgent()