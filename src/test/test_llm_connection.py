#!/usr/bin/env python3
"""
LLMè¿žæŽ¥æµ‹è¯•è„šæœ¬
ç”¨äºŽè¯Šæ–­å’Œæµ‹è¯•ä¸åŒLLMæä¾›å•†çš„è¿žæŽ¥çŠ¶æ€
"""

import asyncio
import pytest
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# æ‰‹åŠ¨åŠ è½½.envæ–‡ä»¶
from dotenv import load_dotenv

# æŸ¥æ‰¾å¹¶åŠ è½½.envæ–‡ä»¶
env_path = Path(__file__).parent.parent.parent / '.env'
if env_path.exists():
    load_dotenv(env_path)
    print(f"âœ… å·²åŠ è½½.envæ–‡ä»¶: {env_path}")
else:
    print(f"âš ï¸  æœªæ‰¾åˆ°.envæ–‡ä»¶: {env_path}")

from kangni_agents.models.llm_implementations import llm_service, CentralizedLLMService
from kangni_agents.models.llm_providers import LLMMessage
from kangni_agents.config import settings
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@pytest.mark.asyncio

async def test_llm_provider(provider_name: str):
    """æµ‹è¯•æŒ‡å®šçš„LLMæä¾›å•†"""
    print(f"\n{'='*50}")
    print(f"æµ‹è¯•LLMæä¾›å•†: {provider_name.upper()}")
    print(f"{'='*50}")
    
    try:
        # åˆ›å»ºæ–°çš„LLMæœåŠ¡å®žä¾‹å¹¶åˆ‡æ¢æä¾›å•†
        test_service = CentralizedLLMService()
        success = test_service.switch_provider(provider_name)
        
        if not success:
            print(f"âŒ æ— æ³•åˆå§‹åŒ– {provider_name} æä¾›å•†")
            return False
        
        print(f"âœ… {provider_name} æä¾›å•†åˆå§‹åŒ–æˆåŠŸ")
        
        # æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
        available = await test_service.is_available()
        if not available:
            print(f"âŒ {provider_name} æœåŠ¡ä¸å¯ç”¨")
            return False
        
        print(f"âœ… {provider_name} æœåŠ¡å¯ç”¨æ€§æ£€æŸ¥é€šè¿‡")
        
        # æµ‹è¯•ç®€å•èŠå¤©
        test_messages = [
            LLMMessage(role="user", content="è¯·å›žç­”: 1+1ç­‰äºŽå‡ ?")
        ]
        
        print(f"ðŸ”„ æ­£åœ¨æµ‹è¯• {provider_name} èŠå¤©åŠŸèƒ½...")
        response = await test_service.chat(test_messages)
        
        print(f"âœ… {provider_name} èŠå¤©æµ‹è¯•æˆåŠŸ")
        print(f"ðŸ“ å“åº”å†…å®¹: {response.content[:100]}{'...' if len(response.content) > 100 else ''}")
        print(f"ðŸ·ï¸  æ¨¡åž‹: {response.model}")
        print(f"ðŸ”§ æä¾›å•†: {response.provider}")
        
        return True
        
    except Exception as e:
        print(f"âŒ {provider_name} æµ‹è¯•å¤±è´¥: {str(e)}")
        logger.exception(f"Error testing {provider_name}")
        return False

@pytest.mark.asyncio

async def test_current_config():
    """æµ‹è¯•å½“å‰é…ç½®çš„LLMæä¾›å•†"""
    print(f"\n{'='*50}")
    print("æµ‹è¯•å½“å‰é…ç½®")
    print(f"{'='*50}")
    
    print(f"å½“å‰é…ç½®çš„æä¾›å•†: {settings.llm_provider}")
    print(f"OpenAI API Key: {'âœ… å·²é…ç½®' if settings.openai_api_key else 'âŒ æœªé…ç½®'}")
    print(f"DeepSeek API Key: {'âœ… å·²é…ç½®' if settings.deepseek_api_key else 'âŒ æœªé…ç½®'}")
    print(f"Alibaba API Key: {'âœ… å·²é…ç½®' if settings.alibaba_api_key else 'âŒ æœªé…ç½®'}")
    
    # æ˜¾ç¤ºAPI keyçš„å‰å‡ ä½ç”¨äºŽè°ƒè¯•ï¼ˆä¸æ˜¾ç¤ºå®Œæ•´keyï¼‰
    if settings.openai_api_key:
        print(f"OpenAI API Key (å‰8ä½): {settings.openai_api_key[:8]}...")
    if settings.deepseek_api_key:
        print(f"DeepSeek API Key (å‰8ä½): {settings.deepseek_api_key[:8]}...")
    if settings.alibaba_api_key:
        print(f"Alibaba API Key (å‰8ä½): {settings.alibaba_api_key[:8]}...")
    
    # èŽ·å–æä¾›å•†ä¿¡æ¯
    provider_info = llm_service.get_provider_info()
    print(f"LLMæœåŠ¡çŠ¶æ€: {'âœ… å¯ç”¨' if provider_info['available'] else 'âŒ ä¸å¯ç”¨'}")
    print(f"å½“å‰å®¢æˆ·ç«¯ç±»åž‹: {provider_info['client_type']}")
    
    if provider_info['available']:
        try:
            # æµ‹è¯•ç®€å•å¯¹è¯
            test_messages = [
                LLMMessage(role="user", content="ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
            ]
            
            print("ðŸ”„ æ­£åœ¨æµ‹è¯•å½“å‰é…ç½®çš„LLM...")
            response = await llm_service.chat(test_messages)
            print("âœ… å½“å‰LLMé…ç½®æµ‹è¯•æˆåŠŸ")
            print(f"ðŸ“ å“åº”: {response.content[:200]}{'...' if len(response.content) > 200 else ''}")
            return True
            
        except Exception as e:
            print(f"âŒ å½“å‰LLMé…ç½®æµ‹è¯•å¤±è´¥: {str(e)}")
            logger.exception("Current config test failed")
            return False
    else:
        print("âŒ å½“å‰LLMæœåŠ¡ä¸å¯ç”¨")
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ðŸš€ å¼€å§‹LLMè¿žæŽ¥æµ‹è¯•")
    print(f"Pythonè·¯å¾„: {sys.executable}")
    print(f"å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {Path(__file__).parent.parent.parent}")

    # é¦–å…ˆæµ‹è¯•å½“å‰é…ç½®
    current_success = await test_current_config()
    
    # æµ‹è¯•æ‰€æœ‰å¯ç”¨çš„æä¾›å•†
    providers_to_test = []
    
    # æ ¹æ®API keyå¯ç”¨æ€§å†³å®šæµ‹è¯•å“ªäº›æä¾›å•†
    if settings.deepseek_api_key:
        providers_to_test.append("deepseek")
    if settings.openai_api_key:
        providers_to_test.append("openai") 
    if settings.alibaba_api_key:
        providers_to_test.append("alibaba")
    
    # Ollamaä¸éœ€è¦API keyï¼Œæ€»æ˜¯æµ‹è¯•
    providers_to_test.append("ollama")
    
    print(f"\nå°†è¦æµ‹è¯•çš„æä¾›å•†: {providers_to_test}")
    
    results = {}
    for provider in providers_to_test:
        results[provider] = await test_llm_provider(provider)
    
    # æ€»ç»“æµ‹è¯•ç»“æžœ
    print(f"\n{'='*50}")
    print("æµ‹è¯•ç»“æžœæ€»ç»“")
    print(f"{'='*50}")
    
    working_providers = []
    failed_providers = []
    
    for provider, success in results.items():
        if success:
            working_providers.append(provider)
            print(f"âœ… {provider.upper()}: å·¥ä½œæ­£å¸¸")
        else:
            failed_providers.append(provider)
            print(f"âŒ {provider.upper()}: è¿žæŽ¥å¤±è´¥")
    
    print(f"\nðŸ“Š å¯ç”¨æä¾›å•†æ•°é‡: {len(working_providers)}/{len(providers_to_test)}")
    
    if working_providers:
        print(f"âœ… æŽ¨èä½¿ç”¨: {working_providers[0].upper()}")
        
        # å¦‚æžœå½“å‰é…ç½®ä¸å·¥ä½œä½†æœ‰å…¶ä»–å¯ç”¨é€‰é¡¹ï¼Œå»ºè®®åˆ‡æ¢
        if not current_success and working_providers:
            print(f"\nðŸ’¡ å»ºè®®: å½“å‰é…ç½®({settings.llm_provider})ä¸å¯ç”¨ï¼Œå»ºè®®åˆ‡æ¢åˆ° {working_providers[0]}")
            print(f"å¯ä»¥åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®: LLM_PROVIDER={working_providers[0]}")
    else:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„LLMæä¾›å•†ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿žæŽ¥")
    
    return len(working_providers) > 0

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)