from typing import Dict, Any, List, Optional, TypedDict, Annotated
from langgraph.graph import StateGraph, END
from langgraph.graph import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
import time

from ..config import settings
from ..models import QueryType, RAGSearchResult, QueryResponse
from ..models.llm_implementations import llm_service
from ..models.llm_providers import LLMMessage
from ..services.rag_service import rag_service
from ..services.database_service import db_service
from ..services.vector_embedding_service import vector_service
from ..services.history_service import history_service
from ..services.memory_service import memory_service
from ..utils.intent_classifier import intent_classifier

import logging
import json

logger = logging.getLogger(__name__)

class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    query: str
    original_query: Optional[str]  # Store original question for history
    rewritten_query: Optional[str]  # Store memory-enhanced question
    intent: Optional[QueryType]
    # RAG-related fields
    rag_results: Optional[List[RAGSearchResult]]
    rag_content: Optional[str]
    rag_has_results: bool
    # Database-related fields (also used for vector search results)
    db_results: Optional[Dict[str, Any]]
    sql_query: Optional[str]
    db_success: bool
    db_results_valid: bool
    needs_vector_search: bool
    formatted_db_results: Optional[str]
    result_too_large: bool
    retry_attempted: bool
    # Response fields
    final_answer: Optional[str]
    source_links: Optional[List[str]]
    # Memory-related fields
    memory_info: Optional[str]
    user_email: Optional[str]
    session_id: Optional[str]
    start_time: Optional[float]

@tool
async def rag_search_tool(query: str) -> Dict[str, Any]:
    """ÊêúÁ¥¢RAGÊñáÊ°£Â∫ìËé∑ÂèñÁõ∏ÂÖ≥‰ø°ÊÅØ - ÊîØÊåÅÂ§ö‰∏™Êï∞ÊçÆÈõÜ"""
    # Áõ¥Êé•‰ΩøÁî®RAGÊúçÂä°ÔºåÂÆÉÂÜÖÈÉ®‰ºöÂ§ÑÁêÜÂ§ö‰∏™Êï∞ÊçÆÈõÜ
    result = await rag_service.search_rag_with_answer(query, top_k=8)
    return result

@tool 
async def database_query_tool(question: str) -> Dict[str, Any]:
    """Êü•ËØ¢Êï∞ÊçÆÂ∫ìËé∑ÂèñÁªüËÆ°‰ø°ÊÅØ"""
    # Áõ¥Êé•ÊâßË°åÊï∞ÊçÆÂ∫ìÊü•ËØ¢
    result = await db_service.query_database(question)
    
    # ËøîÂõûÊ†ºÂºèÂåñÁªìÊûú
    if result.get("success"):
        return format_db_results(result)
    else:
        return {
            "content": f"Êï∞ÊçÆÂ∫ìÊü•ËØ¢Â§±Ë¥•: {result.get('error', 'Êú™Áü•ÈîôËØØ')}",
            "sql_query": result.get("sql_query"),
            "results": [],
            "success": False,
            "error": result.get("error")
        }

@tool
async def vector_database_query_tool(question: str, failed_sql: str = None) -> Dict[str, Any]:
    """‰ΩøÁî®ÂêëÈáèÊêúÁ¥¢Â¢ûÂº∫Êï∞ÊçÆÂ∫ìÊü•ËØ¢ÔºåÊâæÂà∞ÂÆûÈôÖÂ≠òÂú®ÁöÑÂÄºÂπ∂ÈáçÊñ∞ÁîüÊàêSQL"""
    import yaml
    from pathlib import Path
    from ..utils.sql_parser import SQLParser
    
    logger.info(f"Starting vector-enhanced database query for: {question}")
    
    try:
        # Load vector search configuration
        config_path = Path(__file__).parent.parent / "config" / "vector_search_config.yaml"
        vector_config = {}
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                vector_config = yaml.safe_load(f)
        else:
            logger.warning("Vector search config not found, using defaults")
            return {
                "content": "ÂêëÈáèÊêúÁ¥¢ÈÖçÁΩÆÊñá‰ª∂Êú™ÊâæÂà∞",
                "success": False,
                "error": "Missing vector_search_config.yaml"
            }
        
        # Parse the failed SQL to identify tables and fields
        sql_parser = SQLParser()
        parsed_sql = {}
        if failed_sql:
            parsed_sql = sql_parser.parse_sql(failed_sql)
            logger.info(f"Parsed SQL - tables: {parsed_sql.get('tables')}, fields: {parsed_sql.get('fields')}")
        
        # Collect suggestions for all relevant fields
        all_suggestions = {}
        vector_fields = vector_config.get('vector_search_fields', [])
        
        for field_config in vector_fields:
            table = field_config['table']
            field = field_config['field']
            description = field_config.get('description', field)
            
            # Check if this field is relevant to the query
            should_search = False
            
            # If we have a parsed SQL, check if the table/field is mentioned
            if parsed_sql:
                if table in parsed_sql.get('tables', []) or field in parsed_sql.get('fields', []):
                    should_search = True
                    logger.info(f"Field {table}.{field} found in failed SQL")
            
            # Also check if keywords from the question match this field's keywords
            keywords = field_config.get('keywords', [])
            question_lower = question.lower()
            for keyword in keywords:
                if keyword.lower() in question_lower:
                    should_search = True
                    logger.info(f"Keyword '{keyword}' found in question for field {table}.{field}")
                    break
            
            if should_search:
                logger.info(f"Searching for similar values in {table}.{field} ({description})")
                
                # Get similar values from the database
                suggestions = await vector_service.search_similar_values(
                    search_text=question,
                    table_name=table,
                    field_name=field,
                    top_k=settings.max_suggestions or 3,  # This is already the max_suggestions value
                    similarity_threshold=settings.similarity_threshold or 0.3  # This is already the threshold value
                )
                
                if suggestions:
                    # Limit suggestions to max_suggestions
                    limited_suggestions = suggestions[:settings.max_suggestions]
                    # Extract just the field values for display
                    field_values = [suggestion['field_value'] for suggestion in limited_suggestions]
                    logger.info(f"Found {len(limited_suggestions)} suggestions for {description}: {field_values[:3]}...")
                    all_suggestions[f"{table}.{field}"] = {
                        "values": field_values,
                        "description": description,
                        "table": table,
                        "field": field
                    }
        
        if not all_suggestions:
            logger.info("No vector search suggestions found")
            return {
                "content": "Êú™ÊâæÂà∞Áõ∏‰ººÁöÑÊï∞ÊçÆÂ∫ìÂÄº",
                "success": False,
                "suggestions": {},
                "message": "ÂêëÈáèÊêúÁ¥¢Êú™ÊâæÂà∞ÂåπÈÖçÁöÑÂÄº"
            }
        
        # Build enhanced prompt with suggestions
        suggestion_text = "\n\nÂü∫‰∫éÂêëÈáèÊêúÁ¥¢ÊâæÂà∞ÁöÑÊï∞ÊçÆÂ∫ìÂÆûÈôÖÂÄºÔºö\n"
        for field_key, field_data in all_suggestions.items():
            suggestion_text += f"- {field_data['description']} ({field_data['table']}.{field_data['field']}): "
            suggestion_text += f"{', '.join(field_data['values'])}"
            if len(field_data['values']) > 3:
                suggestion_text += f" Á≠â{len(field_data['values'])}‰∏™ÂÄº"
            suggestion_text += "\n"
        
        enhanced_question = f"{question}{suggestion_text}\nËØ∑‰ΩøÁî®Ëøô‰∫õÂÆûÈôÖÂ≠òÂú®ÁöÑÂÄºÈáçÊñ∞ÁîüÊàêSQLÊü•ËØ¢„ÄÇÂ¶ÇÊûúÊúâÂ§ö‰∏™ÂåπÈÖçÂÄºÔºå‰ΩøÁî®ÊúÄÁõ∏‰ººÁöÑÂÄºÊù•Êü•ËØ¢„ÄÇ"
        
        # Generate new SQL with enhanced context
        logger.info("Generating new SQL with vector search suggestions")
        enhanced_result = await db_service.query_database(enhanced_question)
        
        if enhanced_result.get("success"):
            logger.info(f"Vector-enhanced query successful, got {len(enhanced_result.get('results', []))} results")
            enhanced_result["vector_enhanced"] = True
            enhanced_result["suggestions_used"] = all_suggestions
            return format_db_results(enhanced_result)
        else:
            logger.warning(f"Vector-enhanced query failed: {enhanced_result.get('error')}")
            return {
                "content": f"ÂêëÈáèÂ¢ûÂº∫Êü•ËØ¢Â§±Ë¥•: {enhanced_result.get('error', 'Êú™Áü•ÈîôËØØ')}",
                "success": False,
                "suggestions": all_suggestions,
                "sql_query": enhanced_result.get("sql_query"),
                "error": enhanced_result.get("error")
            }
            
    except Exception as e:
        logger.error(f"Error in vector database query tool: {e}")
        return {
            "content": f"ÂêëÈáèÊêúÁ¥¢Âá∫Èîô: {str(e)}",
            "success": False,
            "error": str(e)
        }

def format_db_results(result: Dict[str, Any]) -> Dict[str, Any]:
    """Ê†ºÂºèÂåñÊï∞ÊçÆÂ∫ìÊü•ËØ¢ÁªìÊûú"""
    # Â§ÑÁêÜÊó•ÊúüÂ∫èÂàóÂåñÈóÆÈ¢ò
    def serialize_dates(obj):
        """ÈÄíÂΩíÂ§ÑÁêÜÂØπË±°‰∏≠ÁöÑÊó•ÊúüÁ±ªÂûãÔºåËΩ¨Êç¢‰∏∫Â≠óÁ¨¶‰∏≤"""
        if hasattr(obj, 'isoformat'):  # datetime, date ÂØπË±°
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {key: serialize_dates(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [serialize_dates(item) for item in obj]
        else:
            return obj
    
    # Â∫èÂàóÂåñÁªìÊûú‰∏≠ÁöÑÊó•ÊúüÂØπË±°
    serialized_results = serialize_dates(result.get("results", []))
    
    success = False
    try:
        success = True
        formatted_content = json.dumps({
            "sql_query": result.get("sql_query"),
            "results": serialized_results,
            "vector_enhanced": result.get("vector_enhanced", False),
            "suggestions_used": result.get("suggestions_used", [])
        }, ensure_ascii=False, indent=2)
    except (TypeError, ValueError) as e:
        # Â¶ÇÊûú‰ªçÁÑ∂ÊúâÂ∫èÂàóÂåñÈóÆÈ¢òÔºå‰ΩøÁî®Êõ¥ÂÆâÂÖ®ÁöÑÊ†ºÂºèÂåñÊñπÊ≥ï
        logger.warning(f"JSON serialization failed, using fallback formatting: {e}")
        formatted_content = f"SQLÊü•ËØ¢: {result.get('sql_query', 'N/A')}\n"
        formatted_content += f"ÁªìÊûúÊï∞Èáè: {len(serialized_results)}\n"
        formatted_content += "ÁªìÊûúÊï∞ÊçÆ:\n"
        for i, row in enumerate(serialized_results[:5], 1):  # Âè™ÊòæÁ§∫Ââç5Ë°å
            formatted_content += f"  {i}. {row}\n"
        if len(serialized_results) > 5:
            formatted_content += f"  ... ËøòÊúâ {len(serialized_results) - 5} Ë°åÊï∞ÊçÆ"
    
    return {
        "success": success,
        "content": formatted_content,
        "sql_query": result.get("sql_query"),
        "results": serialized_results,
        "vector_enhanced": result.get("vector_enhanced", False),
        "suggestions_used": result.get("suggestions_used", [])
    }

class KangniReActAgent:
    def __init__(self):
        # ‰ΩøÁî®ÈõÜ‰∏≠ÂºèLLMÊúçÂä°
        self.llm_available = llm_service.llm_available
        self.llm_provider = llm_service.llm_provider
        
        if self.llm_available:
            try:
                # ÁªëÂÆöÂ∑•ÂÖ∑ - Ê∑ªÂä†vector_database_query_tool
                self.tools = [rag_search_tool, database_query_tool, vector_database_query_tool]
                
                # ÊûÑÂª∫Áä∂ÊÄÅÂõæ
                self.workflow = self._build_workflow()

                # ‰øùÂ≠òÁä∂ÊÄÅÂõæÁöÑÂèØËßÜÂåñË°®Á§∫
                self._save_graph_visualization()
                
                logger.info(f"Agent initialized successfully with LLM provider: {self.llm_provider}")
                
            except Exception as e:
                logger.warning(f"Failed to initialize agent: {e}")
                self.llm_available = False
                self.workflow = None
        else:
            logger.warning("LLM service not available, agent features will be disabled")
            self.workflow = None
    
    def _save_graph_visualization(self, filename: str = "docs/workflow_graph.png") -> None:
        """‰øùÂ≠òÁä∂ÊÄÅÂõæÁöÑÂèØËßÜÂåñË°®Á§∫
        
        Args:
            filename: ‰øùÂ≠òÊñá‰ª∂Ë∑ØÂæÑ
        """
        if not self.workflow:
            logger.warning("No workflow to visualize")
            return
            
        try:
            # Â∞ùËØïÁîüÊàê Mermaid Âõæ
            graph = self.workflow.get_graph()
            
            # È¶ñÂÖàÂ∞ùËØï‰øùÂ≠ò‰∏∫ PNGÔºàÈúÄË¶Å graphvizÔºâ
            try:
                with open(filename, "wb") as f:
                    f.write(graph.draw_mermaid_png())
                logger.info(f"Graph visualization saved as PNG: {filename}")
                print(f"‚úÖ Workflow graph saved as: {filename}")
            except Exception as png_error:
                logger.warning(f"Could not save as PNG (graphviz may not be installed): {png_error}")
                
                # ÈÄÄËÄåÊ±ÇÂÖ∂Ê¨°Ôºå‰øùÂ≠ò‰∏∫ Mermaid ÊñáÊú¨Ê†ºÂºè
                mermaid_filename = filename.replace('.png', '.mermaid')
                try:
                    mermaid_text = graph.draw_mermaid()
                    with open(mermaid_filename, "w", encoding="utf-8") as f:
                        f.write(mermaid_text)
                    logger.info(f"Graph saved as Mermaid text: {mermaid_filename}")
                    print(f"‚úÖ Workflow graph saved as Mermaid text: {mermaid_filename}")
                    print(f"   You can visualize it at: https://mermaid.live/")
                    
                    # ÂêåÊó∂ÊâìÂç∞ÂõæÂΩ¢ÁªìÊûÑ
                    print("\nüìä Workflow Structure:")
                    print(mermaid_text)
                except Exception as mermaid_error:
                    logger.error(f"Could not save Mermaid text: {mermaid_error}")
                    
                    # ÊúÄÂêéÁöÑÂ§áÁî®ÊñπÊ°àÔºöÊâìÂç∞ËäÇÁÇπÂíåËæπ
                    print("\nüìä Workflow Nodes and Edges:")
                    print(f"Nodes: {graph.nodes}")
                    print(f"Edges: {graph.edges}")
                    
        except Exception as e:
            logger.error(f"Failed to save graph visualization: {e}")
            print(f"‚ùå Could not visualize workflow: {e}")
    
    def _build_workflow(self) -> StateGraph:
        """ÊûÑÂª∫LangGraphÂ∑•‰ΩúÊµÅ - Êñ∞ÊµÅÁ®ãÔºö1.Âä†ËΩΩËÆ∞ÂøÜ 2.ÊîπÂÜôÈóÆÈ¢ò 3.RAGÊêúÁ¥¢ 4.Êï∞ÊçÆÂ∫ìÊü•ËØ¢ 5.ÂêëÈáèÊêúÁ¥¢ 6.ÁîüÊàêÂìçÂ∫î"""
        workflow = StateGraph(AgentState)
        
        # Ê∑ªÂä†ËäÇÁÇπ
        workflow.add_node("load_memory", self.load_memory_info)
        workflow.add_node("rewrite_question", self.rewrite_question_with_memory)
        workflow.add_node("rag_search", self.execute_rag_search)
        workflow.add_node("check_rag_results", self.check_rag_results)
        workflow.add_node("database_query", self.execute_database_query)
        workflow.add_node("validate_db_results", self.validate_database_results)
        workflow.add_node("database_retry", self.execute_database_retry)
        workflow.add_node("validate_retry_results", self.validate_database_results)
        workflow.add_node("vector_search", self.execute_vector_search)
        workflow.add_node("validate_vector_results", self.validate_database_results)
        workflow.add_node("generate_response", self.generate_response)
        workflow.add_node("save_memory", self.save_memory)
        
        # ËÆæÁΩÆÂÖ•Âè£ÁÇπ
        workflow.set_entry_point("load_memory")
        
        # Ê∑ªÂä†Ëæπ
        workflow.add_edge("load_memory", "rewrite_question")
        workflow.add_edge("rewrite_question", "rag_search")
        workflow.add_edge("rag_search", "check_rag_results")
        
        # Êù°‰ª∂ËæπÔºöÊ£ÄÊü•RAGÁªìÊûú
        def check_rag_condition(state: AgentState) -> str:
            has_rag_results = state.get("rag_has_results", False)
            logger.info(f"Checking RAG results: {has_rag_results}")
            
            if has_rag_results:
                logger.info("RAG has results, routing to generate_response")
                return "generate_response"
            else:
                logger.info("No RAG results, routing to database_query")
                return "database_query"
        
        workflow.add_conditional_edges(
            "check_rag_results",
            check_rag_condition,
            {
                "generate_response": "generate_response",
                "database_query": "database_query"
            }
        )
        
        workflow.add_edge("database_query", "validate_db_results")
        
        # Êù°‰ª∂ËæπÔºöÊ£ÄÊü•Êï∞ÊçÆÂ∫ìÁªìÊûú
        def check_db_condition(state: AgentState) -> str:
            needs_vector_search = state.get("needs_vector_search", False)
            db_results = state.get("db_results", [])
            result_too_large = state.get("result_too_large", False)
            
            logger.info(f"Checking database results - needs_vector: {needs_vector_search}, result_too_large: {result_too_large}")
            
            # Â¶ÇÊûúÁªìÊûúÂ§™Â§ßÔºåÂÖàÂ∞ùËØïÈáçËØï‰ºòÂåñSQL
            if result_too_large and not state.get("retry_attempted", False):
                logger.info("Result too large, routing to database_retry")
                return "database_retry"
            # Â¶ÇÊûú‰∏çÈúÄË¶ÅÂêëÈáèÊêúÁ¥¢ÔºåÁõ¥Êé•ÁîüÊàêÂìçÂ∫î
            elif not needs_vector_search:
                logger.info("Database has valid results, routing to generate_response")
                return "generate_response"
            # Âê¶ÂàôËøõË°åÂêëÈáèÊêúÁ¥¢
            else:
                logger.info("No valid database results, routing to vector_search")
                return "vector_search"
        
        workflow.add_conditional_edges(
            "validate_db_results",
            check_db_condition,
            {
                "generate_response": "generate_response",
                "database_retry": "database_retry",
                "vector_search": "vector_search"
            }
        )
        
        workflow.add_edge("database_retry", "validate_retry_results")
        workflow.add_edge("validate_retry_results", "generate_response")
        workflow.add_edge("vector_search", "validate_vector_results")
        workflow.add_edge("validate_vector_results", "generate_response")
        workflow.add_edge("generate_response", "save_memory")
        workflow.add_edge("save_memory", END)
        
        return workflow.compile()
    
    async def load_memory_info(self, state: AgentState) -> AgentState:
        """Load memory context for the user"""
        user_email = state.get("user_email")
        session_id = state.get("session_id")
        query = state["query"]
        
        logger.info(f"Loading memory context for user: {user_email}, session: {session_id}")
        
        # Get memory context from memory service
        memory_info = ""
        if user_email:
            try:
                memory_context = await memory_service.get_memory_context_for_agent(
                    user_email=user_email,
                    question=query,
                    session_id=session_id
                )

                # Build memory context string
                memory_info = ""
                if memory_context:
                    # Add recent interactions
                    # recent_interactions = memory_context.get("recent_interactions", [])
                    # if recent_interactions:
                    #     memory_info += "\nÊúÄËøëÁöÑ‰∫§‰∫íÂéÜÂè≤:\n"
                    #     for interaction in recent_interactions[:3]:
                    #         memory_info += f"- Q: {interaction['question']}...\n"
                    #         if interaction.get('answer'):
                    #             memory_info += f"  A: {interaction['answer']}...\n"
                    
                    # # Add long-term memories
                    # long_term = memory_context.get("long_term_memories", [])
                    # if long_term:
                    #     memory_info += "\nÁõ∏ÂÖ≥ÁöÑÈïøÊúüËÆ∞ÂøÜ:\n"
                    #     for mem in long_term[:3]:
                    #         memory_info += f"- {mem['content'][:150]}... (ÈáçË¶ÅÊÄß: {mem.get('importance', 'unknown')})\n"
                    
                    # Add short-term memories
                    short_term = memory_context.get("short_term_memories", [])
                    if short_term:
                        memory_info += "\n‰ºöËØù‰∏ä‰∏ãÊñá:\n"
                        for mem in short_term:
                            memory_info += f"- {mem['content']}...\n"

                logger.info(f"Loaded memory context with {len(memory_context.get('short_term_memories', []))} short-term and {len(memory_context.get('long_term_memories', []))} long-term memories")
            except Exception as e:
                logger.error(f"Failed to load memory context: {e}")
        
        return {
            **state,
            "start_time": time.time(),
            "memory_info": memory_info
        }
    
    async def rewrite_question_with_memory(self, state: AgentState) -> AgentState:
        """Rewrite question based on memory context to improve search accuracy"""
        original_query = state["query"]
        memory_info = state.get("memory_info", "")
        
        logger.info(f"Rewriting question with memory context: {original_query}")
        
        # If no memory context available, use original question
        if not memory_info or memory_info.strip() == "":
            logger.info("No memory context available, using original question")
            return {
                **state,
                "original_query": original_query,
                "rewritten_query": original_query
            }
        
        try:
            # Use LLM to rewrite question based on memory context
            rewrite_prompt = f"""ËØ∑Ê†πÊçÆ‰ª•‰∏ãËÆ∞ÂøÜ‰∏ä‰∏ãÊñáÔºåÂ∞ÜÁî®Êà∑ÁöÑÈóÆÈ¢òÊîπÂÜôÂæóÊõ¥ÂÖ∑‰ΩìÔºå‰ª•‰æøÊõ¥Â•ΩÂú∞ÊêúÁ¥¢Áõ∏ÂÖ≥ÊñáÊ°£„ÄÇ

ÂéüÂßãÈóÆÈ¢ò: {original_query}

ËÆ∞ÂøÜ‰∏ä‰∏ãÊñá:
{memory_info}

ÈúÄË¶ÅÁêÜËß£Áî®Êà∑ÁöÑÈóÆÈ¢òÔºåÊ†πÊçÆ‰∏ãÈù¢ÁöÑÊ≠•È™§ÁêÜËß£Áî®Êà∑ÁöÑÈóÆÈ¢ò„ÄÇ
1. ÂÖàÊ£ÄÊü•Áî®Êà∑ÂéüÂßãÈóÆÈ¢òÊòØÂê¶ÈúÄË¶ÅÂéÜÂè≤ËÆ∞ÂøÜÊù•ÂõûÁ≠î  
2. Â¶ÇÊûúÈúÄË¶ÅÔºåÊâæÂà∞‰∏éÈóÆÈ¢òÊúÄÁõ∏ÂÖ≥ÁöÑËÆ∞ÂøÜÂπ∂ÂºïÁî®  
3. ÂÜçÊ†πÊçÆÂΩìÂâçËæìÂÖ•ÁªôÂá∫ÈáçÂÜôÂêéÁöÑÈóÆÈ¢ò

ÊîπÂÜôË¶ÅÊ±Ç:
1. ‰øùÊåÅÂéüÂßãÈóÆÈ¢òÁöÑÊ†∏ÂøÉÊÑèÂõæ
2. ÁªìÂêàËÆ∞ÂøÜ‰∏ä‰∏ãÊñá‰∏≠ÁöÑÁõ∏ÂÖ≥‰ø°ÊÅØÔºå‰ΩøÈóÆÈ¢òÊõ¥ÂÖ∑‰Ωì
3. Â¶ÇÊûúËÆ∞ÂøÜ‰∏ä‰∏ãÊñáÂåÖÂê´Áõ∏ÂÖ≥ÁöÑÂéÜÂè≤ÂØπËØùÊàñ‰ø°ÊÅØÔºåËØ∑Â∞ÜÂÖ∂ËûçÂÖ•Âà∞ÈóÆÈ¢ò‰∏≠
4. ÊîπÂÜôÂêéÁöÑÈóÆÈ¢òÂ∫îËØ•Êõ¥ÂÆπÊòìÂåπÈÖçÂà∞Áõ∏ÂÖ≥ÁöÑÊñáÊ°£
5. ‰øùÊåÅÈóÆÈ¢òÁöÑËá™ÁÑ∂ÊÄßÂíåÂèØËØªÊÄß

ËØ∑Âè™ËøîÂõûÊîπÂÜôÂêéÁöÑÈóÆÈ¢òÔºå‰∏çË¶ÅÂåÖÂê´ÂÖ∂‰ªñËß£Èáä„ÄÇ"""

            llm_messages = [
                LLMMessage(role="system", content="‰Ω†ÊòØ‰∏Ä‰∏™ÈóÆÈ¢òÊîπÂÜô‰∏ìÂÆ∂Ôºå‰∏ìÈó®Ê†πÊçÆËÆ∞ÂøÜ‰∏ä‰∏ãÊñá‰ºòÂåñÈóÆÈ¢ò‰ª•ÊèêÈ´òÊêúÁ¥¢ÂáÜÁ°ÆÊÄß„ÄÇ"),
                LLMMessage(role="user", content=rewrite_prompt)
            ]
            
            response = await llm_service.chat(llm_messages)
            rewritten_query = response.content.strip()
            
            # Ê∏ÖÁêÜÂèØËÉΩÁöÑmarkdownÊ†ºÂºè
            if rewritten_query.startswith("```"):
                rewritten_query = rewritten_query.split("\n", 1)[1] if "\n" in rewritten_query else rewritten_query[3:]
            if rewritten_query.endswith("```"):
                rewritten_query = rewritten_query.rsplit("\n", 1)[0] if "\n" in rewritten_query else rewritten_query[:-3]
            
            rewritten_query = rewritten_query.strip()
            
            # Â¶ÇÊûúÊîπÂÜôÂ§±Ë¥•ÊàñÁªìÊûú‰∏∫Á©∫Ôºå‰ΩøÁî®ÂéüÂßãÈóÆÈ¢ò
            if not rewritten_query or rewritten_query == "":
                logger.warning("Question rewriting failed or returned empty, using original question")
                rewritten_query = original_query
            
            logger.info(f"Question rewritten: '{original_query}' -> '{rewritten_query}'")
            
            return {
                **state,
                "original_query": original_query,
                "rewritten_query": rewritten_query
            }
            
        except Exception as e:
            logger.error(f"Failed to rewrite question: {e}")
            # Fallback to original question
            return {
                **state,
                "original_query": original_query,
                "rewritten_query": original_query
            }
    
    async def execute_rag_search(self, state: AgentState) -> AgentState:
        """ÊâßË°åRAGÊêúÁ¥¢"""
        # Use rewritten query if available, otherwise fallback to original
        query = state.get("rewritten_query") or state["query"]
        
        logger.info(f"Executing RAG search for query: {query}")
        
        try:
            result = await rag_search_tool.ainvoke({"query": query})
            
            # Â≠òÂÇ®RAGÁªìÊûú
            rag_results = result.get("rag_results", [])
            content = result.get("content", "")
            
            # Ê£ÄÊü•ÊòØÂê¶ÊúâÊúâÊïàÁªìÊûú - Êõ¥‰∏•Ê†ºÁöÑÊù°‰ª∂
            has_results = bool(
                content and 
                content.strip() and 
                "Êú™ÊâæÂà∞Áõ∏ÂÖ≥ÊñáÊ°£‰ø°ÊÅØ" not in content and
                "Êâæ‰∏çÂà∞" not in content and
                "Ê≤°ÊúâÊâæÂà∞" not in content and
                "Êó†Ê≥ïÊâæÂà∞" not in content and
                "Ê≤°ÊúâÁõ∏ÂÖ≥" not in content and
                "Êó†Áõ∏ÂÖ≥‰ø°ÊÅØ" not in content and
                "Êú™ÂåÖÂê´ÊâÄÈúÄ‰ø°ÊÅØ" not in content and
                len(content.strip()) > 50  # Á°Æ‰øùÊúâË∂≥Â§üÁöÑÂÜÖÂÆπ
            )
            
            # ÂàõÂª∫Â∑•ÂÖ∑Ê∂àÊÅØ
            tool_message = AIMessage(content=f"RAGÊêúÁ¥¢ÁªìÊûúÔºö\n{content}")
            
            logger.info(f"RAG search completed. Has results: {has_results}")
            
            return {
                **state,
                "rag_results": rag_results,
                "rag_content": content,
                "rag_has_results": has_results,
                "messages": [*state["messages"], tool_message]
            }
            
        except Exception as e:
            logger.error(f"RAG search error: {e}")
            error_message = AIMessage(content=f"RAGÊêúÁ¥¢Êó∂ÂèëÁîüÈîôËØØ: {str(e)}")
            return {
                **state,
                "rag_has_results": False,
                "messages": [*state["messages"], error_message]
            }
    
    async def check_rag_results(self, state: AgentState) -> AgentState:
        """Ê£ÄÊü•RAGÊêúÁ¥¢ÁªìÊûú"""
        # Ëøô‰∏™ËäÇÁÇπ‰∏ªË¶ÅÁî®‰∫éË∑ØÁî±ÂÜ≥Á≠ñÔºåÁä∂ÊÄÅÂ∑≤ÁªèÂú®execute_rag_search‰∏≠ËÆæÁΩÆ
        return state
    
    async def execute_database_query(self, state: AgentState) -> AgentState:
        """ÊâßË°åÊï∞ÊçÆÂ∫ìÊü•ËØ¢"""
        # Use rewritten query if available, otherwise fallback to original
        query = state.get("rewritten_query") or state["query"]
        
        logger.info(f"Executing database query for: {query}")
        
        try:
            result = await database_query_tool.ainvoke({"question": query})
            
            # Â≠òÂÇ®Êï∞ÊçÆÂ∫ìÁªìÊûú
            db_results = result.get("results", [])
            sql_query = result.get("sql_query")
            success = result.get("success", False)
            
            # ÂàõÂª∫Â∑•ÂÖ∑Ê∂àÊÅØ
            tool_message = AIMessage(content=f"Êï∞ÊçÆÂ∫ìÊü•ËØ¢ÁªìÊûúÔºö\n{result['content']}")
            
            logger.info(f"Database query completed. Success: {success}, Results count: {len(db_results) if db_results else 0}")
            
            return {
                **state,
                "db_results": db_results,
                "sql_query": sql_query,
                "db_success": success,
                "messages": [*state["messages"], tool_message]
            }
            
        except Exception as e:
            logger.error(f"Database query error: {e}")
            error_message = AIMessage(content=f"Êï∞ÊçÆÂ∫ìÊü•ËØ¢Êó∂ÂèëÁîüÈîôËØØ: {str(e)}")
            return {
                **state,
                "db_success": False,
                "messages": [*state["messages"], error_message]
            }
    
    async def validate_database_results(self, state: AgentState) -> AgentState:
        """È™åËØÅÂ∑•ÂÖ∑ÊâßË°åÁªìÊûúÁöÑÊúâÊïàÊÄß - ‰ΩøÁî®LLMÊô∫ËÉΩÂà§Êñ≠"""

        db_results = state.get("db_results", [])
        sql_query = state.get("sql_query")
        query = state.get("rewritten_query") or state["query"]

        # Ê£ÄÊü•ÁªìÊûúÂ§ßÂ∞èÊòØÂê¶ËøáÂ§ßÔºà‰ªÖÂú®ÂàùÊ¨°È™åËØÅÊó∂Ê£ÄÊü•Ôºâ
        if not state.get("retry_attempted", False):
            result_too_large = self._check_result_size(db_results)
            if result_too_large:
                logger.info("Result too large, routing to database_retry")
                return {
                    **state,
                    "result_too_large": result_too_large,
                    "retry_attempted": False,  # Ê†áËÆ∞‰∏∫ÈúÄË¶ÅÈáçËØï
                }
            
        # ÂÆâÂÖ®Âú∞Â∫èÂàóÂåñÊï∞ÊçÆÂ∫ìÁªìÊûúÔºåÈÅøÂÖçJSONËΩ¨‰πâÈóÆÈ¢òÔºàÊîØÊåÅ Decimal / datetime / date Á≠âÔºâ
        def safe_serialize_results(results):
            if not results:
                return "Á©∫ÁªìÊûú"
            try:
                def default_serializer(obj):
                    # datetime/date ÂØπË±°
                    if hasattr(obj, "isoformat"):
                        return obj.isoformat()
                    # ÂÖ∂‰ªñÂ¶Ç Decimal Á≠âÂØπË±°
                    return str(obj)
                # ‰ΩøÁî®json.dumpsÁ°Æ‰øùÊ≠£Á°ÆÁöÑJSONÊ†ºÂºèÔºåÂπ∂Â§ÑÁêÜÁâπÊÆäÁ±ªÂûã
                json_str = json.dumps(results, ensure_ascii=False, indent=2, default=default_serializer)
                return json_str
            except (TypeError, ValueError) as e:
                # Â¶ÇÊûúJSONÂ∫èÂàóÂåñÂ§±Ë¥•Ôºå‰ΩøÁî®ÁÆÄÂçïÁöÑÂ≠óÁ¨¶‰∏≤Ë°®Á§∫
                logger.warning(f"JSON serialization failed: {e}, using fallback")
                if isinstance(results, list):
                    return f"ÁªìÊûúÂàóË°®ÔºåÂÖ±{len(results)}Êù°ËÆ∞ÂΩï"
                elif isinstance(results, dict):
                    return f"ÁªìÊûúÂ≠óÂÖ∏ÔºåÂåÖÂê´{len(results)}‰∏™Â≠óÊÆµ"
                else:
                    return str(results)

        # ÊûÑÂª∫ËÆ©LLMÂà§Êñ≠ÂíåÊ†ºÂºèÂåñÁöÑÊèêÁ§∫
        validation_prompt = f"""ËØ∑ÂàÜÊûê‰ª•‰∏ãÂ∑•ÂÖ∑ÊâßË°åÁªìÊûúÔºåÂà§Êñ≠Êü•ËØ¢ÊòØÂê¶ÊàêÂäü„ÄÇÂ¶ÇÊûúÊü•ËØ¢ÁªìÊûúÊàêÂäüÔºåÁõ¥Êé•Ê†ºÂºèÂåñÁªìÊûú„ÄÇ

Áî®Êà∑ÈóÆÈ¢ò: {query}

ÁîüÊàêÁöÑSQL: {sql_query if sql_query else "Êó†"}

Êï∞ÊçÆÂ∫ìÊü•ËØ¢ÁªìÊûú: {safe_serialize_results(db_results)}

ËØ∑Êåâ‰ª•‰∏ãÊ≠•È™§Â§ÑÁêÜÔºö

1. ÂàÜÊûêÊü•ËØ¢ÁªìÊûúÔºö
   - SQLÊü•ËØ¢ÊòØÂê¶ÊàêÂäüÁîüÊàêÔºüÔºàÊòØ/Âê¶Ôºâ
   - Êï∞ÊçÆÂ∫ìÊü•ËØ¢ÊòØÂê¶ËøîÂõû‰∫ÜÊúâÊïàÊï∞ÊçÆÔºüÔºàÊòØ/Âê¶Ôºâ
   - Â¶ÇÊûúSQLÊàêÂäü‰ΩÜËøîÂõû0Êù°ËÆ∞ÂΩïÔºåÊàñËÄÖcount(*)‰∏∫0ÔºåËØ¥ÊòéÁî®Êà∑ËæìÂÖ•ÂèØËÉΩ‰∏çÂ§üÂáÜÁ°ÆÔºåÈúÄË¶ÅÂêëÈáèÊêúÁ¥¢Êù•ÊâæÂà∞Ê≠£Á°ÆÁöÑÊï∞ÊçÆÂ∫ìÂÄºÔºàÊòØ/Âê¶Ôºâ

2. Â¶ÇÊûúÊï∞ÊçÆÂ∫ìÊü•ËØ¢ËøîÂõû‰∫ÜÊúâÊïàÊï∞ÊçÆÔºåËØ∑Áõ¥Êé•Ê†ºÂºèÂåñÁªìÊûúÔºö
   - Áî®Ëá™ÁÑ∂ËØ≠Ë®ÄÂõûÁ≠îÁî®Êà∑ÁöÑÈóÆÈ¢ò
   - Â¶ÇÊûúÊü•ËØ¢ÁªìÊûúÂåÖÂê´COUNT(*)ÊàñÊï∞ÈáèÁªüËÆ°ÔºåËØ∑ÊòéÁ°ÆËØ¥Âá∫ÂÖ∑‰ΩìÁöÑÊï∞Â≠ó
   - ÂØπ‰∫éÊï∞ÊçÆÂàóË°®ÔºåÁî®Ê∏ÖÊô∞ÁöÑÊñπÂºèÂ±ïÁ§∫ÂÖ≥ÈîÆ‰ø°ÊÅØ
   - Â¶ÇÊûúÊï∞ÊçÆÈáèÂæàÂ§ßÔºåÂè™ÊòæÁ§∫ÂâçÂá†Êù°ËÆ∞ÂΩïÂπ∂ËØ¥ÊòéÊÄªÊï∞
   - ‰øùÊåÅÂõûÁ≠îÁÆÄÊ¥ÅÊòé‰∫ÜÔºåÈáçÁÇπÁ™ÅÂá∫

Âà§Êñ≠Ê†áÂáÜÔºö
- Â¶ÇÊûúSQLÁîüÊàêÊàêÂäü‰ΩÜËøîÂõû0Êù°ËÆ∞ÂΩïÔºåÂàôËÆ§‰∏∫Â§±Ë¥•
- Â¶ÇÊûúSQLËøêË°åÊä•ÈîôÔºåÂàôËÆ§‰∏∫Â§±Ë¥•
- Â¶ÇÊûúÊü•ËØ¢ËøîÂõû‰∫ÜÊï∞ÊçÆÔºåÂàôËÆ§‰∏∫ÊàêÂäüÔºå‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÊìç‰Ωú

ËØ∑Êåâ‰ª•‰∏ãÊ†ºÂºèÂõûÁ≠îÔºö
SQLÁîüÊàêÊàêÂäü: [ÊòØ/Âê¶]
ÊúâÊïàÊï∞ÊçÆ: [ÊòØ/Âê¶]
ÈúÄË¶ÅÂêëÈáèÊêúÁ¥¢: [Â¶ÇÊûúSQLÊàêÂäü‰ΩÜÊó†Êï∞ÊçÆÔºåÊàñÁî®Êà∑ËæìÂÖ•‰∏çÂ§üÂáÜÁ°ÆÔºåÂàôÂ°´"ÊòØ"ÔºåÂê¶ÂàôÂ°´"Âê¶"]
Ê†ºÂºèÂåñÁªìÊûú: [Â¶ÇÊûúÊúâÊïàÊï∞ÊçÆÔºåËØ∑Áõ¥Êé•ÁªôÂá∫Ê†ºÂºèÂåñÁöÑÂõûÁ≠îÔºåÂê¶ÂàôÂÜô"Êó†"]"""

        # ËΩ¨Êç¢Ê∂àÊÅØÊ†ºÂºè‰∏∫LLMMessage
        llm_messages = [
            LLMMessage(role="system", content="‰Ω†ÊòØ‰∏Ä‰∏™Êï∞ÊçÆÂ∫ìÊü•ËØ¢ÁªìÊûúÂàÜÊûêÂä©ÊâãÔºåÈúÄË¶ÅÂáÜÁ°ÆÂà§Êñ≠Êü•ËØ¢ÁªìÊûúÁöÑÊúâÊïàÊÄß„ÄÇ"),
            LLMMessage(role="user", content=validation_prompt)
        ]
        
        # ‰∏çËÆæÁΩÆmax_tokensÔºåËÆ©ÊúçÂä°Âô®‰ΩøÁî®ÈªòËÆ§ÂÄº
        response = await llm_service.chat(llm_messages)
        response_text = response.content.lower()
        
        # Ëß£ÊûêLLMÁöÑÂà§Êñ≠ÁªìÊûú
        has_valid_sql = "sqlÁîüÊàêÊàêÂäü: ÊòØ" in response_text or "sqlÁîüÊàêÊàêÂäüÔºöÊòØ" in response_text
        has_valid_data = "ÊúâÊïàÊï∞ÊçÆ: ÊòØ" in response_text or "ÊúâÊïàÊï∞ÊçÆÔºöÊòØ" in response_text
        needs_vector = "ÈúÄË¶ÅÂêëÈáèÊêúÁ¥¢: ÊòØ" in response_text or "ÈúÄË¶ÅÂêëÈáèÊêúÁ¥¢ÔºöÊòØ" in response_text
        
        # È¢ùÂ§ñÊ£ÄÊü•ÔºöÂ¶ÇÊûúSQLÊàêÂäü‰ΩÜÁªìÊûú‰∏∫Á©∫Ôºå‰πüÈúÄË¶ÅÂêëÈáèÊêúÁ¥¢
        result_count = len(db_results) if db_results else 0
        sql_successful_but_empty = has_valid_sql and result_count == 0

        
        # Á°ÆÂÆöÊòØÂê¶ÈúÄË¶ÅÂêëÈáèÊêúÁ¥¢
        needs_vector_search = not has_valid_data or sql_successful_but_empty or needs_vector
              
        # ÊèêÂèñÊ†ºÂºèÂåñÁªìÊûú
        formatted_db_results = None
        format_match = response_text.find("Ê†ºÂºèÂåñÁªìÊûú:")
        if format_match == -1:
            format_match = response_text.find("Ê†ºÂºèÂåñÁªìÊûúÔºö")
        
        if format_match != -1:
            # ÊèêÂèñÊ†ºÂºèÂåñÁªìÊûúÈÉ®ÂàÜÔºàÂåÖÊã¨Â§öË°åÔºâ
            format_text = response_text[format_match:]
            if "Ê†ºÂºèÂåñÁªìÊûú:" in format_text:
                formatted_db_results = format_text.split("Ê†ºÂºèÂåñÁªìÊûú:")[1].strip()
            elif "Ê†ºÂºèÂåñÁªìÊûúÔºö" in format_text:
                formatted_db_results = format_text.split("Ê†ºÂºèÂåñÁªìÊûúÔºö")[1].strip()
            
            # Â¶ÇÊûúÊ†ºÂºèÂåñÁªìÊûúÊòØ"Êó†"ÊàñÁ©∫ÔºåÂàôËÆæ‰∏∫None
            if formatted_db_results in ["Êó†", "", "Êó†Êï∞ÊçÆ", "Êó†ÁªìÊûú"]:
                formatted_db_results = None
        
        # Â¶ÇÊûúÊ≤°Êúâ‰ªéLLMËé∑ÂèñÂà∞Ê†ºÂºèÂåñÁªìÊûúÔºå‰ΩÜÊúâÊúâÊïàÊï∞ÊçÆÔºå‰ΩøÁî®ÁÆÄÂçïÊ†ºÂºèÂåñ
        if has_valid_data and db_results and not formatted_db_results:
            formatted_db_results = self._format_database_response(db_results, sql_query)
        
        # ÂØπ‰∫éÈáçËØïÂêéÁöÑÁªìÊûúÔºå‰∏çÂÜçÊ£ÄÊü•Â§ßÂ∞èÔºåÁõ¥Êé•‰ΩøÁî®LLMÂà§Êñ≠
        is_retry_validation = state.get("retry_attempted", False)
        
        logger.info(f"LLM validation result: sql_valid={has_valid_sql}, data_valid={has_valid_data}, result_count={result_count}, needs_vector_search={needs_vector_search}, is_retry={is_retry_validation}")
        logger.info(f"Formatted results: {'Yes' if formatted_db_results else 'No'}")
                
        return {
            **state,
            "db_results_valid": has_valid_data,
            "needs_vector_search": needs_vector_search,
            "result_too_large": False,  # ÈáçËØïÂêé‰∏çÂÜçÊ†áËÆ∞‰∏∫ËøáÂ§ß
            "retry_attempted": is_retry_validation,  # ‰øùÊåÅÈáçËØïÁä∂ÊÄÅ
            "formatted_db_results": formatted_db_results
        }
    
    async def execute_database_retry(self, state: AgentState) -> AgentState:
        """ÊâßË°åÊï∞ÊçÆÂ∫ìÊü•ËØ¢ÈáçËØï - ‰ºòÂåñSQL‰ª•ÂáèÂ∞ëÁªìÊûúÂ§ßÂ∞è"""
        # Use rewritten query if available, otherwise fallback to original
        query = state.get("rewritten_query") or state["query"]
        original_sql = state.get("sql_query")
        
        logger.info(f"Executing database retry for large results: {query}")
        
        try:
            # ‰ΩøÁî®LLM‰ºòÂåñSQL
            optimized_sql = await self._optimize_sql_for_large_results(original_sql, query)
            
            # ‰ΩøÁî®‰ºòÂåñÂêéÁöÑSQLÈáçÊñ∞Êü•ËØ¢Êï∞ÊçÆÂ∫ì
            logger.info(f"Retrying with optimized SQL: {optimized_sql}")
            db_results = await db_service.execute_sql_query(optimized_sql)
            
            if db_results is not None:
                logger.info(f"Database retry successful, got {len(db_results)} results")
                
                # Ê†ºÂºèÂåñÁªìÊûú
                formatted_content = self._format_database_response(db_results, optimized_sql)
                
                # ÂàõÂª∫Â∑•ÂÖ∑Ê∂àÊÅØ
                tool_message = AIMessage(content=f"Êï∞ÊçÆÂ∫ìÈáçËØïÁªìÊûúÔºà‰ºòÂåñÂêéÔºâÔºö\n{formatted_content}")
                
                return {
                    **state,
                    "db_results": db_results,
                    "sql_query": optimized_sql,
                    "db_success": True,
                    "retry_attempted": True,
                    "messages": [*state["messages"], tool_message]
                }
            else:
                logger.warning("Database retry failed: No results returned")
                error_message = AIMessage(content="Êï∞ÊçÆÂ∫ìÈáçËØïÂ§±Ë¥•: Êú™ËøîÂõûÁªìÊûú")
                return {
                    **state,
                    "db_success": False,
                    "retry_attempted": True,
                    "messages": [*state["messages"], error_message]
                }
                
        except Exception as e:
            logger.error(f"Database retry error: {e}")
            error_message = AIMessage(content=f"Êï∞ÊçÆÂ∫ìÈáçËØïÊó∂ÂèëÁîüÈîôËØØ: {str(e)}")
            return {
                **state,
                "db_success": False,
                "retry_attempted": True,
                "messages": [*state["messages"], error_message]
            }
    
    async def execute_vector_search(self, state: AgentState) -> AgentState:
        """ÊâßË°åÂêëÈáèÊêúÁ¥¢Â¢ûÂº∫"""
        # Use rewritten query if available, otherwise fallback to original
        query = state.get("rewritten_query") or state["query"]
        failed_sql = state.get("sql_query")
        
        logger.info(f"Executing vector search enhancement for: {query}")
        
        try:
            result = await vector_database_query_tool.ainvoke({
                "question": query,
                "failed_sql": failed_sql or ""
            })
            
            # ‰ΩøÁî®Êï∞ÊçÆÂ∫ìÁä∂ÊÄÅÂ≠óÊÆµÂ≠òÂÇ®ÂêëÈáèÊêúÁ¥¢ÁªìÊûú
            db_success = result.get("success", False)
            results = result.get("results", [])
            
            # ÂàõÂª∫Â∑•ÂÖ∑Ê∂àÊÅØ
            tool_message = AIMessage(content=f"ÂêëÈáèÊêúÁ¥¢Â¢ûÂº∫ÁªìÊûúÔºö\n{result.get('content', '')}")
            
            logger.info(f"Vector search completed. Success: {db_success}, Results count: {len(results) if results else 0}")
            
            return {
                **state,
                "db_results": results,
                "sql_query": result.get("sql_query"),
                "db_success": db_success,
                "messages": [*state["messages"], tool_message]
            }
            
        except Exception as e:
            logger.error(f"Vector search error: {e}")
            error_message = AIMessage(content=f"ÂêëÈáèÊêúÁ¥¢Êó∂ÂèëÁîüÈîôËØØ: {str(e)}")
            return {
                **state,
                "db_success": False,
                "messages": [*state["messages"], error_message]
            }
    
    
    async def generate_response(self, state: AgentState) -> AgentState:
        """ÁîüÊàêÊúÄÁªàÂõûÁ≠î - ÊåâÁÖßÊñ∞ÊµÅÁ®ãÔºöRAG -> Êï∞ÊçÆÂ∫ì -> ÂêëÈáèÊêúÁ¥¢"""
        rag_has_results = state.get("rag_has_results", False)
        db_results_valid = state.get("db_results_valid", False)
        formatted_db_results = state.get("formatted_db_results")
        original_query = state.get("original_query", state.get("query", ""))
        
        logger.info(f"Generating response for original query '{original_query}' - RAG: {rag_has_results}, DB: {db_results_valid}")
        
        # 1. Â¶ÇÊûúRAGÊúâÁªìÊûúÔºåÁõ¥Êé•ËøîÂõûRAGÁ≠îÊ°à
        if rag_has_results:
            rag_content = state.get("rag_content", "")
            if rag_content:
                logger.info("Returning RAG answer")
                ai_message = AIMessage(content=rag_content)
                return {
                    **state,
                    "final_answer": rag_content,
                    "messages": [*state["messages"], ai_message]
                }
        
        # 2. Â¶ÇÊûúÊï∞ÊçÆÂ∫ìÊü•ËØ¢ÊúâÊúâÊïàÁªìÊûúÔºåËøîÂõûÊ†ºÂºèÂåñÁªìÊûú
        elif db_results_valid and formatted_db_results:
            logger.info("Returning formatted database/vector search answer")
            ai_message = AIMessage(content=formatted_db_results)
            return {
                **state,
                "final_answer": formatted_db_results,
                "messages": [*state["messages"], ai_message]
            }
        
        # 3. Â¶ÇÊûúÈÉΩÊ≤°ÊúâÁªìÊûúÔºåËøîÂõûÊó†Ê≥ïÊâæÂà∞‰ø°ÊÅØÁöÑÊ∂àÊÅØ
        else:
            no_result_message = "Êä±Ê≠âÔºåÊó†Ê≥ïÊâæÂà∞‰∏éÊÇ®ÈóÆÈ¢òÁõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØ„ÄÇËØ∑Â∞ùËØïÈáçÊñ∞Ë°®Ëø∞ÊÇ®ÁöÑÈóÆÈ¢òÊàñÊèê‰æõÊõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ"
            logger.warning("No valid results found from any source")
            
            ai_message = AIMessage(content=no_result_message)
            return {
                **state,
                "final_answer": no_result_message,
                "messages": [*state["messages"], ai_message]
            }
    
    def _format_database_response(self, db_results: List[Dict], sql_query: str = None) -> str:
        """Ê†ºÂºèÂåñÊï∞ÊçÆÂ∫ìÊü•ËØ¢ÁªìÊûú"""
        if not db_results:
            return "Êú™ÊâæÂà∞Áõ∏ÂÖ≥Êï∞ÊçÆ"
        
        # ÊûÑÂª∫Ê†ºÂºèÂåñÂõûÁ≠î
        answer_parts = []
        
        # Ê∑ªÂä†ÁªìÊûúÁªüËÆ°
        result_count = len(db_results)
        answer_parts.append(f"Êü•ËØ¢ÁªìÊûúÔºöÂÖ±ÊâæÂà∞ {result_count} Êù°ËÆ∞ÂΩï\n")
        
        # Ê∑ªÂä†ÁªìÊûúÊï∞ÊçÆ
        if result_count > 0:
            answer_parts.append("Êï∞ÊçÆËØ¶ÊÉÖÔºö")
            
            # ÊòæÁ§∫Ââç10Êù°ËÆ∞ÂΩï
            display_count = min(10, result_count)
            for i, row in enumerate(db_results[:display_count], 1):
                if isinstance(row, dict):
                    # Ê†ºÂºèÂåñÂ≠óÂÖ∏Êï∞ÊçÆ
                    row_str = ", ".join([f"{k}: {v}" for k, v in row.items()])
                    answer_parts.append(f"{i}. {row_str}")
                else:
                    # Ê†ºÂºèÂåñÂÖ∂‰ªñÁ±ªÂûãÊï∞ÊçÆ
                    answer_parts.append(f"{i}. {row}")
            
            if result_count > display_count:
                answer_parts.append(f"... ËøòÊúâ {result_count - display_count} Êù°ËÆ∞ÂΩï")
        
        return "\n".join(answer_parts)
    
    def _check_result_size(self, db_results: List[Dict]) -> bool:
        """Ê£ÄÊü•Êï∞ÊçÆÂ∫ìÁªìÊûúÂ§ßÂ∞èÊòØÂê¶ËøáÂ§ß"""
        if not db_results:
            return False
        
        # ËÆ°ÁÆóÁªìÊûúÁöÑÂ≠óÁ¨¶ÈïøÂ∫¶
        try:
            # ‰ΩøÁî®Ëá™ÂÆö‰πâÂ∫èÂàóÂåñÂô®Â§ÑÁêÜDecimalÁ≠âÁâπÊÆäÁ±ªÂûã
            def json_serializer(obj):
                if hasattr(obj, 'isoformat'):  # datetime, date ÂØπË±°
                    return obj.isoformat()
                elif hasattr(obj, '__str__'):  # DecimalÁ≠âÂØπË±°
                    return str(obj)
                else:
                    return obj
            
            result_json = json.dumps(db_results, ensure_ascii=False, default=json_serializer)
            result_length = len(result_json)
            logger.info(f"Database result length: {result_length} characters")
            return result_length > 50000  # Ë∂ÖËøá10000Â≠óÁ¨¶ËÆ§‰∏∫ËøáÂ§ß
        except Exception as e:
            logger.warning(f"Failed to calculate result size: {e}")
            # Â¶ÇÊûúÊó†Ê≥ïËÆ°ÁÆóJSONÈïøÂ∫¶Ôºå‰ΩøÁî®ËÆ∞ÂΩïÊï∞Èáè‰Ωú‰∏∫Á≤óÁï•‰º∞ËÆ°
            return len(db_results) > 5000
    
    async def _optimize_sql_for_large_results(self, original_sql: str, question: str) -> str:
        """‰ΩøÁî®LLM‰ºòÂåñSQLÊü•ËØ¢‰ª•ÂáèÂ∞ëÁªìÊûúÂ§ßÂ∞è"""
        optimization_prompt = f"""ËØ∑‰ºòÂåñ‰ª•‰∏ãSQLÊü•ËØ¢‰ª•ÂáèÂ∞ëÁªìÊûúÂ§ßÂ∞è„ÄÇÂéüÂßãÊü•ËØ¢ËøîÂõû‰∫ÜÂ§™Â§öÊï∞ÊçÆÔºåÈúÄË¶ÅÊ∑ªÂä†DISTINCTÊàñGROUP BYÊù•ËÅöÂêàÁªìÊûú„ÄÇ

Áî®Êà∑ÈóÆÈ¢ò: {question}
ÂéüÂßãSQL: {original_sql}

‰ºòÂåñË¶ÅÊ±Ç:
1. Â¶ÇÊûúÊü•ËØ¢ËøîÂõûÈáçÂ§çÊï∞ÊçÆÔºåÊ∑ªÂä†DISTINCT
2. Â¶ÇÊûúÈúÄË¶ÅÁªüËÆ°‰ø°ÊÅØÔºå‰ΩøÁî®GROUP BYÂíåËÅöÂêàÂáΩÊï∞(COUNT, SUM, AVGÁ≠â)
3. ‰øùÊåÅÊü•ËØ¢ÁöÑÊ†∏ÂøÉÈÄªËæë‰∏çÂèò
4. ‰ºòÂÖà‰ΩøÁî®ËÅöÂêàÂáΩÊï∞Êù•Êèê‰æõÁªüËÆ°‰ø°ÊÅØËÄå‰∏çÊòØËØ¶ÁªÜÂàóË°®
5. Â¶ÇÊûúÂèØËÉΩÔºåÊ∑ªÂä†LIMITÂ≠êÂè•ÈôêÂà∂ÁªìÊûúÊï∞Èáè

ËØ∑Âè™ËøîÂõû‰ºòÂåñÂêéÁöÑSQLÊü•ËØ¢Ôºå‰∏çË¶ÅÂåÖÂê´ÂÖ∂‰ªñËß£Èáä„ÄÇ"""

        llm_messages = [
            LLMMessage(role="system", content="‰Ω†ÊòØ‰∏Ä‰∏™SQL‰ºòÂåñ‰∏ìÂÆ∂Ôºå‰∏ìÈó®Â§ÑÁêÜÂ§ßÊï∞ÊçÆÈáèÊü•ËØ¢ÁöÑ‰ºòÂåñ„ÄÇ"),
            LLMMessage(role="user", content=optimization_prompt)
        ]
        
        try:
            response = await llm_service.chat(llm_messages)
            optimized_sql = response.content.strip()
            
            # Ê∏ÖÁêÜSQLÔºåÁßªÈô§ÂèØËÉΩÁöÑmarkdownÊ†ºÂºè
            if optimized_sql.startswith("```sql"):
                optimized_sql = optimized_sql[6:]
            if optimized_sql.endswith("```"):
                optimized_sql = optimized_sql[:-3]
            optimized_sql = optimized_sql.strip()
            
            logger.info(f"SQL optimized: {original_sql} -> {optimized_sql}")
            return optimized_sql
            
        except Exception as e:
            logger.error(f"Failed to optimize SQL: {e}")
            return original_sql
    
    async def save_memory(self, state: AgentState) -> AgentState:
        """Save interaction to memory system"""
        user_email = state.get("user_email")
        session_id = state.get("session_id")
        query = state["query"]
        final_answer = state.get("final_answer", "")
        
        if not user_email:
            logger.info("No user email provided, skipping memory save")
            return state
        
        try:
            query_history_id = await self._save_query_history(state)
            # Extract and store memories from this interaction
            if query_history_id and final_answer:
                memory_ids = await memory_service.extract_and_store_memories(
                    query_id=query_history_id,
                    user_email=user_email,
                    question=query,
                    answer=final_answer,
                    session_id=session_id,
                    feedback_type=None  # Will be updated when user provides feedback
                )
                logger.info(f"Saved {len(memory_ids)} memories for interaction")
            
            # Periodically consolidate memories (every 10th query)
            if query_history_id and query_history_id % 10 == 0:
                consolidation_result = await memory_service.consolidate_memories(
                    user_email=user_email,
                    session_id=session_id
                )
                logger.info(f"Memory consolidation result: {consolidation_result}")
        except Exception as e:
            logger.error(f"Failed to save memory: {e}")
        
        return state

    async def _save_query_history(self, state: AgentState) -> int:
        """
        Save query history to database
        
        Args:
            state: The agent state containing all query information
            
        Returns:
            int: The query history ID if successful, None if failed
        """
        try:
            # Extract data from agent state - use original query for history
            query = state.get("original_query") or state.get("query", "")
            user_email = state.get("user_email")
            session_id = state.get("session_id", "default")
            answer = state.get("final_answer", "")
            sql_query = state.get("sql_query")
            query_type = state.get("intent")
            rag_results = state.get("rag_results", [])
            
            # Determine success based on whether we have an answer
            success = bool(answer and answer.strip())
            
            # Get LLM info from agent if available
            llm_provider = ""
            model_name = ""
            if hasattr(self, 'llm_provider') and self.llm_provider:
                llm_provider = str(self.llm_provider.value if hasattr(self.llm_provider, 'value') else self.llm_provider)
            if hasattr(self, 'model_name') and self.model_name:
                model_name = self.model_name
            
            # Calculate processing time (simplified - could be enhanced with actual timing)
            processing_time_ms = (time.time() - state.get("start_time")) * 1000  # Default value, could be calculated from actual start/end times

            # Â§ÑÁêÜRAGÁªìÊûúÔºåÊèêÂèñÊñáÊ°£‰ø°ÊÅØÂπ∂ÂéªÈáç
            unique_documents = {}
            if rag_results is not None:
                for result in rag_results:
                    if hasattr(result, 'metadata') and result.metadata:
                        doc_id = result.metadata.get('document_id')
                        if doc_id and doc_id not in unique_documents:
                            unique_documents[doc_id] = {
                                "document_id": doc_id,
                                "document_name": result.metadata.get('document_name', ''),
                                "dataset_name": result.metadata.get('dataset_name', '')
                            }
            
            # ËΩ¨Êç¢‰∏∫ÂàóË°®Ê†ºÂºè
            sources = list(unique_documents.values())

            # Save to history
            history = await history_service.save_query_history(
                session_id=session_id,
                user_email=user_email,
                question=query,
                answer=answer,
                sql_query=sql_query,
                sources=sources,
                query_type=query_type,
                success=success,
                processing_time_ms=processing_time_ms,
                llm_provider=llm_provider,
                model_name=model_name
            )
            
            logger.info(f"Saved query history with ID: {history.id}")
            return history.id
            
        except Exception as e:
            logger.error(f"Failed to save query history: {e}")
            return None
    
    async def query(self, question: str, context: Optional[str] = None, user_email: Optional[str] = None, session_id: Optional[str] = None) -> QueryResponse:
        """Â§ÑÁêÜÁî®Êà∑Êü•ËØ¢"""
        if not self.llm_available or not self.workflow:
            return QueryResponse(
                answer="Êä±Ê≠âÔºåAIÊúçÂä°ÊöÇÊó∂‰∏çÂèØÁî®ÔºåËØ∑Ê£ÄÊü•ÈÖçÁΩÆ",
                query_type=QueryType.HYBRID,
                confidence=0.0
            )
            
        try:
            # ÂàùÂßãÁä∂ÊÄÅ
            initial_state = {
                "messages": [HumanMessage(content=question)],
                "query": question,
                "original_query": question,  # Store original question for history
                "rewritten_query": None,  # Will be set by rewrite_question_with_memory
                "intent": None,
                # RAG-related fields
                "rag_results": None,
                "rag_content": None,
                "rag_has_results": False,
                # Database-related fields (also used for vector search results)
                "db_results": None,
                "sql_query": None,
                "db_success": False,
                "db_results_valid": False,
                "needs_vector_search": False,
                "formatted_db_results": None,
                "result_too_large": False,
                "retry_attempted": False,
                # Response fields
                "final_answer": None,
                "source_links": None,
                # Memory-related fields
                "memory_info": None,
                "user_email": user_email,
                "session_id": session_id,
                "start_time": time.time()
            }
            
            # ËøêË°åÂ∑•‰ΩúÊµÅ
            final_state = await self.workflow.ainvoke(
                initial_state,
                config=RunnableConfig(recursion_limit=20)
            )
            
            # Á°Æ‰øùÊúâÊúâÊïàÁöÑÁ≠îÊ°à
            answer = final_state.get("final_answer")
            if not answer or answer.strip() == "":
                # Â¶ÇÊûúÊ≤°ÊúâÊúÄÁªàÁ≠îÊ°àÔºåÂ∞ùËØï‰ªéÊ∂àÊÅØ‰∏≠Ëé∑ÂèñÊúÄÂêé‰∏Ä‰∏™AIÊ∂àÊÅØ
                messages = final_state.get("messages", [])
                for msg in reversed(messages):
                    if isinstance(msg, AIMessage) and msg.content and msg.content.strip():
                        answer = msg.content.strip()
                        break
                
                if not answer or answer.strip() == "":
                    answer = "Êä±Ê≠âÔºåÊó†Ê≥ïÁîüÊàêÂõûÁ≠î"
            
            # Á°ÆÂÆöÊü•ËØ¢Á±ªÂûã
            query_type = final_state.get("intent")
            if not query_type:
                # Ê†πÊçÆÁªìÊûúÊù•Ê∫êÁ°ÆÂÆöÊü•ËØ¢Á±ªÂûã
                if final_state.get("rag_has_results", False):
                    query_type = QueryType.RAG
                elif final_state.get("db_results_valid", False):
                    query_type = QueryType.DATABASE
                else:
                    query_type = QueryType.HYBRID
            
            # ÊûÑÂª∫Â¢ûÂº∫ÁöÑÂìçÂ∫î
            response = QueryResponse(
                answer=answer,
                query_type=query_type,
                reasoning=None,  # ÁÆÄÂåñÂìçÂ∫îÔºå‰∏çÂÜçÂåÖÂê´reasoning
                confidence=0.8
            )
            
            # Ê∑ªÂä†SQLÊü•ËØ¢‰ø°ÊÅØ
            if final_state.get("sql_query"):
                response.sql_query = final_state["sql_query"]
            
            # Ê∑ªÂä†RAGÊ∫êÊñá‰ª∂‰ø°ÊÅØÔºàÂ¶ÇÊûúÊù•Ëá™RAGÔºâ
            if final_state.get("rag_results"):
                # Â§ÑÁêÜRAGÁªìÊûúÔºåÊèêÂèñÊñáÊ°£‰ø°ÊÅØÂπ∂ÂéªÈáç
                unique_documents = {}
                for result in final_state["rag_results"]:
                    if hasattr(result, 'metadata') and result.metadata:
                        doc_id = result.metadata.get('document_id')
                        if doc_id and doc_id not in unique_documents:
                            unique_documents[doc_id] = {
                                "document_id": doc_id,
                                "document_name": result.metadata.get('document_name', ''),
                                "dataset_name": result.metadata.get('dataset_name', '')
                            }
                
                # ËΩ¨Êç¢‰∏∫ÂàóË°®Ê†ºÂºè
                response.sources = list(unique_documents.values())
            
            # ËÆ∞ÂΩïÊü•ËØ¢ÂÆåÊàêÊÉÖÂÜµ
            rag_used = final_state.get("rag_has_results", False)
            db_used = final_state.get("db_results_valid", False)
            
            logger.info(f"Query completed - RAG: {rag_used}, DB: {db_used}, SQL: {bool(response.sql_query)}, Sources: {len(response.sources) if response.sources else 0}")
            
            return response
            
        except Exception as e:
            logger.error(f"Agent query error: {e}")
            return QueryResponse(
                answer=f"Â§ÑÁêÜÊü•ËØ¢Êó∂ÂèëÁîüÈîôËØØ: {str(e)}",
                query_type=QueryType.HYBRID,
                confidence=0.0
            )

# ÂÖ®Â±ÄÂÆû‰æã
kangni_agent = KangniReActAgent()