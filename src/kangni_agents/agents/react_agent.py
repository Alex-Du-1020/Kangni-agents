from typing import Dict, Any, List, Optional, TypedDict, Annotated
from langgraph.graph import StateGraph, END
from langgraph.graph import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig

from ..config import settings
from ..models import QueryType, RAGSearchResult, QueryResponse
from ..models.llm_implementations import llm_service
from ..models.llm_providers import LLMMessage
from ..services.rag_service import rag_service
from ..services.database_service import db_service
from ..services.vector_embedding_service import vector_service
from ..utils.intent_classifier import intent_classifier

import logging
import json

logger = logging.getLogger(__name__)

class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    query: str
    intent: Optional[QueryType]
    rag_results: Optional[List[RAGSearchResult]]
    db_results: Optional[Dict[str, Any]]
    sql_query: Optional[str]
    source_links: Optional[List[str]]
    final_answer: Optional[str]
    reasoning: Optional[str]
    needs_tools: bool
    tool_to_use: Optional[str]
    # æ–°å¢å­—æ®µæ”¯æŒéªŒè¯å’Œfallbackæµç¨‹
    db_results_valid: bool
    needs_fallback: bool
    needs_vector_search: bool  # Added this missing field!
    fallback_executed: bool
    has_mixed_results: bool
    vector_enhanced: Optional[bool]  # Also add this for tracking
    suggestions_used: Optional[Dict[str, Any]]  # And this for tracking suggestions
    validation_reason: Optional[str]  # And validation reason

@tool
async def rag_search_tool(query: str, dataset_id: Optional[str] = None) -> Dict[str, Any]:
    """æœç´¢RAGæ–‡æ¡£åº“è·å–ç›¸å…³ä¿¡æ¯"""
    if not dataset_id:
        dataset_id = settings.ragflow_default_dataset_id
    
    results = await rag_service.search_rag(query, dataset_id)
    
    if not results:
        return {
            "content": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ä¿¡æ¯",
            "sources": [],
            "source_links": []
        }
    
    # æ ¼å¼åŒ–ç»“æœ - åªè¿”å›æ ¸å¿ƒä¿¡æ¯
    formatted_results = []
    source_links = []
    
    for i, result in enumerate(results[:5], 1):
        # ç®€åŒ–å“åº”ï¼ŒåªåŒ…å«å¿…è¦ä¿¡æ¯
        doc_info = {
            "document_id": getattr(result, 'document_id', f'doc_{i}'),
            "dataset_id": dataset_id,
            "document_name": getattr(result, 'document_name', f'Document {i}')
        }
        
        # æ„å»ºç®€æ´çš„æ˜¾ç¤ºæ–‡æœ¬
        formatted_results.append(f"{i}. [{doc_info['document_name']}] (ID: {doc_info['document_id']})")
        
        if result.metadata and 'source' in result.metadata:
            source_links.append(result.metadata['source'])
    
    return {
        "content": "\n".join(formatted_results),
        "sources": results[:5],
        "source_links": list(set(source_links))  # å»é‡
    }

@tool 
async def database_query_tool(question: str) -> Dict[str, Any]:
    """æŸ¥è¯¢æ•°æ®åº“è·å–ç»Ÿè®¡ä¿¡æ¯"""
    # ç›´æ¥æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢
    result = await db_service.query_database(question)
    
    # è¿”å›æ ¼å¼åŒ–ç»“æœ
    if result.get("success"):
        return format_db_results(result)
    else:
        return {
            "content": f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}",
            "sql_query": result.get("sql_query"),
            "results": [],
            "success": False,
            "error": result.get("error")
        }

@tool
async def vector_database_query_tool(question: str, failed_sql: str = None) -> Dict[str, Any]:
    """ä½¿ç”¨å‘é‡æœç´¢å¢å¼ºæ•°æ®åº“æŸ¥è¯¢ï¼Œæ‰¾åˆ°å®é™…å­˜åœ¨çš„å€¼å¹¶é‡æ–°ç”ŸæˆSQL"""
    import yaml
    from pathlib import Path
    from ..utils.sql_parser import SQLParser
    
    logger.info(f"Starting vector-enhanced database query for: {question}")
    
    try:
        # Load vector search configuration
        config_path = Path(__file__).parent.parent / "config" / "vector_search_config.yaml"
        vector_config = {}
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                vector_config = yaml.safe_load(f)
        else:
            logger.warning("Vector search config not found, using defaults")
            return {
                "content": "å‘é‡æœç´¢é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°",
                "success": False,
                "error": "Missing vector_search_config.yaml"
            }
        
        # Parse the failed SQL to identify tables and fields
        sql_parser = SQLParser()
        parsed_sql = {}
        if failed_sql:
            parsed_sql = sql_parser.parse_sql(failed_sql)
            logger.info(f"Parsed SQL - tables: {parsed_sql.get('tables')}, fields: {parsed_sql.get('fields')}")
        
        # Collect suggestions for all relevant fields
        all_suggestions = {}
        vector_fields = vector_config.get('vector_search_fields', [])
        
        for field_config in vector_fields:
            table = field_config['table']
            field = field_config['field']
            description = field_config.get('description', field)
            
            # Check if this field is relevant to the query
            should_search = False
            
            # If we have a parsed SQL, check if the table/field is mentioned
            if parsed_sql:
                if table in parsed_sql.get('tables', []) or field in parsed_sql.get('fields', []):
                    should_search = True
                    logger.info(f"Field {table}.{field} found in failed SQL")
            
            # Also check if keywords from the question match this field's keywords
            keywords = field_config.get('keywords', [])
            question_lower = question.lower()
            for keyword in keywords:
                if keyword.lower() in question_lower:
                    should_search = True
                    logger.info(f"Keyword '{keyword}' found in question for field {table}.{field}")
                    break
            
            if should_search:
                logger.info(f"Searching for similar values in {table}.{field} ({description})")
                
                # Get similar values from the database
                suggestions = await vector_service.search_similar_values(
                    search_text=question,
                    table_name=table,
                    field_name=field,
                    top_k=settings.max_suggestions or 3,  # This is already the max_suggestions value
                    similarity_threshold=settings.similarity_threshold or 0.3  # This is already the threshold value
                )
                
                if suggestions:
                    # Limit suggestions to max_suggestions
                    limited_suggestions = suggestions[:settings.max_suggestions]
                    # Extract just the field values for display
                    field_values = [suggestion['field_value'] for suggestion in limited_suggestions]
                    logger.info(f"Found {len(limited_suggestions)} suggestions for {description}: {field_values[:3]}...")
                    all_suggestions[f"{table}.{field}"] = {
                        "values": field_values,
                        "description": description,
                        "table": table,
                        "field": field
                    }
        
        if not all_suggestions:
            logger.info("No vector search suggestions found")
            return {
                "content": "æœªæ‰¾åˆ°ç›¸ä¼¼çš„æ•°æ®åº“å€¼",
                "success": False,
                "suggestions": {},
                "message": "å‘é‡æœç´¢æœªæ‰¾åˆ°åŒ¹é…çš„å€¼"
            }
        
        # Build enhanced prompt with suggestions
        suggestion_text = "\n\nåŸºäºå‘é‡æœç´¢æ‰¾åˆ°çš„æ•°æ®åº“å®é™…å€¼ï¼š\n"
        for field_key, field_data in all_suggestions.items():
            suggestion_text += f"- {field_data['description']} ({field_data['table']}.{field_data['field']}): "
            suggestion_text += f"{', '.join(field_data['values'])}"
            if len(field_data['values']) > 3:
                suggestion_text += f" ç­‰{len(field_data['values'])}ä¸ªå€¼"
            suggestion_text += "\n"
        
        enhanced_question = f"{question}{suggestion_text}\nè¯·ä½¿ç”¨è¿™äº›å®é™…å­˜åœ¨çš„å€¼é‡æ–°ç”ŸæˆSQLæŸ¥è¯¢ã€‚å¦‚æœæœ‰å¤šä¸ªåŒ¹é…å€¼ï¼Œä½¿ç”¨æœ€ç›¸ä¼¼çš„å€¼æ¥æŸ¥è¯¢ã€‚"
        
        # Generate new SQL with enhanced context
        logger.info("Generating new SQL with vector search suggestions")
        enhanced_result = await db_service.query_database(enhanced_question)
        
        if enhanced_result.get("success"):
            logger.info(f"Vector-enhanced query successful, got {len(enhanced_result.get('results', []))} results")
            enhanced_result["vector_enhanced"] = True
            enhanced_result["suggestions_used"] = all_suggestions
            return format_db_results(enhanced_result)
        else:
            logger.warning(f"Vector-enhanced query failed: {enhanced_result.get('error')}")
            return {
                "content": f"å‘é‡å¢å¼ºæŸ¥è¯¢å¤±è´¥: {enhanced_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                "success": False,
                "suggestions": all_suggestions,
                "sql_query": enhanced_result.get("sql_query"),
                "error": enhanced_result.get("error")
            }
            
    except Exception as e:
        logger.error(f"Error in vector database query tool: {e}")
        return {
            "content": f"å‘é‡æœç´¢å‡ºé”™: {str(e)}",
            "success": False,
            "error": str(e)
        }

def format_db_results(result: Dict[str, Any]) -> Dict[str, Any]:
    """æ ¼å¼åŒ–æ•°æ®åº“æŸ¥è¯¢ç»“æœ"""
    # å¤„ç†æ—¥æœŸåºåˆ—åŒ–é—®é¢˜
    def serialize_dates(obj):
        """é€’å½’å¤„ç†å¯¹è±¡ä¸­çš„æ—¥æœŸç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
        if hasattr(obj, 'isoformat'):  # datetime, date å¯¹è±¡
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {key: serialize_dates(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [serialize_dates(item) for item in obj]
        else:
            return obj
    
    # åºåˆ—åŒ–ç»“æœä¸­çš„æ—¥æœŸå¯¹è±¡
    serialized_results = serialize_dates(result.get("results", []))
    
    success = False
    try:
        success = True
        formatted_content = json.dumps({
            "sql_query": result.get("sql_query"),
            "results": serialized_results,
            "vector_enhanced": result.get("vector_enhanced", False),
            "suggestions_used": result.get("suggestions_used", [])
        }, ensure_ascii=False, indent=2)
    except (TypeError, ValueError) as e:
        # å¦‚æœä»ç„¶æœ‰åºåˆ—åŒ–é—®é¢˜ï¼Œä½¿ç”¨æ›´å®‰å…¨çš„æ ¼å¼åŒ–æ–¹æ³•
        logger.warning(f"JSON serialization failed, using fallback formatting: {e}")
        formatted_content = f"SQLæŸ¥è¯¢: {result.get('sql_query', 'N/A')}\n"
        formatted_content += f"ç»“æœæ•°é‡: {len(serialized_results)}\n"
        formatted_content += "ç»“æœæ•°æ®:\n"
        for i, row in enumerate(serialized_results[:5], 1):  # åªæ˜¾ç¤ºå‰5è¡Œ
            formatted_content += f"  {i}. {row}\n"
        if len(serialized_results) > 5:
            formatted_content += f"  ... è¿˜æœ‰ {len(serialized_results) - 5} è¡Œæ•°æ®"
    
    return {
        "success": success,
        "content": formatted_content,
        "sql_query": result.get("sql_query"),
        "results": serialized_results,
        "vector_enhanced": result.get("vector_enhanced", False),
        "suggestions_used": result.get("suggestions_used", [])
    }

class KangniReActAgent:
    def __init__(self):
        # ä½¿ç”¨é›†ä¸­å¼LLMæœåŠ¡
        self.llm_available = llm_service.llm_available
        self.llm_provider = llm_service.llm_provider
        
        if self.llm_available:
            try:
                # ç»‘å®šå·¥å…· - æ·»åŠ vector_database_query_tool
                self.tools = [rag_search_tool, database_query_tool, vector_database_query_tool]
                
                # æ„å»ºçŠ¶æ€å›¾
                self.workflow = self._build_workflow()

                # ä¿å­˜çŠ¶æ€å›¾çš„å¯è§†åŒ–è¡¨ç¤º
                self._save_graph_visualization()
                
                logger.info(f"Agent initialized successfully with LLM provider: {self.llm_provider}")
                
            except Exception as e:
                logger.warning(f"Failed to initialize agent: {e}")
                self.llm_available = False
                self.workflow = None
        else:
            logger.warning("LLM service not available, agent features will be disabled")
            self.workflow = None
    
    def _save_graph_visualization(self, filename: str = "docs/workflow_graph.png") -> None:
        """ä¿å­˜çŠ¶æ€å›¾çš„å¯è§†åŒ–è¡¨ç¤º
        
        Args:
            filename: ä¿å­˜æ–‡ä»¶è·¯å¾„
        """
        if not self.workflow:
            logger.warning("No workflow to visualize")
            return
            
        try:
            # å°è¯•ç”Ÿæˆ Mermaid å›¾
            graph = self.workflow.get_graph()
            
            # é¦–å…ˆå°è¯•ä¿å­˜ä¸º PNGï¼ˆéœ€è¦ graphvizï¼‰
            try:
                with open(filename, "wb") as f:
                    f.write(graph.draw_mermaid_png())
                logger.info(f"Graph visualization saved as PNG: {filename}")
                print(f"âœ… Workflow graph saved as: {filename}")
            except Exception as png_error:
                logger.warning(f"Could not save as PNG (graphviz may not be installed): {png_error}")
                
                # é€€è€Œæ±‚å…¶æ¬¡ï¼Œä¿å­˜ä¸º Mermaid æ–‡æœ¬æ ¼å¼
                mermaid_filename = filename.replace('.png', '.mermaid')
                try:
                    mermaid_text = graph.draw_mermaid()
                    with open(mermaid_filename, "w", encoding="utf-8") as f:
                        f.write(mermaid_text)
                    logger.info(f"Graph saved as Mermaid text: {mermaid_filename}")
                    print(f"âœ… Workflow graph saved as Mermaid text: {mermaid_filename}")
                    print(f"   You can visualize it at: https://mermaid.live/")
                    
                    # åŒæ—¶æ‰“å°å›¾å½¢ç»“æ„
                    print("\nğŸ“Š Workflow Structure:")
                    print(mermaid_text)
                except Exception as mermaid_error:
                    logger.error(f"Could not save Mermaid text: {mermaid_error}")
                    
                    # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰“å°èŠ‚ç‚¹å’Œè¾¹
                    print("\nğŸ“Š Workflow Nodes and Edges:")
                    print(f"Nodes: {graph.nodes}")
                    print(f"Edges: {graph.edges}")
                    
        except Exception as e:
            logger.error(f"Failed to save graph visualization: {e}")
            print(f"âŒ Could not visualize workflow: {e}")
    
    def _build_workflow(self) -> StateGraph:
        """æ„å»ºLangGraphå·¥ä½œæµ"""
        workflow = StateGraph(AgentState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("classify_intent", self.classify_intent)
        workflow.add_node("agent_reasoning", self.agent_reasoning)
        workflow.add_node("tool_execution", self.execute_tools)
        workflow.add_node("validate_results", self.validate_results)
        workflow.add_node("vector_search", self.vector_search_enhancement)  # æ–°å¢å‘é‡æœç´¢èŠ‚ç‚¹
        workflow.add_node("fallback_search", self.fallback_search)
        workflow.add_node("generate_response", self.generate_response)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("classify_intent")
        
        # æ·»åŠ åŸºç¡€è¾¹
        workflow.add_edge("classify_intent", "agent_reasoning")
        
        # ç›´æ¥è¾¹ï¼šæ€»æ˜¯æ‰§è¡Œå·¥å…·
        workflow.add_edge("agent_reasoning", "tool_execution")
        workflow.add_edge("tool_execution", "validate_results")
        
        # æ¡ä»¶è¾¹ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦å‘é‡æœç´¢å¢å¼º
        def should_use_vector_search(state: AgentState) -> str:
            # Debug logging
            logger.info(f"Checking vector search condition - needs_vector_search: {state.get('needs_vector_search')}, needs_fallback: {state.get('needs_fallback')}")
            
            # å¦‚æœæ•°æ®åº“æŸ¥è¯¢è¿”å›0ç»“æœï¼Œä½¿ç”¨å‘é‡æœç´¢
            if state.get("needs_vector_search", False):
                logger.info("Routing to vector_search")
                return "vector_search"
            # å¦åˆ™æ£€æŸ¥æ˜¯å¦éœ€è¦fallback
            elif state.get("needs_fallback", False):
                logger.info("Routing to fallback_search")
                return "fallback_search"
            else:
                logger.info("Routing to generate_response")
                return "generate_response"
        
        workflow.add_conditional_edges(
            "validate_results", 
            should_use_vector_search,
            {
                "vector_search": "vector_search",
                "fallback_search": "fallback_search",
                "generate_response": "generate_response"
            }
        )
        workflow.add_edge("vector_search", "generate_response")
        workflow.add_edge("fallback_search", "generate_response")
        workflow.add_edge("generate_response", END)
        
        return workflow.compile()
    
    async def classify_intent(self, state: AgentState) -> AgentState:
        """æ„å›¾åˆ†ç±»èŠ‚ç‚¹"""
        query = state["query"]
        intent = intent_classifier.classify_intent(query)
        explanation = intent_classifier.get_classification_explanation(query, intent)
        
        logger.info(f"Intent classified as: {intent} - {explanation}")
        
        return {
            **state,
            "intent": intent,
            "reasoning": explanation
        }
    
    async def agent_reasoning(self, state: AgentState) -> AgentState:
        """Agentæ¨ç†èŠ‚ç‚¹"""
        query = state["query"]
        intent = state["intent"]
        
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œéœ€è¦åˆ†æç”¨æˆ·é—®é¢˜å¹¶å†³å®šä½¿ç”¨å“ªäº›å·¥å…·ã€‚ä½ æœ‰ä¸¤ä¸ªå·¥å…·å¯ç”¨ï¼š

1. rag_search_tool: ç”¨äºæœç´¢æ–‡æ¡£å’ŒçŸ¥è¯†åº“ï¼Œé€‚åˆå›ç­”æ¦‚å¿µã€åŸå› ã€æ–¹æ³•ç­‰é—®é¢˜
2. database_query_tool: ç”¨äºæŸ¥è¯¢æ•°æ®åº“ï¼Œé€‚åˆå›ç­”ç»Ÿè®¡ã€æ•°æ®åˆ†æç­‰é—®é¢˜
   - ç‰¹åˆ«æ³¨æ„ï¼šå½“ç”¨æˆ·æåˆ°"è®¢å•"ä½†æ²¡æœ‰æŒ‡å®šå…·ä½“ç±»å‹æ—¶ï¼Œç³»ç»Ÿä¼šé»˜è®¤æŸ¥è¯¢ kn_quality_trace_prod_orderï¼ˆç”Ÿäº§è®¢å•è¡¨ï¼‰

å½“å‰é—®é¢˜æ„å›¾åˆ†ç±»ä¸º: {intent}
åˆ†ç±»åŸå› : {state.get('reasoning', '')}

ç”¨æˆ·é—®é¢˜: {query}

è¯·åˆ†æé—®é¢˜å¹¶å†³å®šä½¿ç”¨å“ªäº›å·¥å…·ï¼š
1. å¦‚æœé—®é¢˜æ˜ç¡®éœ€è¦æ•°æ®åº“æŸ¥è¯¢ï¼ˆå¦‚ç»Ÿè®¡ã€è®¡æ•°ã€å…·ä½“æ•°æ®ï¼‰ï¼Œé€‰æ‹© database_query_tool
2. å¦‚æœé—®é¢˜æ˜ç¡®éœ€è¦æ–‡æ¡£æœç´¢ï¼ˆå¦‚æ¦‚å¿µè§£é‡Šã€åŸå› åˆ†æï¼‰ï¼Œé€‰æ‹© rag_search_tool  
3. å¦‚æœé—®é¢˜æ„å›¾ä¸æ˜ç¡®æˆ–éœ€è¦ç»¼åˆä¿¡æ¯ï¼Œé€‰æ‹© bothï¼ˆåŒæ—¶ä½¿ç”¨ä¸¤ä¸ªå·¥å…·ï¼‰

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
å·¥å…·é€‰æ‹©: [rag_search_tool/database_query_tool/both]
ç†ç”±: [ç®€è¦è¯´æ˜ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›å·¥å…·]
"""
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºLLMMessage
        llm_messages = [
            LLMMessage(role="system", content=system_prompt),
            LLMMessage(role="user", content=query)
        ]
        
        # Debug logging for LLM input
        logger.debug(f"LLM reasoning input: {system_prompt}")
        
        try:
            response = await llm_service.chat(llm_messages)
            
            # Debug logging for LLM output
            logger.debug(f"LLM reasoning output: {response.content}")
            
            # è§£æå“åº”ï¼Œç¡®å®šå·¥å…·é€‰æ‹©
            response_text = response.content.lower()
            
            # ç¡®å®šå·¥å…·ç±»å‹
            tool_to_use = None
            needs_tools = True  # æ€»æ˜¯éœ€è¦å·¥å…·ï¼Œä¸å†ç›´æ¥å›ç­”
            
            if "å·¥å…·é€‰æ‹©: both" in response_text or "å·¥å…·é€‰æ‹©ï¼šboth" in response_text:
                tool_to_use = "both"
            elif "rag_search_tool" in response_text:
                tool_to_use = "rag_search_tool"
            elif "database_query_tool" in response_text:
                tool_to_use = "database_query_tool"
            else:
                # é»˜è®¤ä½¿ç”¨bothå·¥å…·
                tool_to_use = "both"
            
            logger.info(f"Tool analysis: needs_tools={needs_tools}, tool_to_use={tool_to_use}")
            
            # åˆ›å»ºAIMessageå“åº”
            ai_message = AIMessage(content=response.content)
            
            return {
                **state,
                "needs_tools": needs_tools,
                "tool_to_use": tool_to_use,
                "messages": [*state["messages"], ai_message]
            }
            
        except Exception as e:
            logger.error(f"LLM reasoning error: {e}")
            error_message = AIMessage(content=f"æŠ±æ­‰ï¼Œåˆ†ææ‚¨çš„é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {
                **state,
                "needs_tools": False,
                "messages": [*state["messages"], error_message]
            }
    
    async def execute_tools(self, state: AgentState) -> AgentState:
        """å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹"""
        tool_to_use = state.get("tool_to_use")
        query = state["query"]
        
        if not tool_to_use:
            logger.warning("No tool specified for execution")
            return state
            
        logger.info(f"Executing tool: {tool_to_use} with query: {query}")
        
        try:
            messages = state["messages"]
            
            if tool_to_use == "rag_search_tool":
                result = await rag_search_tool.ainvoke({"query": query})
                
                # å­˜å‚¨RAGç»“æœ
                state["rag_results"] = result.get("sources", [])
                state["source_links"] = result.get("source_links", [])
                
                # åˆ›å»ºå·¥å…·æ¶ˆæ¯
                tool_message = AIMessage(content=f"RAGæœç´¢ç»“æœï¼š\n{result['content']}")
                messages.append(tool_message)
                
            elif tool_to_use == "database_query_tool":
                result = await database_query_tool.ainvoke({"question": query})
                
                # å­˜å‚¨æ•°æ®åº“ç»“æœ
                state["db_results"] = result.get("results", [])
                state["sql_query"] = result.get("sql_query")
                
                # åˆ›å»ºå·¥å…·æ¶ˆæ¯
                tool_message = AIMessage(content=f"æ•°æ®åº“æŸ¥è¯¢ç»“æœï¼š\n{result['content']}")
                messages.append(tool_message)
                
            elif tool_to_use == "both":
                # åŒæ—¶æ‰§è¡Œä¸¤ä¸ªå·¥å…·
                logger.info("Executing both RAG and database tools")
                
                # æ‰§è¡ŒRAGæœç´¢
                rag_result = await rag_search_tool.ainvoke({"query": query})
                state["rag_results"] = rag_result.get("sources", [])
                state["source_links"] = rag_result.get("source_links", [])
                rag_message = AIMessage(content=f"RAGæœç´¢ç»“æœï¼š\n{rag_result['content']}")
                messages.append(rag_message)
                
                # æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢
                db_result = await database_query_tool.ainvoke({"question": query})
                state["db_results"] = db_result.get("results", [])
                state["sql_query"] = db_result.get("sql_query")
                db_message = AIMessage(content=f"æ•°æ®åº“æŸ¥è¯¢ç»“æœï¼š\n{db_result['content']}")
                messages.append(db_message)
                
                logger.info("Both tools executed successfully")
                
            else:
                tool_message = AIMessage(content=f"æœªçŸ¥å·¥å…·: {tool_to_use}")
                messages.append(tool_message)
            
            logger.debug(f"Tool {tool_to_use} execution completed")
            
            return {
                **state,
                "messages": messages
            }
            
        except Exception as e:
            logger.error(f"Tool execution error: {e}")
            error_message = AIMessage(content=f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}")
            return {
                **state,
                "messages": [*state["messages"], error_message]
            }
    
    
    async def validate_results(self, state: AgentState) -> AgentState:
        """éªŒè¯å·¥å…·æ‰§è¡Œç»“æœçš„æœ‰æ•ˆæ€§ - ä½¿ç”¨LLMæ™ºèƒ½åˆ¤æ–­"""
        tool_to_use = state.get("tool_to_use")
        
        if tool_to_use in ["database_query_tool", "both"]:
            # è·å–æ•°æ®åº“æŸ¥è¯¢ç»“æœ
            db_results = state.get("db_results", [])
            sql_query = state.get("sql_query")
            query = state["query"]
            
            # è·å–RAGç»“æœï¼ˆå¦‚æœä½¿ç”¨äº†bothå·¥å…·ï¼‰
            rag_results = state.get("rag_results", [])
            
            # æ„å»ºè®©LLMåˆ¤æ–­çš„æç¤º
            validation_prompt = f"""è¯·åˆ†æä»¥ä¸‹å·¥å…·æ‰§è¡Œç»“æœï¼Œåˆ¤æ–­æŸ¥è¯¢æ˜¯å¦æˆåŠŸä»¥åŠæ˜¯å¦éœ€è¦ä½¿ç”¨å‘é‡æœç´¢å¢å¼ºã€‚

ç”¨æˆ·é—®é¢˜: {query}

ç”Ÿæˆçš„SQL: {sql_query if sql_query else "æ— "}

æ•°æ®åº“æŸ¥è¯¢ç»“æœ: {json.dumps(db_results, ensure_ascii=False, indent=2) if db_results else "ç©ºç»“æœ"}

RAGæœç´¢ç»“æœ: {"æ‰¾åˆ°ç›¸å…³æ–‡æ¡£" if rag_results else "æ— RAGç»“æœ"}

è¯·åˆ†æå¹¶å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
1. SQLæŸ¥è¯¢æ˜¯å¦æˆåŠŸç”Ÿæˆï¼Ÿï¼ˆæ˜¯/å¦ï¼‰
2. æ•°æ®åº“æŸ¥è¯¢æ˜¯å¦è¿”å›äº†æœ‰æ•ˆæ•°æ®ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰
3. å¦‚æœæ²¡æœ‰è¿”å›æ•°æ®ï¼Œæ˜¯å¦åº”è¯¥ä½¿ç”¨å‘é‡æœç´¢æ¥æ‰¾åˆ°æ­£ç¡®çš„æ•°æ®åº“å€¼ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰
4. å¦‚æœå‘é‡æœç´¢ä¹Ÿä¸é€‚ç”¨ï¼Œæ˜¯å¦åº”è¯¥ä½¿ç”¨RAGæ–‡æ¡£æœç´¢ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆï¼Ÿï¼ˆæ˜¯/å¦ï¼‰

åˆ¤æ–­æ ‡å‡†ï¼š
- å¦‚æœSQLç”ŸæˆæˆåŠŸä½†è¿”å›0æ¡è®°å½•ï¼Œå¾ˆå¯èƒ½æ˜¯æŸ¥è¯¢æ¡ä»¶ä¸­çš„å€¼ä¸å‡†ç¡®ï¼ˆå¦‚é¡¹ç›®åç§°æ‹¼å†™é”™è¯¯ï¼‰ï¼Œåº”è¯¥ä½¿ç”¨å‘é‡æœç´¢
- å¦‚æœSQLç”Ÿæˆå¤±è´¥æˆ–æŸ¥è¯¢å‡ºé”™ï¼Œåº”è¯¥ä½¿ç”¨RAGä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
- å¦‚æœæŸ¥è¯¢è¿”å›äº†æ•°æ®ï¼Œåˆ™è®¤ä¸ºæˆåŠŸï¼Œä¸éœ€è¦é¢å¤–æ“ä½œ
- å¦‚æœä½¿ç”¨äº†bothå·¥å…·ä¸”RAGæœ‰ç»“æœï¼Œä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ç»“æœï¼ŒRAGä½œä¸ºè¡¥å……

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
SQLç”ŸæˆæˆåŠŸ: [æ˜¯/å¦]
æœ‰æ•ˆæ•°æ®: [æ˜¯/å¦]  
éœ€è¦å‘é‡æœç´¢: [æ˜¯/å¦]
éœ€è¦RAGå¤‡ç”¨: [æ˜¯/å¦]
åŸå› : [ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±]"""

            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºLLMMessage
            llm_messages = [
                LLMMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“æŸ¥è¯¢ç»“æœåˆ†æåŠ©æ‰‹ï¼Œéœ€è¦å‡†ç¡®åˆ¤æ–­æŸ¥è¯¢ç»“æœçš„æœ‰æ•ˆæ€§ã€‚"),
                LLMMessage(role="user", content=validation_prompt)
            ]
            
            try:
                response = await llm_service.chat(llm_messages)
                response_text = response.content.lower()
                
                # è§£æLLMçš„åˆ¤æ–­ç»“æœ
                has_valid_sql = "sqlç”ŸæˆæˆåŠŸ: æ˜¯" in response_text or "sqlç”ŸæˆæˆåŠŸï¼šæ˜¯" in response_text
                has_valid_data = "æœ‰æ•ˆæ•°æ®: æ˜¯" in response_text or "æœ‰æ•ˆæ•°æ®ï¼šæ˜¯" in response_text
                needs_vector = "éœ€è¦å‘é‡æœç´¢: æ˜¯" in response_text or "éœ€è¦å‘é‡æœç´¢ï¼šæ˜¯" in response_text
                needs_rag = "éœ€è¦ragå¤‡ç”¨: æ˜¯" in response_text or "éœ€è¦ragå¤‡ç”¨ï¼šæ˜¯" in response_text
                
                # æå–åŸå› 
                reason_match = response_text.find("åŸå› :")
                if reason_match == -1:
                    reason_match = response_text.find("åŸå› ï¼š")
                reason = response_text[reason_match:].split('\n')[0] if reason_match != -1 else "æœªæä¾›åŸå› "
                
                logger.info(f"LLM validation result: sql_valid={has_valid_sql}, data_valid={has_valid_data}, needs_vector={needs_vector}, needs_rag={needs_rag}")
                logger.info(f"Validation reason: {reason}")
                
                return {
                    **state,
                    "db_results_valid": has_valid_data,
                    "needs_vector_search": needs_vector,  # Changed from needs_vector_search to match what we check
                    "needs_fallback": needs_rag,
                    "fallback_executed": False,
                    "validation_reason": reason
                }
                
            except Exception as e:
                logger.error(f"LLM validation error: {e}, falling back to simple check")
                # å¦‚æœLLMåˆ¤æ–­å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„å¤‡ç”¨é€»è¾‘
                # æ£€æŸ¥æ˜¯å¦æœ‰SQLå’Œç»“æœ
                has_sql = bool(sql_query)
                has_results = False
                
                # å°è¯•å¤šç§æ–¹å¼æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
                if db_results:
                    if isinstance(db_results, list) and len(db_results) > 0:
                        has_results = True
                    elif isinstance(db_results, dict) and db_results:
                        has_results = True
                    elif isinstance(db_results, str) and db_results.strip():
                        has_results = True
                
                # ç®€å•åˆ¤æ–­é€»è¾‘
                needs_vector_search = has_sql and not has_results and state.get("intent") == QueryType.DATABASE
                needs_fallback = not has_sql or (not has_results and state.get("intent") != QueryType.DATABASE)
                
                logger.info(f"Fallback validation: has_sql={has_sql}, has_results={has_results}, needs_vector={needs_vector_search}, needs_fallback={needs_fallback}")
                
                return {
                    **state,
                    "db_results_valid": has_sql and has_results,
                    "needs_vector_search": needs_vector_search,
                    "needs_fallback": needs_fallback,
                    "fallback_executed": False,
                    "validation_reason": "ä½¿ç”¨å¤‡ç”¨éªŒè¯é€»è¾‘"
                }
        else:
            # RAGæœç´¢æˆ–å…¶ä»–æƒ…å†µï¼Œç›´æ¥è®¤ä¸ºæœ‰æ•ˆ
            return {
                **state,
                "db_results_valid": True,
                "needs_vector_search": False,
                "needs_fallback": False,
                "fallback_executed": False,
                "validation_reason": "RAGæœç´¢ï¼Œæ— éœ€æ•°æ®åº“éªŒè¯"
            }

    async def vector_search_enhancement(self, state: AgentState) -> AgentState:
        """ä½¿ç”¨å‘é‡æœç´¢å¢å¼ºæ•°æ®åº“æŸ¥è¯¢"""
        query = state["query"]
        failed_sql = state.get("sql_query")
        
        logger.info(f"Starting vector search enhancement for query: {query}")
        
        try:
            # è°ƒç”¨å‘é‡æ•°æ®åº“æŸ¥è¯¢å·¥å…·
            result = await vector_database_query_tool.ainvoke({
                "question": query,
                "failed_sql": failed_sql
            })
            
            # æ£€æŸ¥å‘é‡æœç´¢æ˜¯å¦æˆåŠŸ
            # æ³¨æ„ï¼šå¯¹äºCOUNTæŸ¥è¯¢ï¼Œå³ä½¿ç»“æœæ˜¯0ä¹Ÿæ˜¯æˆåŠŸçš„æŸ¥è¯¢
            vector_success = result.get("success", False)
            
            if vector_success:
                # å‘é‡æœç´¢æˆåŠŸï¼Œæ›´æ–°çŠ¶æ€
                results = result.get("results", [])
                logger.info(f"Vector search successful, found {len(results)} results")
                
                # åˆ›å»ºå·¥å…·æ¶ˆæ¯ï¼Œæ˜¾ç¤ºå…·ä½“çš„æŸ¥è¯¢ç»“æœ
                tool_message = AIMessage(
                    content=f"å‘é‡æœç´¢å¢å¼ºç»“æœï¼š\n{result.get('content', '')}"
                )
                
                return {
                    **state,
                    "db_results": results,
                    "sql_query": result.get("sql_query"),
                    "vector_enhanced": True,
                    "suggestions_used": result.get("suggestions_used", {}),
                    "db_results_valid": True,  # å³ä½¿COUNTæ˜¯0ï¼Œä¹Ÿæ˜¯æœ‰æ•ˆçš„æŸ¥è¯¢ç»“æœ
                    "messages": [*state["messages"], tool_message]
                }
            else:
                # å‘é‡æœç´¢å¤±è´¥ï¼ˆæ¯”å¦‚SQLç”Ÿæˆå¤±è´¥ï¼‰
                logger.warning("Vector search enhancement failed")
                
                no_result_message = AIMessage(
                    content="å‘é‡æœç´¢æœªèƒ½æ‰¾åˆ°åŒ¹é…çš„æ•°æ®åº“å€¼ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥æŸ¥è¯¢æ¡ä»¶"
                )
                
                # å¦‚æœå‘é‡æœç´¢ä¹Ÿå¤±è´¥ï¼Œå¯èƒ½éœ€è¦å°è¯•RAG
                return {
                    **state,
                    "vector_enhanced": False,
                    "needs_fallback": True,  # å°è¯•RAGä½œä¸ºæœ€åçš„æ‰‹æ®µ
                    "messages": [*state["messages"], no_result_message]
                }
                
        except Exception as e:
            logger.error(f"Vector search enhancement error: {e}")
            error_message = AIMessage(content=f"å‘é‡æœç´¢å¢å¼ºæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            
            return {
                **state,
                "vector_enhanced": False,
                "needs_fallback": True,  # å‡ºé”™æ—¶å°è¯•RAG
                "messages": [*state["messages"], error_message]
            }
    
    async def fallback_search(self, state: AgentState) -> AgentState:
        """å½“æ•°æ®åº“æŸ¥è¯¢æ— æ•ˆç»“æœæ—¶ï¼Œæ‰§è¡ŒRAGæœç´¢ä½œä¸ºfallback"""
        query = state["query"]
        
        logger.info(f"Executing RAG fallback search for query: {query}")
        
        try:
            # æ‰§è¡ŒRAGæœç´¢
            result = await rag_search_tool.ainvoke({"query": query})
            
            # å­˜å‚¨RAGç»“æœ
            rag_results = result.get("sources", [])
            source_links = result.get("source_links", [])
            
            # æ£€æŸ¥RAGæ˜¯å¦æœ‰æœ‰æ•ˆç»“æœ
            has_rag_results = bool(rag_results and len(rag_results) > 0)
            
            if has_rag_results:
                # æ ‡è®°ä¸ºæ··åˆç»“æœï¼ˆåŒ…å«SQLå’ŒRAGï¼‰
                has_mixed_results = bool(state.get("sql_query"))
                
                # åˆ›å»ºå·¥å…·æ¶ˆæ¯
                tool_message = AIMessage(content=f"RAGæœç´¢ç»“æœï¼ˆä½œä¸ºè¡¥å……ï¼‰ï¼š\n{result['content']}")
                
                logger.info(f"Fallback search successful: {len(rag_results)} results found")
                
                return {
                    **state,
                    "rag_results": rag_results,
                    "source_links": source_links,
                    "fallback_executed": True,
                    "has_mixed_results": has_mixed_results,
                    "messages": [*state["messages"], tool_message]
                }
            else:
                # RAGä¹Ÿæ²¡æœ‰æ‰¾åˆ°ç»“æœ
                no_result_message = AIMessage(content="RAGæœç´¢ä¹Ÿæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
                
                logger.warning("Fallback search found no results")
                
                return {
                    **state,
                    "fallback_executed": True,
                    "has_mixed_results": False,
                    "messages": [*state["messages"], no_result_message]
                }
                
        except Exception as e:
            logger.error(f"Fallback search error: {e}")
            error_message = AIMessage(content=f"è¡¥å……æœç´¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {
                **state,
                "fallback_executed": True,
                "messages": [*state["messages"], error_message]
            }
    
    async def generate_response(self, state: AgentState) -> AgentState:
        """ç”Ÿæˆæœ€ç»ˆå›ç­”"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·ç»“æœ
        has_tool_results = False
        for msg in state["messages"]:
            if isinstance(msg, ToolMessage):
                has_tool_results = True
                break
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ··åˆç»“æœï¼ˆSQL + RAGï¼‰
        has_mixed_results = state.get("has_mixed_results", False)
        
        if has_mixed_results:
            # å¦‚æœæœ‰æ··åˆç»“æœï¼Œæä¾›ç‰¹æ®Šæç¤º
            final_prompt = """è¯·åŸºäºä»¥ä¸Šå·¥å…·æ‰§è¡Œç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚ç‰¹åˆ«æ³¨æ„ï¼š
1. æ•°æ®åº“æŸ¥è¯¢æœªæ‰¾åˆ°ç›¸å…³ç»“æœï¼Œä½†RAGæœç´¢æ‰¾åˆ°äº†è¡¥å……ä¿¡æ¯
2. è¯·ç»“åˆRAGæœç´¢ç»“æœæ¥å›ç­”ç”¨æˆ·é—®é¢˜  
3. å¦‚æœRAGç»“æœèƒ½å¤Ÿå›ç­”ç”¨æˆ·é—®é¢˜ï¼Œè¯·æä¾›æ¸…æ™°å‡†ç¡®çš„ç­”æ¡ˆ
4. å¦‚æœRAGç»“æœä¹Ÿä¸èƒ½å®Œå…¨å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜å“ªäº›ä¿¡æ¯å¯ä»¥æä¾›ï¼Œå“ªäº›æ— æ³•ç¡®å®š
5. ä¿æŒå›ç­”çš„é€»è¾‘æ€§å’Œå¯è¯»æ€§

è¯·ç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦é‡å¤å·¥å…·è°ƒç”¨è¿‡ç¨‹ã€‚
"""
        elif has_tool_results:
            # å¦‚æœæœ‰å·¥å…·ç»“æœï¼ŒåŸºäºå·¥å…·ç»“æœç”Ÿæˆå›ç­”
            final_prompt = """è¯·åŸºäºä»¥ä¸Šå·¥å…·æ‰§è¡Œç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚å›ç­”è¦æ±‚ï¼š
1. å‡†ç¡®ã€å®Œæ•´åœ°å›ç­”ç”¨æˆ·é—®é¢˜
2. å¦‚æœæŸ¥è¯¢ç»“æœåŒ…å«COUNT(*)æˆ–æ•°é‡ç»Ÿè®¡ï¼Œè¯·æ˜ç¡®è¯´å‡ºå…·ä½“çš„æ•°å­—
3. åˆç†æ•´åˆå·¥å…·ç»“æœä¿¡æ¯
4. ä¿æŒå›ç­”çš„é€»è¾‘æ€§å’Œå¯è¯»æ€§
5. å¦‚æœå·¥å…·ç»“æœä¸è¶³ï¼Œè¦æ˜ç¡®è¯´æ˜

è¯·ç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦é‡å¤å·¥å…·è°ƒç”¨è¿‡ç¨‹ã€‚
"""
        else:
            # å¦‚æœæ²¡æœ‰å·¥å…·ç»“æœï¼Œç›´æ¥å›ç­”
            final_prompt = """è¯·åŸºäºä»¥ä¸Šå¯¹è¯ç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚å›ç­”è¦æ±‚ï¼š
1. å‡†ç¡®ã€å®Œæ•´åœ°å›ç­”ç”¨æˆ·é—®é¢˜
2. ä¿æŒå›ç­”çš„é€»è¾‘æ€§å’Œå¯è¯»æ€§
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¦æ˜ç¡®è¯´æ˜

è¯·ç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚
"""
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºLLMMessage
        llm_messages = []
        for msg in state["messages"]:
            if hasattr(msg, 'content'):
                role = "user" if isinstance(msg, HumanMessage) else "assistant"
                llm_messages.append(LLMMessage(role=role, content=msg.content))
        
        # æ·»åŠ æœ€ç»ˆæç¤º
        llm_messages.append(LLMMessage(role="user", content=final_prompt))
        
        # Debug logging for final response generation
        logger.debug(f"Final response generation input: {[{'role': msg.role, 'content': msg.content} for msg in llm_messages]}")
        
        try:
            response = await llm_service.chat(llm_messages)
            
            # Debug logging for final response output
            logger.debug(f"Final response output: {response.content}")
            
            # åˆ›å»ºAIMessageå“åº”
            ai_message = AIMessage(content=response.content)
            
            return {
                **state,
                "final_answer": response.content,
                "messages": [*state["messages"], ai_message]
            }
            
        except Exception as e:
            logger.error(f"LLM final response error: {e}")
            error_message = AIMessage(content=f"æŠ±æ­‰ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {
                **state,
                "final_answer": f"æŠ±æ­‰ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "messages": [*state["messages"], error_message]
            }
    
    async def query(self, question: str, context: Optional[str] = None) -> QueryResponse:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        if not self.llm_available or not self.workflow:
            return QueryResponse(
                answer="æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®",
                query_type=QueryType.HYBRID,
                confidence=0.0
            )
            
        try:
            # åˆå§‹çŠ¶æ€
            initial_state = {
                "messages": [HumanMessage(content=question)],
                "query": question,
                "intent": None,
                "rag_results": None,
                "db_results": None,
                "sql_query": None,
                "source_links": None,
                "final_answer": None,
                "reasoning": None,
                "needs_tools": False,
                "tool_to_use": None,
                # æ–°å¢å­—æ®µçš„é»˜è®¤å€¼
                "db_results_valid": False,
                "needs_fallback": False,
                "needs_vector_search": False,  # Added
                "fallback_executed": False,
                "has_mixed_results": False,
                "vector_enhanced": None,  # Added
                "suggestions_used": None,  # Added
                "validation_reason": None  # Added
            }
            
            # è¿è¡Œå·¥ä½œæµ
            final_state = await self.workflow.ainvoke(
                initial_state,
                config=RunnableConfig(recursion_limit=10)
            )
            
            # ç¡®ä¿æœ‰æœ‰æ•ˆçš„ç­”æ¡ˆ
            answer = final_state.get("final_answer")
            if not answer or answer.strip() == "":
                # å¦‚æœæ²¡æœ‰æœ€ç»ˆç­”æ¡ˆï¼Œå°è¯•ä»æ¶ˆæ¯ä¸­è·å–æœ€åä¸€ä¸ªAIæ¶ˆæ¯
                messages = final_state.get("messages", [])
                for msg in reversed(messages):
                    if isinstance(msg, AIMessage) and msg.content and msg.content.strip():
                        answer = msg.content.strip()
                        break
                
                if not answer or answer.strip() == "":
                    answer = "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›ç­”"
            
            # æ„å»ºå¢å¼ºçš„å“åº”
            response = QueryResponse(
                answer=answer,
                query_type=final_state.get("intent", QueryType.HYBRID),
                reasoning=final_state.get("reasoning"),
                confidence=0.8
            )
            
            # æ·»åŠ SQLæŸ¥è¯¢ä¿¡æ¯ï¼ˆå¦‚æœæ¥è‡ªæ•°æ®åº“ï¼‰
            if final_state.get("sql_query"):
                response.sql_query = final_state["sql_query"]
            
            # æ·»åŠ RAGæºæ–‡ä»¶ä¿¡æ¯ï¼ˆå¦‚æœæ¥è‡ªRAGï¼‰
            if final_state.get("rag_results"):
                # å¤„ç†RAGç»“æœï¼Œæå–æ–‡æ¡£ä¿¡æ¯å¹¶å»é‡
                unique_documents = {}
                for result in final_state["rag_results"]:
                    if hasattr(result, 'metadata') and result.metadata:
                        doc_id = result.metadata.get('document_id')
                        if doc_id and doc_id not in unique_documents:
                            unique_documents[doc_id] = {
                                "document_id": doc_id,
                                "document_name": result.metadata.get('document_name', ''),
                                "dataset_name": result.metadata.get('dataset_name', '')
                            }
                
                # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
                response.sources = list(unique_documents.values())
            
            logger.info(f"Query completed successfully. SQL: {bool(response.sql_query)}, Sources: {len(response.sources) if response.sources else 0}")
            
            return response
            
        except Exception as e:
            logger.error(f"Agent query error: {e}")
            return QueryResponse(
                answer=f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                query_type=QueryType.HYBRID,
                confidence=0.0
            )

# å…¨å±€å®ä¾‹
kangni_agent = KangniReActAgent()